#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
V2X ì§€ì—­ ê·¸ë¦¬ë“œ ê¸°ë°˜ ì´ìƒíƒì§€ë¥¼ ìœ„í•œ acell.py - í´ë˜ìŠ¤ ê· í˜• ì¡°ì • ê°œì„ ë²„ì „

í•µì‹¬ ë³€ê²½ì :
- ë…¸ë“œ: ì°¨ëŸ‰ â†’ ì§€ì—­ ê²©ì
- ë°ì´í„°: ê°œë³„ ì°¨ëŸ‰ ì´ìƒì ìˆ˜ â†’ ê²©ìë³„ í‰ê·  ì´ìƒì ìˆ˜
- ì•ˆì •ì„±: ê³ ì •ëœ ê·¸ë˜í”„ êµ¬ì¡°
- í´ë˜ìŠ¤ ê· í˜•: ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ë™ì  ì„ê³„ê°’ ì ìš©

Author: V2X Grid-based Anomaly Detection Team  
Date: 2025-06-07
"""

import numpy as np
import pandas as pd
import tensorflow.compat.v1 as tf  # TF 2.x í˜¸í™˜ì„±
import json
import os
from sklearn.utils.class_weight import compute_class_weight

dim = 20

def load_assist_data(dataset):
    """ê¸°ì¡´ Shenzhen ë°ì´í„° ë¡œë”© (í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)"""
    sz_adj = pd.read_csv('%s_adj.csv'%dataset, header=None)
    adj = np.array(sz_adj)  # â† np.matì„ np.arrayë¡œ ë³€ê²½
    data = pd.read_csv('sz_speed.csv')
    return data, adj

def load_v2x_data(dataset='v2x'):
    """
    V2X ì§€ì—­ ê·¸ë¦¬ë“œ ê¸°ë°˜ ì´ìƒíƒì§€ ë°ì´í„° ë¡œë”©
    
    Args:
        dataset (str): ë°ì´í„°ì…‹ ì´ë¦„ (ê¸°ë³¸ê°’: 'v2x')
    
    Returns:
        data (DataFrame): ê²©ìë³„ ì´ìƒì ìˆ˜ ë°ì´í„° (ì‹œê°„ Ã— ê²©ì)
        adj (matrix): ê²©ì ê°„ ì¸ì ‘í–‰ë ¬ (ê²©ì Ã— ê²©ì)
        poi_data (ndarray): ê²©ìë³„ ì •ì  ì†ì„± (ê²©ì Ã— íŠ¹ì„±)
        weather_data (ndarray): ì‹œê°„ë³„ ë™ì  ì†ì„± (ì‹œê°„ Ã— íŠ¹ì„±)
    """
    print(f"ğŸ—ºï¸ V2X ì§€ì—­ ê·¸ë¦¬ë“œ ì´ìƒíƒì§€ ë°ì´í„° ë¡œë”©: {dataset}")
    
    # ë°ì´í„° í´ë” ê²½ë¡œ
    data_dir = f'v2x_astgcn_data'
    
    # ë©”íƒ€ë°ì´í„° í™•ì¸
    metadata_path = os.path.join(data_dir, 'metadata.json')
    if os.path.exists(metadata_path):
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
            task_type = metadata.get('task_type', 'anomaly_detection')
            node_type = metadata.get('node_type', 'regional_grid')
            print(f"   ğŸ“‹ Task: {task_type}")
            print(f"   ğŸ—ºï¸ Node Type: {node_type}")
            
            # ê²©ì ì •ë³´ ì¶œë ¥
            if 'grid_info' in metadata:
                grid_info = metadata['grid_info']
                print(f"   ğŸ“Š ê²©ì í¬ê¸°: {grid_info.get('grid_size_meters', 500)}m")
                print(f"   ğŸ”¢ í™œì„± ê²©ì: {grid_info.get('active_grids_used', 'N/A')}ê°œ")
    else:
        print("   âš ï¸ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        task_type = 'anomaly_detection'
        node_type = 'regional_grid'
    
    # 1. ê²©ì ê°„ ì¸ì ‘í–‰ë ¬ ë¡œë”©
    adj_path = os.path.join(data_dir, f'{dataset}_adj.csv')
    if not os.path.exists(adj_path):
        raise FileNotFoundError(f"âŒ ê²©ì ì¸ì ‘í–‰ë ¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {adj_path}")
    
    adj_df = pd.read_csv(adj_path, header=None)
    adj = np.array(adj_df) 
    print(f"   âœ… ê²©ì ì¸ì ‘í–‰ë ¬: {adj.shape}")
    
    # ì¸ì ‘í–‰ë ¬ í†µê³„
    connection_ratio = np.count_nonzero(adj) / (adj.shape[0] * adj.shape[1])
    avg_connections = np.count_nonzero(adj, axis=1).mean()
    print(f"     ğŸ“Š ì—°ê²° ë¹„ìœ¨: {connection_ratio:.3f}")
    print(f"     ğŸ“Š ê²©ìë‹¹ í‰ê·  ì—°ê²°: {avg_connections:.1f}ê°œ")
    
    # 2. ê²©ìë³„ ì´ìƒì ìˆ˜ ë°ì´í„° ë¡œë”©
    speed_path = os.path.join(data_dir, f'{dataset}_speed.csv')
    if not os.path.exists(speed_path):
        raise FileNotFoundError(f"âŒ ì´ìƒì ìˆ˜ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {speed_path}")
    
    data = pd.read_csv(speed_path, header=None)
    print(f"   âœ… ê²©ìë³„ ì´ìƒì ìˆ˜: {data.shape} (ì‹œê°„ Ã— ê²©ì)")
    
    # ì´ìƒì ìˆ˜ ë°ì´í„° ê²€ì¦
    data_values = data.values
    print(f"     ğŸ“Š ì´ìƒì ìˆ˜ ë²”ìœ„: {data_values.min():.3f} ~ {data_values.max():.3f}")
    print(f"     ğŸ“Š í‰ê·  ì´ìƒì ìˆ˜: {data_values.mean():.3f}")
    
    # ì´ìƒ ë¹„ìœ¨ ê³„ì‚° (ì„ê³„ê°’ 0.3 ê¸°ì¤€)
    anomaly_ratio = (data_values > 0.3).mean()
    print(f"     ğŸ”¥ ì „ì²´ ì´ìƒ ë¹„ìœ¨: {anomaly_ratio*100:.2f}%")
    
    # ê²©ìë³„ ì´ìƒì ìˆ˜ ë¶„í¬
    grid_anomaly_means = data.mean(axis=0)
    print(f"     ğŸ“ˆ ê²©ìë³„ ì´ìƒì ìˆ˜:")
    print(f"       ìµœì†Œ: {grid_anomaly_means.min():.3f}")
    print(f"       ìµœëŒ€: {grid_anomaly_means.max():.3f}")
    print(f"       í‘œì¤€í¸ì°¨: {grid_anomaly_means.std():.3f}")
    
    # 3. ê²©ìë³„ ì •ì  ì†ì„± ë¡œë”© (POI)
    poi_path = os.path.join(data_dir, f'{dataset}_poi.csv')
    if not os.path.exists(poi_path):
        raise FileNotFoundError(f"âŒ ê²©ì POI íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {poi_path}")
    
    poi_df = pd.read_csv(poi_path, header=None)
    poi_data = poi_df.values
    print(f"   âœ… ê²©ìë³„ ì •ì  ì†ì„±: {poi_data.shape} (ê²©ì Ã— íŠ¹ì„±)")
    
    if task_type == 'anomaly_detection':
        print(f"     ğŸ“Š ê²©ì íŠ¹ì„±: [ê²½ë„, ìœ„ë„, ì¤‘ì‹¬ê±°ë¦¬, ë„ì‹¬ì—¬ë¶€, êµí†µë°€ë„, ë„¤íŠ¸ì›Œí¬ì¤‘ìš”ë„, ...]")
        # ë„ì‹¬ ê²©ì ë¹„ìœ¨
        if poi_data.shape[1] >= 4:
            downtown_grids = poi_data[:, 3]  # ë„ì‹¬ì—¬ë¶€
            downtown_ratio = downtown_grids.mean()
            print(f"     ğŸ¢ ë„ì‹¬ ê²©ì ë¹„ìœ¨: {downtown_ratio*100:.1f}%")
    
    # 4. ì‹œê°„ë³„ ë™ì  ì†ì„± ë¡œë”© (Weather)
    weather_path = os.path.join(data_dir, f'{dataset}_weather.csv')
    if not os.path.exists(weather_path):
        raise FileNotFoundError(f"âŒ Weather íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {weather_path}")
    
    weather_df = pd.read_csv(weather_path, header=None)
    weather_data = weather_df.values
    print(f"   âœ… ì‹œê°„ë³„ ë™ì  ì†ì„±: {weather_data.shape} (ì‹œê°„ Ã— íŠ¹ì„±)")
    
    if task_type == 'anomaly_detection':
        print(f"     ğŸ“Š ì‹œê°„ íŠ¹ì„±: [Period_F, Period_A, Period_N, Period_D, ..., ì´ìƒìœ„í—˜ë„, ë„¤íŠ¸ì›Œí¬í’ˆì§ˆ]")
        # ì´ìƒíƒì§€ ê´€ë ¨ íŠ¹ì„± í™•ì¸
        if weather_data.shape[1] >= 16:
            anomaly_risks = weather_data[:, -3]  # ì´ìƒìœ„í—˜ë„ (ë’¤ì—ì„œ 3ë²ˆì§¸)
            network_qualities = weather_data[:, -2]  # ë„¤íŠ¸ì›Œí¬í’ˆì§ˆ (ë’¤ì—ì„œ 2ë²ˆì§¸)
            print(f"     ğŸš¨ ì‹œê°„ë³„ í‰ê·  ì´ìƒìœ„í—˜ë„: {anomaly_risks.mean():.3f}")
            print(f"     ğŸ“¡ ì‹œê°„ë³„ í‰ê·  ë„¤íŠ¸ì›Œí¬í’ˆì§ˆ: {network_qualities.mean():.3f}")
    
    # ë°ì´í„° ê²€ì¦ ë° NaN ì²˜ë¦¬
    if np.isnan(data_values).any():
        print("   âš ï¸ ì´ìƒì ìˆ˜ ë°ì´í„°ì— NaN ë°œê²¬, 0ìœ¼ë¡œ ëŒ€ì²´")
        data = data.fillna(0)
    
    if np.isnan(poi_data).any():
        print("   âš ï¸ POI ë°ì´í„°ì— NaN ë°œê²¬, í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´")
        poi_data = pd.DataFrame(poi_data).fillna(pd.DataFrame(poi_data).mean()).values
    
    if np.isnan(weather_data).any():
        print("   âš ï¸ Weather ë°ì´í„°ì— NaN ë°œê²¬, í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´")
        weather_data = pd.DataFrame(weather_data).fillna(pd.DataFrame(weather_data).mean()).values
    
    return data, adj, poi_data, weather_data

def calculate_class_weights_v2x(trainY, method='balanced', verbose=True):
    """
    V2X ì´ìƒíƒì§€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° - ì„ê³„ê°’ ìˆ˜ì • ë²„ì „
    """
    # ğŸ”§ í•µì‹¬ ìˆ˜ì •: ì•ˆì „í•œ íƒ€ì… ë³€í™˜
    if isinstance(trainY, np.matrix):
        y_data = np.asarray(trainY)
    else:
        y_data = np.array(trainY)
    
    # ë°ì´í„° í‰í‰í™”
    y_flat = y_data.flatten()
    
    # ğŸ”§ í•µì‹¬ ìˆ˜ì •: ë” í˜„ì‹¤ì ì¸ ë™ì  ì„ê³„ê°’ ê³„ì‚°
    non_zero_values = y_flat[y_flat > 0]
    if len(non_zero_values) > 0:
        try:
            # ì—¬ëŸ¬ ì„ê³„ê°’ í›„ë³´ ê³„ì‚°
            p90 = np.percentile(non_zero_values, 90)
            p80 = np.percentile(non_zero_values, 80)
            p75 = np.percentile(non_zero_values, 75)
            mean_std = non_zero_values.mean() + non_zero_values.std()
            
            # ê°€ì¥ í˜„ì‹¤ì ì¸ ì„ê³„ê°’ ì„ íƒ (ë” ë‚®ì€ ë²”ìœ„)
            candidates = [p90, p80, p75, mean_std, 0.3, 0.25]
            dynamic_threshold = min([c for c in candidates if c >= 0.1 and c <= 0.6])
            
        except:
            dynamic_threshold = 0.25
        dynamic_threshold = float(dynamic_threshold)
    else:
        dynamic_threshold = 0.25
    
    # ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ê³„ê°’ ì ìš©
    y_binary = (y_flat > dynamic_threshold).astype(int)
    
    # í´ë˜ìŠ¤ ê°œìˆ˜ ê³„ì‚°
    pos_count = np.sum(y_binary == 1)  # ì´ìƒ ë°ì´í„°
    neg_count = np.sum(y_binary == 0)  # ì •ìƒ ë°ì´í„°
    total_count = len(y_binary)
    
    # í´ë˜ìŠ¤ ë¹„ìœ¨
    pos_ratio = pos_count / total_count
    neg_ratio = neg_count / total_count
    
    if verbose:
        print(f"ğŸ“Š V2X í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ (ìˆ˜ì •ëœ ì„ê³„ê°’: {dynamic_threshold:.3f}):")
        print(f"   ì „ì²´ ìƒ˜í”Œ: {total_count:,}")
        print(f"   ì •ìƒ ë°ì´í„°: {neg_count:,} ({neg_ratio:.1%})")
        print(f"   ì´ìƒ ë°ì´í„°: {pos_count:,} ({pos_ratio:.1%})")
        if pos_count > 0:
            print(f"   ë¶ˆê· í˜• ë¹„ìœ¨: {neg_count/pos_count:.1f}:1")
    
    # ê°€ì¤‘ì¹˜ ê³„ì‚°
    if pos_count == 0:
        print("âš ï¸ ì—¬ì „íˆ ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        print(f"ğŸ’¡ ì„ê³„ê°’ì„ ë” ë‚®ì¶°ë³´ì„¸ìš”: {dynamic_threshold * 0.6:.3f}")
        pos_weight = 1.0
    else:
        if method == 'balanced':
            pos_weight = neg_count / pos_count
        elif method == 'moderate':
            pos_weight = np.sqrt(neg_count / pos_count)
        elif method == 'conservative':
            pos_weight = min(3.0, neg_count / pos_count)
        else:
            pos_weight = 1.0
        
        # ê·¹ë‹¨ì  ê°€ì¤‘ì¹˜ ë°©ì§€
        pos_weight = np.clip(pos_weight, 1.5, 20.0)
    
    class_distribution = {
        'total_samples': total_count,
        'positive_samples': pos_count,
        'negative_samples': neg_count,
        'positive_ratio': pos_ratio,
        'negative_ratio': neg_ratio,
        'imbalance_ratio': neg_count / pos_count if pos_count > 0 else float('inf'),
        'dynamic_threshold': dynamic_threshold
    }
    
    if verbose:
        print(f"ğŸ¯ ê³„ì‚°ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ({method}): {pos_weight:.2f}")
    
    return pos_weight, class_distribution

def balance_anomaly_data(trainX, trainY, method='oversample', target_ratio=0.15, verbose=True):
    """
    ì´ìƒ ë°ì´í„° ê· í˜• ë§ì¶”ê¸°
    
    Args:
        trainX, trainY: í›ˆë ¨ ë°ì´í„°
        method: 'oversample', 'threshold_adjust' ì¤‘ ì„ íƒ
        target_ratio: ëª©í‘œ ì´ìƒ ë°ì´í„° ë¹„ìœ¨
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        balanced_trainX, balanced_trainY: ê· í˜• ë§ì¶˜ ë°ì´í„°
    """
    original_shape = trainY.shape
    if verbose:
        print(f"ğŸ”„ ë°ì´í„° ê· í˜• ì¡°ì • ì‹œì‘ - ì›ë³¸ í˜•íƒœ: {original_shape}")
    
    # í˜„ì¬ ì´ìƒ ë¹„ìœ¨ ê³„ì‚°
    y_flat = trainY.flatten()
    current_anomaly_ratio = (y_flat > 0.3).mean()
    
    if verbose:
        print(f"   í˜„ì¬ ì´ìƒ ë¹„ìœ¨: {current_anomaly_ratio:.3%}")
        print(f"   ëª©í‘œ ì´ìƒ ë¹„ìœ¨: {target_ratio:.3%}")
    
    if method == 'threshold_adjust':
        # ì„ê³„ê°’ ì¡°ì •ìœ¼ë¡œ ì´ìƒ ë¹„ìœ¨ ë§ì¶”ê¸°
        if current_anomaly_ratio < target_ratio:
            # ì„ê³„ê°’ì„ ë‚®ì¶°ì„œ ì´ìƒ ë°ì´í„° ì¦ê°€
            sorted_values = np.sort(y_flat[y_flat > 0])
            if len(sorted_values) > 0:
                target_idx = max(0, int(len(sorted_values) * (1 - target_ratio / current_anomaly_ratio)))
                new_threshold = sorted_values[target_idx] if target_idx < len(sorted_values) else 0.2
                
                # ìƒˆë¡œìš´ ì„ê³„ê°’ ì ìš©
                adjusted_trainY = trainY.copy()
                adjusted_trainY = np.where(adjusted_trainY > new_threshold, 
                                         adjusted_trainY, 
                                         adjusted_trainY * 0.5)  # ê²½ê³„ê°’ ë¶€ë“œëŸ½ê²Œ ì¡°ì •
                
                if verbose:
                    new_ratio = (adjusted_trainY.flatten() > 0.3).mean()
                    print(f"   ì„ê³„ê°’ ì¡°ì •: {0.3:.3f} â†’ {new_threshold:.3f}")
                    print(f"   ì¡°ì • í›„ ì´ìƒ ë¹„ìœ¨: {new_ratio:.3%}")
                
                return trainX, adjusted_trainY
        
        if verbose:
            print("   ì„ê³„ê°’ ì¡°ì • ë¶ˆí•„ìš” - ì›ë³¸ ë°ì´í„° ì‚¬ìš©")
        return trainX, trainY
    
    elif method == 'oversample':
        # ì‹œê°„ ë‹¨ìœ„ ì˜¤ë²„ìƒ˜í”Œë§ (ë” í˜„ì‹¤ì )
        time_steps, num_grids = trainY.shape
        
        # ì´ìƒì´ ë§ì€ ì‹œê°„ ìŠ¤í… ì°¾ê¸°
        time_anomaly_scores = (trainY > 0.3).mean(axis=1)  # ê° ì‹œê°„ë³„ ì´ìƒ ê²©ì ë¹„ìœ¨
        high_anomaly_times = np.where(time_anomaly_scores > time_anomaly_scores.mean())[0]
        
        if len(high_anomaly_times) > 0:
            # ì´ìƒì´ ë§ì€ ì‹œê°„ ìŠ¤í… ë³µì œ
            target_additional = int(time_steps * target_ratio / current_anomaly_ratio) - time_steps
            target_additional = min(target_additional, time_steps // 2)  # ìµœëŒ€ 50% ì¶”ê°€
            
            if target_additional > 0:
                # ë†’ì€ ì´ìƒ ì‹œê°„ë“¤ ì¤‘ì—ì„œ ëœë¤ ì„ íƒí•˜ì—¬ ë³µì œ
                selected_times = np.random.choice(high_anomaly_times, 
                                                size=min(target_additional, len(high_anomaly_times)), 
                                                replace=True)
                
                # ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€í•˜ì—¬ ë³µì œ
                noise_scale = 0.01
                additional_X = []
                additional_Y = []
                
                for time_idx in selected_times:
                    if time_idx < len(trainX):
                        # X ë°ì´í„°ì— ë…¸ì´ì¦ˆ ì¶”ê°€
                        noise_X = np.random.normal(0, noise_scale, trainX[time_idx].shape)
                        new_X = trainX[time_idx] + noise_X
                        additional_X.append(new_X)
                        
                        # Y ë°ì´í„°ëŠ” ì›ë³¸ ìœ ì§€ (ì•½ê°„ì˜ ìŠ¤ì¼€ì¼ë§ë§Œ)
                        scale_factor = np.random.uniform(0.95, 1.05)
                        new_Y = trainY[time_idx] * scale_factor
                        additional_Y.append(new_Y)
                
                if additional_X:
                    balanced_trainX = np.vstack([trainX] + additional_X)
                    balanced_trainY = np.vstack([trainY] + additional_Y)
                    
                    if verbose:
                        new_ratio = (balanced_trainY.flatten() > 0.3).mean()
                        print(f"   ì˜¤ë²„ìƒ˜í”Œë§ ì™„ë£Œ:")
                        print(f"     ì¶”ê°€ëœ ì‹œê°„ ìŠ¤í…: {len(additional_X):,}")
                        print(f"     ì¡°ì • í›„ í˜•íƒœ: {balanced_trainY.shape}")
                        print(f"     ì¡°ì • í›„ ì´ìƒ ë¹„ìœ¨: {new_ratio:.3%}")
                    
                    return balanced_trainX, balanced_trainY
        
        if verbose:
            print("   ì˜¤ë²„ìƒ˜í”Œë§ ì¡°ê±´ ë¯¸ì¶©ì¡± - ì›ë³¸ ë°ì´í„° ì‚¬ìš©")
        return trainX, trainY
    
    # ê¸°ë³¸ì ìœ¼ë¡œ ì›ë³¸ ë°ì´í„° ë°˜í™˜
    return trainX, trainY

def create_anomaly_threshold_labels(data, method='dynamic', smooth=True, verbose=True):
    """
    ì´ìƒíƒì§€ë¥¼ ìœ„í•œ ê°œì„ ëœ ì´ì§„ ë¼ë²¨ ìƒì„± - ì„ê³„ê°’ ìˆ˜ì • ë²„ì „
    """
    # ğŸ”§ í•µì‹¬ ìˆ˜ì •: np.matë¥¼ np.arrayë¡œ ê°•ì œ ë³€í™˜
    if hasattr(data, 'values'):
        data_values = data.values
    else:
        data_values = data
    
    # matrix íƒ€ì…ì„ arrayë¡œ ë³€í™˜
    if isinstance(data_values, np.matrix):
        data_values = np.asarray(data_values)
    
    # ì•ˆì „í•œ íƒ€ì… ë³€í™˜
    data_values = np.array(data_values, dtype=np.float32)
    
    if verbose:
        print(f"   ğŸ“Š ë°ì´í„° íƒ€ì… ë³€í™˜: {type(data)} â†’ {type(data_values)}")
        print(f"   ğŸ“Š ë°ì´í„° í˜•íƒœ: {data_values.shape}")
        print(f"   ğŸ“Š ë°ì´í„° ë²”ìœ„: {data_values.min():.3f} ~ {data_values.max():.3f}")
        print(f"   ğŸ“Š ë°ì´í„° í‰ê· : {data_values.mean():.3f}")
    
    if method == 'dynamic':
        # ğŸ”§ í•µì‹¬ ìˆ˜ì •: ë” í˜„ì‹¤ì ì¸ ì„ê³„ê°’ ê³„ì‚°
        non_zero_mask = data_values > 0
        non_zero_values = data_values[non_zero_mask]
        
        if len(non_zero_values) > 0:
            try:
                non_zero_flat = non_zero_values.flatten()
                
                # ğŸ¯ ìƒˆë¡œìš´ ì„ê³„ê°’ ì „ëµ
                # 1. ìƒìœ„ 10% ê¸°ì¤€ (ë” ë§ì€ ì´ìƒ ë°ì´í„° í¬í•¨)
                threshold_90 = np.percentile(non_zero_flat, 90)
                
                # 2. ìƒìœ„ 20% ê¸°ì¤€ (ê· í˜•ì¡íŒ ì ‘ê·¼)
                threshold_80 = np.percentile(non_zero_flat, 80)
                
                # 3. í‰ê·  + í‘œì¤€í¸ì°¨ ê¸°ì¤€
                mean_val = non_zero_flat.mean()
                std_val = non_zero_flat.std()
                threshold_stat = mean_val + std_val
                
                # ğŸ”§ ê°€ì¥ ì ì ˆí•œ ì„ê³„ê°’ ì„ íƒ (ë” ë‚®ì€ ê°’)
                threshold_candidates = [threshold_90, threshold_80, threshold_stat, 0.3]
                threshold = min([t for t in threshold_candidates if t > 0.1])
                
                # ì•ˆì „ ë²”ìœ„ í™•ë³´
                threshold = max(0.15, min(0.6, threshold))
                
                if verbose:
                    print(f"   ğŸ¯ ì„ê³„ê°’ í›„ë³´ë“¤:")
                    print(f"     90th percentile: {threshold_90:.3f}")
                    print(f"     80th percentile: {threshold_80:.3f}")
                    print(f"     mean + std: {threshold_stat:.3f}")
                    print(f"     ì„ íƒëœ ì„ê³„ê°’: {threshold:.3f}")
                
            except Exception as e:
                if verbose:
                    print(f"   âš ï¸ Percentile ê³„ì‚° ì‹¤íŒ¨: {e}")
                # í´ë°±: ê³ ì • ì„ê³„ê°’
                threshold = 0.25
        else:
            threshold = 0.25
            
    elif method == 'percentile':
        # ì „ì²´ ë°ì´í„° ê¸°ì¤€ ìƒìœ„ 15% (ë” ê³µê²©ì )
        try:
            data_flat = data_values.flatten()
            threshold = np.percentile(non_zero_flat, 80)  # ë” ë§ì€ ì´ìƒ ë°ì´í„° í¬í•¨
            threshold = max(0.2, min(0.4, threshold))     # ì•ˆì „ ë²”ìœ„ ë³´ì¥
        except:
            threshold = 0.25
    else:  # fixed
        threshold = 0.25  # ê¸°ë³¸ê°’ì„ 0.3ì—ì„œ 0.25ë¡œ ë‚®ì¶¤
    
    # ì„ê³„ê°’ì„ scalarë¡œ í™•ì‹¤íˆ ë³€í™˜
    threshold = float(threshold)
    
    binary_labels = (data_values > threshold).astype(float)
    
    if smooth:
        # ì‹œê°„ì  ìŠ¤ë¬´ë”© (ì—°ì†ëœ ì´ìƒ íŒ¨í„´ ê°•í™”)
        try:
            from scipy import ndimage
            
            # ê° ê²©ìë³„ë¡œ ì‹œê°„ ì¶• ìŠ¤ë¬´ë”©
            for grid_idx in range(binary_labels.shape[1]):
                grid_series = binary_labels[:, grid_idx]
                
                # 3ì  ì´ë™í‰ê· ìœ¼ë¡œ ìŠ¤ë¬´ë”©
                smoothed = ndimage.uniform_filter1d(grid_series.astype(float), size=3)
                binary_labels[:, grid_idx] = (smoothed > 0.3).astype(float)
        except ImportError:
            if verbose:
                print("   âš ï¸ scipy ì—†ìŒ - ìŠ¤ë¬´ë”© ê±´ë„ˆë›°ê¸°")
        except Exception as e:
            if verbose:
                print(f"   âš ï¸ ìŠ¤ë¬´ë”© ì‹¤íŒ¨: {e}")
    
    if verbose:
        anomaly_ratio = binary_labels.mean()
        print(f"   ğŸ“Š ì„ê³„ê°’ ë°©ë²•: {method}")
        print(f"   ğŸ“Š ìµœì¢… ì‚¬ìš©ëœ ì„ê³„ê°’: {threshold:.3f}")
        print(f"   ğŸ“Š ìƒì„±ëœ ì´ìƒ ë¹„ìœ¨: {anomaly_ratio:.3%}")
        
        # ì¶”ê°€ ê²€ì¦ ì •ë³´
        if anomaly_ratio == 0:
            print(f"   âš ï¸ ì—¬ì „íˆ ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            print(f"   ğŸ’¡ ë” ë‚®ì€ ì„ê³„ê°’ ì‹œë„: {threshold * 0.7:.3f}")
        elif anomaly_ratio > 0.5:
            print(f"   âš ï¸ ì´ìƒ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤ ({anomaly_ratio:.1%})")
            print(f"   ğŸ’¡ ë” ë†’ì€ ì„ê³„ê°’ ì‹œë„: {threshold * 1.3:.3f}")
        else:
            print(f"   âœ… ì ì ˆí•œ ì´ìƒ ë¹„ìœ¨ ë‹¬ì„±!")
    
    return binary_labels, threshold

def create_anomaly_threshold_labels_stable(data, threshold=0.3, verbose=True):
    """
    ì•ˆì •í™”ëœ ì´ìƒíƒì§€ ë¼ë²¨ ìƒì„± - ê³ ì • ì„ê³„ê°’ ì‚¬ìš©
    """
    # ì•ˆì „í•œ ë°ì´í„° ë³€í™˜
    if hasattr(data, 'values'):
        data_values = data.values
    else:
        data_values = data
    
    if isinstance(data_values, np.matrix):
        data_values = np.asarray(data_values)
    
    data_values = np.array(data_values, dtype=np.float32)
    
    if verbose:
        print(f"   ğŸ“Š ì•ˆì •í™”ëœ ë¼ë²¨ ìƒì„±:")
        print(f"     ë°ì´í„° í˜•íƒœ: {data_values.shape}")
        print(f"     ë°ì´í„° ë²”ìœ„: {data_values.min():.3f} ~ {data_values.max():.3f}")
        print(f"     ê³ ì • ì„ê³„ê°’: {threshold:.3f}")
    
    # NaN/Inf ì²˜ë¦¬
    data_values = np.nan_to_num(data_values, nan=0.0, posinf=1.0, neginf=0.0)
    
    # ê³ ì • ì„ê³„ê°’ìœ¼ë¡œ ì´ì§„í™”
    binary_labels = (data_values > threshold).astype(float)
    
    if verbose:
        anomaly_ratio = binary_labels.mean()
        print(f"     ìƒì„±ëœ ì´ìƒ ë¹„ìœ¨: {anomaly_ratio:.3%}")
        
        if anomaly_ratio == 0:
            print(f"     âš ï¸ ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            print(f"     ğŸ’¡ ì„ê³„ê°’ì„ ë‚®ì¶°ë³´ì„¸ìš”: {threshold * 0.7:.3f}")
        elif anomaly_ratio > 0.5:
            print(f"     âš ï¸ ì´ìƒ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤")
            print(f"     ğŸ’¡ ì„ê³„ê°’ì„ ë†’ì—¬ë³´ì„¸ìš”: {threshold * 1.3:.3f}")
        else:
            print(f"     âœ… ì ì ˆí•œ ì´ìƒ ë¹„ìœ¨!")
    
    return binary_labels, threshold

def calculate_class_weights_stable(trainY, threshold=0.3, max_weight=3.0, verbose=True):
    """
    ì•ˆì •í™”ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° - ê°€ì¤‘ì¹˜ ì œí•œ
    """
    # ì•ˆì „í•œ íƒ€ì… ë³€í™˜
    if isinstance(trainY, np.matrix):
        y_data = np.asarray(trainY)
    else:
        y_data = np.array(trainY)
    
    y_flat = y_data.flatten()
    
    # ê³ ì • ì„ê³„ê°’ìœ¼ë¡œ ì´ì§„í™”
    y_binary = (y_flat > threshold).astype(int)
    
    # í´ë˜ìŠ¤ ê°œìˆ˜ ê³„ì‚°
    pos_count = np.sum(y_binary == 1)
    neg_count = np.sum(y_binary == 0)
    total_count = len(y_binary)
    
    if verbose:
        print(f"ğŸ“Š ì•ˆì •í™”ëœ í´ë˜ìŠ¤ ë¶„í¬ (ê³ ì • ì„ê³„ê°’: {threshold:.3f}):")
        print(f"   ì „ì²´ ìƒ˜í”Œ: {total_count:,}")
        print(f"   ì •ìƒ ë°ì´í„°: {neg_count:,} ({neg_count/total_count:.1%})")
        print(f"   ì´ìƒ ë°ì´í„°: {pos_count:,} ({pos_count/total_count:.1%})")
    
    # ê°€ì¤‘ì¹˜ ê³„ì‚° (ì œí•œ ì ìš©)
    if pos_count == 0:
        print("âš ï¸ ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        pos_weight = 1.0
    else:
        raw_weight = neg_count / pos_count
        pos_weight = min(max_weight, max(1.5, raw_weight))  # 1.5~3.0 ë²”ìœ„
        
        if verbose:
            print(f"   ì›ì‹œ ê°€ì¤‘ì¹˜: {raw_weight:.2f}")
            print(f"   ì œí•œëœ ê°€ì¤‘ì¹˜: {pos_weight:.2f} (ìµœëŒ€ {max_weight})")
    
    return pos_weight

def preprocess_data_stable(data1, time_len, train_rate, seq_len, pre_len, 
                          model_name, scheme, poi_data=None, weather_data=None,
                          threshold=0.3):
    """
    ì•ˆì •í™”ëœ V2X ì´ìƒíƒì§€ ì „ì²˜ë¦¬ - ì¼ê´€ëœ ë°ì´í„° ì²˜ë¦¬
    """
    print(f"ğŸ› ï¸ ì•ˆì •í™”ëœ V2X ì´ìƒíƒì§€ ì „ì²˜ë¦¬:")
    print(f"   ğŸ“Š ë°ì´í„° í˜•íƒœ: {data1.shape}")
    print(f"   ğŸ¯ ê³ ì • ì„ê³„ê°’: {threshold}")
    print(f"   ğŸ”§ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}, ì˜ˆì¸¡ ê¸¸ì´: {pre_len}")
    
    # ì•ˆì „í•œ ë°ì´í„° ë³€í™˜
    if isinstance(data1, np.matrix):
        data1 = np.asarray(data1)
    
    data_values = np.array(data1, dtype=np.float32)
    
    # NaN/Inf ì²˜ë¦¬
    data_values = np.nan_to_num(data_values, nan=0.0, posinf=1.0, neginf=0.0)
    
    print(f"   ğŸ“Š ë°ì´í„° í†µê³„:")
    print(f"     ë²”ìœ„: {data_values.min():.3f} ~ {data_values.max():.3f}")
    print(f"     í‰ê· : {data_values.mean():.3f}")
    
    # ğŸ¯ í•µì‹¬: ì—°ì†ê°’ê³¼ ì´ì§„ê°’ ë™ì¼í•œ ì„ê³„ê°’ ì‚¬ìš©
    binary_labels, _ = create_anomaly_threshold_labels_stable(
        data_values, threshold=threshold, verbose=True
    )
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    train_size = int(time_len * train_rate)
    
    # ğŸ¯ ì¤‘ìš”: ë¼ë²¨ë„ ì—°ì†ê°’ ì‚¬ìš© (ì¼ê´€ì„± í™•ë³´)
    train_data = data_values[:train_size]
    test_data = data_values[train_size:]
    
    train_labels = binary_labels[:train_size]
    test_labels = binary_labels[train_size:]
    
    print(f"   âœ‚ï¸ ë¶„í•  ì™„ë£Œ:")
    print(f"     í›ˆë ¨: {train_data.shape}")
    print(f"     í…ŒìŠ¤íŠ¸: {test_data.shape}")
    
    # ì‹œí€€ìŠ¤ ìƒì„± (ë‹¨ìˆœí™”)
    trainX, trainY, testX, testY = [], [], [], []
    
    # í›ˆë ¨ ì‹œí€€ìŠ¤
    for i in range(seq_len, len(train_data) - pre_len + 1):
        # ì…ë ¥: ì—°ì†ê°’
        seq_x = train_data[i-seq_len:i].T  # (nodes, seq_len)
        # ë¼ë²¨: ì´ì§„ê°’
        seq_y = train_labels[i:i+pre_len].T  # (nodes, pre_len)
        
        trainX.append(seq_x)
        trainY.append(seq_y)
    
    # í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤
    for i in range(seq_len, len(test_data) - pre_len + 1):
        seq_x = test_data[i-seq_len:i].T
        seq_y = test_labels[i:i+pre_len].T
        
        testX.append(seq_x)
        testY.append(seq_y)
    
    # ë°°ì—´ ë³€í™˜
    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)
    
    # ì°¨ì› ì¡°ì •: (samples, seq_len, nodes)
    trainX = np.transpose(trainX, (0, 2, 1))
    trainY = np.transpose(trainY, (0, 2, 1))
    testX = np.transpose(testX, (0, 2, 1))
    testY = np.transpose(testY, (0, 2, 1))
    
    # ì•ˆì •í™”ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    pos_weight = calculate_class_weights_stable(
        trainY, threshold=threshold, max_weight=3.0, verbose=True
    )
    
    # ìµœì¢… ê²€ì¦
    train_anomaly_ratio = (trainY > threshold).mean()
    test_anomaly_ratio = (testY > threshold).mean()
    
    print(f"   âœ… ì•ˆì •í™” ì™„ë£Œ:")
    print(f"     trainX: {trainX.shape}")
    print(f"     trainY: {trainY.shape}")
    print(f"     testX: {testX.shape}")
    print(f"     testY: {testY.shape}")
    print(f"     í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f}")
    print(f"     í›ˆë ¨ ì´ìƒ ë¹„ìœ¨: {train_anomaly_ratio:.2%}")
    print(f"     í…ŒìŠ¤íŠ¸ ì´ìƒ ë¹„ìœ¨: {test_anomaly_ratio:.2%}")
    
    # ê· í˜• ì •ë³´ ë°˜í™˜
    balance_info = {
        'pos_weight': pos_weight,
        'threshold_used': threshold,
        'train_anomaly_ratio': train_anomaly_ratio,
        'test_anomaly_ratio': test_anomaly_ratio
    }
    
    return trainX, trainY, testX, testY, balance_info

# ê¸°ì¡´ preprocess_data í•¨ìˆ˜ë¥¼ ì•ˆì •í™”ëœ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´
def preprocess_data(data1, time_len, train_rate, seq_len, pre_len, model_name, scheme, poi_data=None, weather_data=None):
    """
    V2X ë°ì´í„° ì „ì²˜ë¦¬ (ì•ˆì •í™”ëœ ë²„ì „ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸)
    """
    print("ğŸ”„ ì•ˆì •í™”ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸...")
    return preprocess_data_stable(
        data1, time_len, train_rate, seq_len, pre_len,
        model_name, scheme, poi_data, weather_data,
        threshold=0.3  # ê³ ì • ì„ê³„ê°’ ì‚¬ìš©
    )

# ê¸°ì¡´ preprocess_data_grid_anomaly í•¨ìˆ˜ë„ ì•ˆì •í™”ëœ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´
def preprocess_data_grid_anomaly(data1, time_len, train_rate, seq_len, pre_len, model_name, scheme, poi_data=None, weather_data=None):
    """
    ì§€ì—­ ê·¸ë¦¬ë“œ ê¸°ë°˜ ì´ìƒíƒì§€ ë°ì´í„° ì „ì²˜ë¦¬ (ì•ˆì •í™”ëœ ë²„ì „ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸)
    """
    print("ğŸ”„ ì•ˆì •í™”ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸...")
    return preprocess_data_stable(
        data1, time_len, train_rate, seq_len, pre_len,
        model_name, scheme, poi_data, weather_data,
        threshold=0.3  # ê³ ì • ì„ê³„ê°’ ì‚¬ìš©
    )

# ê¸°ì¡´ Unit í´ë˜ìŠ¤ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (ë³€ê²½ ì—†ìŒ)
class Unit():
    def __init__(self, dim, num_nodes, reuse = None):
        self.dim = dim
        self.num_nodes = num_nodes
    def call(self, inputs, time_len):
        x, e = inputs  
        unit_matrix1 = tf.matmul(x, e)
        unit_matrix = tf.convert_to_tensor(unit_matrix1)
        self.weight_unit ,self.bias_unit = self._emb(dim, time_len)
        
        x1 = tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit)
        x_output = tf.add(x1, self.bias_unit)
        return x_output
    
    def _emb(self, dim, time_len):
        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):
            weight_unit = tf.get_variable(name = 'weight_unit', shape = (self.dim, self.num_nodes), dtype = tf.float32)
            bias_unit = tf.get_variable(name = 'bias_unit', shape =(time_len,1), initializer = tf.constant_initializer(dtype=tf.float32))
        w = weight_unit
        b = bias_unit
        return w, b

class Unit1():
    def __init__(self, dim, num_nodes, reuse = None):
        self.dim = dim
        self.num_nodes = num_nodes
    def call(self, inputs, time_len):
        x, e = inputs  
        unit_matrix1 = tf.matmul(x, e)
        unit_matrix=tf.convert_to_tensor(unit_matrix1)
        self.weight_unit1 ,self.bias_unit1 = self._emb(dim, time_len)
        
        x1=tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit1)
        x_output= tf.add(x1, self.bias_unit1)
        return x_output
    
    def _emb(self, dim, time_len):
        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):
            weight_unit1 = tf.get_variable(name = 'weight_unit1', shape = (self.dim, self.num_nodes), dtype = tf.float32)
            bias_unit1 = tf.get_variable(name = 'bias_unit1', shape =(time_len,1), initializer = tf.constant_initializer(dtype=tf.float32))
        w1 = weight_unit1
        bb = bias_unit1
        return w1, bb

class Unit2():
    def __init__(self, dim, num_nodes, time_len, reuse = None):
        self.dim = dim
        self.num_nodes = num_nodes
        self.time_len = time_len
    def call(self, inputs, time_len):
        x, e = inputs
        x = np.transpose(x)
        x = x.astype(np.float64)
        unit_matrix1 = tf.matmul(x, e)
        unit_matrix = tf.convert_to_tensor(unit_matrix1)
        self.weight_unit ,self.bias_unit = self._emb(dim, time_len)
        
        x1 = tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit)
        self.x_output = tf.add(x1, self.bias_unit)
        self.x_output = tf.transpose(self.x_output)
        return self.x_output
    
    def _emb(self, dim, time_len):
        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):
            self.weight_unit = tf.get_variable(name = 'weight_unit', shape = (self.dim, self.time_len), dtype = tf.float32)
            self.bias_unit = tf.get_variable(name = 'bias_unit', shape =(self.num_nodes,1), initializer = tf.constant_initializer(dtype=tf.float32))
        self.w = self.weight_unit
        self.b = self.bias_unit
        return self.w, self.b

class Unit3():
    def __init__(self, dim, num_nodes, time_len, reuse = None):
        self.dim = dim
        self.num_nodes = num_nodes
        self.time_len = time_len
    def call(self, inputs, time_len):
        x, e = inputs
        x = np.transpose(x)
        x = x.astype(np.float64)
        unit_matrix1 = tf.matmul(x, e)
        unit_matrix = tf.convert_to_tensor(unit_matrix1)
        self.weight_unit ,self.bias_unit = self._emb(dim, time_len)
        
        x1 = tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit)
        x_output = tf.add(x1, self.bias_unit)
        x_output = tf.transpose(x_output)
        return x_output
    
    def _emb(self, dim, time_len):
        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):
            weight_unit = tf.get_variable(name = 'weight_unit', shape = (self.dim, self.time_len), dtype = tf.float32)
            bias_unit = tf.get_variable(name = 'bias_unit', shape =(self.num_nodes,1), initializer = tf.constant_initializer(dtype=tf.float32))
        w = weight_unit
        b = bias_unit
        return w, b

class Unit4():
    def __init__(self, dim, num_nodes, reuse = None):
        self.dim = dim
        self.num_nodes = num_nodes
    def call(self, inputs, time_len):
        x, e = inputs  
        unit_matrix1 = tf.matmul(x, e)
        unit_matrix = tf.convert_to_tensor(unit_matrix1)
        self.weight_unit ,self.bias_unit = self._emb(dim, time_len)
        
        x1 = tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit)
        x_output = tf.add(x1, self.bias_unit)
        return x_output
    
    def _emb(self, dim, time_len):
        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):
            weight_unit = tf.get_variable(name = 'weight_unit', shape = (self.dim, self.num_nodes), dtype = tf.float32)
            bias_unit = tf.get_variable(name = 'bias_unit', shape =(time_len,1), initializer = tf.constant_initializer(dtype=tf.float32))
        w = weight_unit
        b = bias_unit
        return w, b

class Unit5():
    def __init__(self, dim, num_nodes, reuse = None):
        self.dim = dim
        self.num_nodes = num_nodes
    def call(self, inputs, time_len):
        x, e = inputs  
        unit_matrix1 = tf.matmul(x, e)
        unit_matrix=tf.convert_to_tensor(unit_matrix1)
        self.weight_unit1 ,self.bias_unit1 = self._emb(dim, time_len)
        
        x1=tf.matmul(tf.cast(unit_matrix,tf.float32),self.weight_unit1)
        self.x_output= tf.add(x1, self.bias_unit1)
        return self.x_output
    
    def _emb(self, dim, time_len):
        with tf.variable_scope('a',reuse = tf.AUTO_REUSE):
            self.weight_unit1 = tf.get_variable(name = 'weight_unit1', shape = (self.dim, self.num_nodes), dtype = tf.float32)
            self.bias_unit1 = tf.get_variable(name = 'bias_unit1', shape =(time_len,1), initializer = tf.constant_initializer(dtype=tf.float32))
        self.w1 = self.weight_unit1
        self.bb = self.bias_unit1
        return self.w1, self.bb

print("âœ… ê°œì„ ëœ acell.py ë¡œë“œ ì™„ë£Œ!")