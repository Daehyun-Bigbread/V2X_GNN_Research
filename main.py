#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
V2X AST-GCN ì´ìƒíƒì§€ ë©”ì¸ ì‹¤í–‰ íŒŒì¼ - ê³ ì • ì„ê³„ê°’ ì•ˆì •í™” ë²„ì „

í•µì‹¬ ê°œì„ ì‚¬í•­:
1. ê³ ì • ì„ê³„ê°’ ì‚¬ìš© (0.3) - í•™ìŠµ ì¤‘ ë³€ê²½ ê¸ˆì§€
2. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì œí•œ (ìµœëŒ€ 3.0)
3. ì•ˆì •í™”ëœ í‰ê°€ í•¨ìˆ˜
4. ì¼ê´€ëœ ë°ì´í„° ì²˜ë¦¬

Author: V2X Anomaly Detection Team (Stabilized)
Date: 2025-06-07
"""

import pickle as pkl
import tensorflow as tf
import os
import time
import numpy as np
import tensorflow.compat.v1 as tf_v1
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# ğŸ¯ ê³ ì • ì„ê³„ê°’ ì„¤ì • (í•µì‹¬)
FIXED_THRESHOLD = 0.3  # í•™ìŠµ ì¤‘ ì ˆëŒ€ ë³€ê²½ ì•ˆí•¨
MAX_CLASS_WEIGHT = 3.0  # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì œí•œ

print(f"ğŸ¯ ì•ˆì •í™” ì„¤ì •:")
print(f"   ê³ ì • ì„ê³„ê°’: {FIXED_THRESHOLD}")
print(f"   ìµœëŒ€ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {MAX_CLASS_WEIGHT}")

# Apple GPU (Metal) ì„¤ì •
print("ğŸ” GPU ì„¤ì • ì¤‘...")
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print(f"ğŸš€ Apple GPU (Metal) í™œì„±í™” ì„±ê³µ! - {gpus[0]}")
    except RuntimeError as e:
        print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
else:
    print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")

# TF 1.x í˜¸í™˜ ëª¨ë“œë¡œ ì „í™˜
import tensorflow.compat.v1 as tf_v1
tf_v1.disable_v2_behavior()

import pandas as pd
import numpy as np
import math
import os
import numpy.linalg as la
from acell import preprocess_data, load_assist_data, load_v2x_data
from tgcn import tgcnCell

from visualization import plot_result, plot_error
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, confusion_matrix, classification_report)
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import time

time_start = time.time()

###### Settings ######
flags = tf_v1.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_float('learning_rate', 0.0001, 'Initial learning rate.')
flags.DEFINE_integer('training_epoch', 50, 'Number of epochs to train.')
flags.DEFINE_integer('gru_units', 64, 'hidden units of gru.')
flags.DEFINE_integer('seq_len', 10, 'time length of inputs.')
flags.DEFINE_integer('pre_len', 3, 'time length of prediction.')
flags.DEFINE_float('train_rate', 0.8, 'rate of training set.')
flags.DEFINE_integer('batch_size', 32, 'batch size.')
flags.DEFINE_string('dataset', 'v2x', 'dataset')
flags.DEFINE_string('model_name', 'ast-gcn', 'ast-gcn')
flags.DEFINE_integer('scheme', 3, 'scheme')
flags.DEFINE_string('noise_name', 'None', 'None or Gauss or Possion')
flags.DEFINE_float('noise_param', 0, 'Parameter for noise')

model_name = FLAGS.model_name
noise_name = FLAGS.noise_name
data_name = FLAGS.dataset
train_rate = FLAGS.train_rate
seq_len = FLAGS.seq_len
output_dim = pre_len = FLAGS.pre_len
batch_size = FLAGS.batch_size
lr = FLAGS.learning_rate
training_epoch = FLAGS.training_epoch
gru_units = FLAGS.gru_units
scheme = FLAGS.scheme
PG = FLAGS.noise_param

###### load data ######
print(f"ğŸš¨ V2X ì´ìƒíƒì§€ ë°ì´í„° ë¡œë”©: {data_name}")

if data_name == 'v2x':
    data, adj, poi_data, weather_data = load_v2x_data('v2x')
    print(f"âœ… V2X ì´ìƒíƒì§€ ë°ì´í„° ë¡œë”© ì™„ë£Œ")
elif data_name == 'sz':
    data, adj = load_assist_data('sz')
    poi_data, weather_data = None, None
    print(f"âœ… Shenzhen ë°ì´í„° ë¡œë”© ì™„ë£Œ")
else:
    raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {data_name}")

### Perturbation Analysis (ê¸°ì¡´ê³¼ ë™ì¼)
def MaxMinNormalization(x, Max, Min):
    x = (x - Min) / (Max - Min)
    return x

if noise_name == 'Gauss':
    Gauss = np.random.normal(0, PG, size=data.shape)
    noise_Gauss = MaxMinNormalization(Gauss, np.max(Gauss), np.min(Gauss))
    data = data + noise_Gauss
elif noise_name == 'Possion':
    Possion = np.random.poisson(PG, size=data.shape)
    noise_Possion = MaxMinNormalization(Possion, np.max(Possion), np.min(Possion))
    data = data + noise_Possion
else:
    data = data

time_len = data.shape[0]
num_nodes = data.shape[1]
data1 = np.mat(data, dtype=np.float32)

#### normalization (ì´ìƒì ìˆ˜ëŠ” ì´ë¯¸ 0-1 ë²”ìœ„ì´ë¯€ë¡œ ìµœì†Œí•œì˜ ì •ê·œí™”)
max_value = np.max(data1)
if max_value > 0:
    data1 = data1 / max_value
else:
    print("âš ï¸ ëª¨ë“  ì´ìƒì ìˆ˜ê°€ 0ì…ë‹ˆë‹¤. ì •ê·œí™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

if hasattr(data, 'columns'):
    data1.columns = data.columns

# ëª¨ë¸ ë° ìŠ¤í‚´ ì •ë³´ ì¶œë ¥
if model_name == 'ast-gcn':
    if scheme == 1:
        name = 'V2X POI only (Anomaly Detection)' if data_name == 'v2x' else 'add poi dim'
    elif scheme == 2:
        name = 'V2X Weather only (Anomaly Detection)' if data_name == 'v2x' else 'add weather dim'
    else:
        name = 'V2X POI + Weather (Anomaly Detection)' if data_name == 'v2x' else 'add poi + weather dim'
else:
    name = 'tgcn (Anomaly Detection)'

print('ğŸ“Š ì´ìƒíƒì§€ ëª¨ë¸ ì •ë³´:')
print(f'   task: anomaly_detection')
print(f'   model: {model_name}')
print(f'   dataset: {data_name}')
print(f'   scheme: {scheme} ({name})')
print(f'   data shape: {data1.shape}')
print(f'   time_len: {time_len}, num_nodes: {num_nodes}')
print(f'   noise_name: {noise_name}')
print(f'   noise_param: {PG}')

# ê°œì„ ëœ ì „ì²˜ë¦¬ (í´ë˜ìŠ¤ ê· í˜• ì •ë³´ í¬í•¨)
print(f"\nğŸ”„ ì´ìƒíƒì§€ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")

try:
    # ê°œì„ ëœ ì „ì²˜ë¦¬ í˜¸ì¶œ (balance_info ì¶”ê°€ ë°˜í™˜)
    trainX, trainY, testX, testY, balance_info = preprocess_data(
        data1, time_len, train_rate, seq_len, pre_len, model_name, scheme,
        poi_data, weather_data
    )
    
    # í´ë˜ìŠ¤ ê· í˜• ì •ë³´ ì¶”ì¶œ
    pos_weight = balance_info['pos_weight']
    threshold_used = balance_info['threshold_used']
    train_anomaly_ratio = balance_info['train_anomaly_ratio']
    test_anomaly_ratio = balance_info['test_anomaly_ratio']
    
    print(f"ğŸ¯ í´ë˜ìŠ¤ ê· í˜• ì •ë³´:")
    print(f"   ì–‘ì„± í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f}")
    print(f"   ì‚¬ìš©ëœ ì„ê³„ê°’: {threshold_used:.3f}")
    print(f"   í›ˆë ¨ ì´ìƒ ë¹„ìœ¨: {train_anomaly_ratio:.2%}")
    print(f"   í…ŒìŠ¤íŠ¸ ì´ìƒ ë¹„ìœ¨: {test_anomaly_ratio:.2%}")
    
except (ValueError, TypeError) as e:
    # ê¸°ì¡´ ì „ì²˜ë¦¬ í´ë°±
    print(f"âš ï¸ ê°œì„ ëœ ì „ì²˜ë¦¬ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©: {e}")
    trainX, trainY, testX, testY = preprocess_data(
        data1, time_len, train_rate, seq_len, pre_len, model_name, scheme,
        poi_data, weather_data
    )
    
    # ìˆ˜ë™ìœ¼ë¡œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    y_flat = trainY.flatten()
    pos_count = np.sum(y_flat > 0.3)
    neg_count = len(y_flat) - pos_count
    pos_weight = np.clip(neg_count / (pos_count + 1e-8), 2.0, 15.0)
    threshold_used = 0.3
    train_anomaly_ratio = pos_count / len(y_flat)
    test_anomaly_ratio = (testY.flatten() > 0.3).mean()
    
    print(f"ğŸ¯ ê¸°ë³¸ í´ë˜ìŠ¤ ê· í˜• ì •ë³´:")
    print(f"   ê³„ì‚°ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f}")
    print(f"   ì‚¬ìš©ëœ ì„ê³„ê°’: {threshold_used:.3f}")

totalbatch = int(trainX.shape[0] / batch_size)
training_data_count = len(trainX)

print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ:")
print(f"   total batches: {totalbatch}")
print(f"   training samples: {training_data_count}")

def create_balanced_loss_function(pos_weight=10.0, use_focal=True):
    """
    í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ í•´ê²°í•˜ëŠ” ê· í˜• ì†ì‹¤í•¨ìˆ˜ ìƒì„±
    """
    
    def focal_loss(y_true, y_pred, alpha=0.75, gamma=2.0):
        """Focal Loss êµ¬í˜„"""
        y_true = tf_v1.cast(y_true, tf_v1.float32)
        
        # Sigmoid ì ìš©
        y_pred_sigmoid = tf_v1.nn.sigmoid(y_pred)
        y_pred_sigmoid = tf_v1.clip_by_value(y_pred_sigmoid, 1e-8, 1.0 - 1e-8)
        
        # Cross Entropy
        ce_loss = -y_true * tf_v1.math.log(y_pred_sigmoid) - (1 - y_true) * tf_v1.math.log(1 - y_pred_sigmoid)
        
        # Focal Weight (gamma ìœ ì§€)
        pt = tf_v1.where(tf_v1.equal(y_true, 1), y_pred_sigmoid, 1 - y_pred_sigmoid)
        focal_weight = alpha * tf_v1.pow(1 - pt, gamma)
        
        return focal_weight * ce_loss
    
    def balanced_loss(y_true, y_pred):
        """ê· í˜• ì†ì‹¤í•¨ìˆ˜"""
        
        # 1. Weighted Binary Cross Entropy (pos_weight ì¦ê°€)
        weighted_bce = tf_v1.nn.weighted_cross_entropy_with_logits(
            labels=y_true,
            logits=y_pred,
            pos_weight=pos_weight * 1.5
        )
        
        if use_focal:
            # 2. Focal Loss ì¶”ê°€
            focal_loss_val = focal_loss(y_true, y_pred, alpha=0.75, gamma=2.0)
            
            # 3. ê²°í•© (BCE 70%, Focal 30%)
            combined_loss = 0.7 * weighted_bce + 0.3 * focal_loss_val
        else:
            combined_loss = weighted_bce
        
        return tf_v1.reduce_mean(combined_loss)
    
    return balanced_loss

def find_optimal_threshold_detailed(y_true, y_pred_proba, thresholds=None):
    """
    ìƒì„¸í•œ ìµœì  ì„ê³„ê°’ íƒìƒ‰
    """
    if thresholds is None:
        thresholds = np.arange(0.1, 0.9, 0.05)
    
    # ë°ì´í„° í‰í‰í™”
    y_true_flat = y_true.flatten()
    y_pred_proba_flat = y_pred_proba.flatten()
    
    results = []
    best_f1 = 0
    best_threshold = 0.5
    
    print("ğŸ” ì„ê³„ê°’ë³„ ì„±ëŠ¥ ë¶„ì„:")
    print("-" * 70)
    print(f"{'Threshold':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}")
    print("-" * 70)
    
    for threshold in thresholds:
        # ì´ì§„ ì˜ˆì¸¡
        y_pred_binary = (y_pred_proba_flat > threshold).astype(int)
        y_true_binary = (y_true_flat > threshold_used).astype(int)
        
        # í˜¼ë™í–‰ë ¬ ê³„ì‚°
        tp = np.sum((y_true_binary == 1) & (y_pred_binary == 1))
        fp = np.sum((y_true_binary == 0) & (y_pred_binary == 1))
        fn = np.sum((y_true_binary == 1) & (y_pred_binary == 0))
        tn = np.sum((y_true_binary == 0) & (y_pred_binary == 0))
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        precision = tp / (tp + fp + 1e-8)
        recall = tp / (tp + fn + 1e-8)
        f1 = 2 * precision * recall / (precision + recall + 1e-8)
        
        print(f"{threshold:<10.2f} {precision:<10.4f} {recall:<10.4f} {f1:<10.4f}")
        
        # ìµœì  F1 ì ìˆ˜ ì°¾ê¸°
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    print("-" * 70)
    print(f"ğŸ¯ ìµœì  ì„ê³„ê°’: {best_threshold:.2f} (F1: {best_f1:.4f})")
    
    return best_threshold, best_f1

def evaluate_anomaly_detection_improved(y_true, y_pred_logits, threshold=None):
    """
    ê°œì„ ëœ ì´ìƒíƒì§€ í‰ê°€
    """
    # í™•ë¥ ë¡œ ë³€í™˜
    y_pred_proba = tf_v1.nn.sigmoid(y_pred_logits).eval(session=sess)
    
    # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (ì²« ë²ˆì§¸ í‰ê°€ì—ì„œë§Œ)
    if threshold is None:
        threshold, _ = find_optimal_threshold_detailed(y_true, y_pred_proba)
    
    # ì´ì§„ ì˜ˆì¸¡
    y_pred_binary = (y_pred_proba > threshold).astype(int)
    
    # ë°ì´í„° í‰í‰í™”
    y_true_flat = y_true.flatten()
    y_pred_flat = y_pred_binary.flatten()
    y_pred_proba_flat = y_pred_proba.flatten()
    
    # ì‹¤ì œ ë¼ë²¨ ì´ì§„í™” (threshold_used ê¸°ì¤€)
    y_true_binary = (y_true_flat > threshold_used).astype(int)
    
    # í˜¼ë™í–‰ë ¬
    tp = np.sum((y_true_binary == 1) & (y_pred_flat == 1))
    fp = np.sum((y_true_binary == 0) & (y_pred_flat == 1))
    fn = np.sum((y_true_binary == 1) & (y_pred_flat == 0))
    tn = np.sum((y_true_binary == 0) & (y_pred_flat == 0))
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    accuracy = (tp + tn) / (tp + fp + fn + tn + 1e-8)
    precision = tp / (tp + fp + 1e-8)
    recall = tp / (tp + fn + 1e-8)
    f1 = 2 * precision * recall / (precision + recall + 1e-8)
    
    # AUC ê³„ì‚°
    try:
        if len(np.unique(y_true_binary)) > 1:
            auc = roc_auc_score(y_true_binary, y_pred_proba_flat)
        else:
            auc = 0.5
    except:
        auc = 0.5
    
    # ìƒì„¸ ì¶œë ¥
    print(f"ğŸ” ê°œì„ ëœ ì´ìƒíƒì§€ í‰ê°€ (ì„ê³„ê°’: {threshold:.3f}):")
    print(f"   ğŸ“Š í˜¼ë™í–‰ë ¬: TP={tp}, FP={fp}, FN={fn}, TN={tn}")
    print(f"   ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
    print(f"     Accuracy: {accuracy:.4f}")
    print(f"     Precision: {precision:.4f}")
    print(f"     Recall: {recall:.4f}")
    print(f"     F1-Score: {f1:.4f}")
    print(f"     AUC: {auc:.4f}")
    print(f"   ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:")
    print(f"     ì‹¤ì œ ì´ìƒ ë¹„ìœ¨: {np.mean(y_true_binary):.3f}")
    print(f"     ì˜ˆì¸¡ ì´ìƒ ë¹„ìœ¨: {np.mean(y_pred_flat):.3f}")
    
    return accuracy, precision, recall, f1, auc, threshold

def evaluate_anomaly_numpy(y_true, y_logits, threshold=None, threshold_used=0.3):
    """
    NumPy ê¸°ë°˜ ì´ìƒíƒì§€ í‰ê°€
    """
    # 1) ë¡œì§“ â†’ í™•ë¥ 
    y_proba = 1.0 / (1.0 + np.exp(-y_logits))
    y_proba_flat = y_proba.flatten()
    y_true_flat = y_true.flatten()
    
    # 2) y_true ì´ì§„í™”
    y_true_bin = (y_true_flat > threshold_used).astype(int)
    
    # 3) threshold íƒìƒ‰ (Noneì´ë©´)
    if threshold is None:
        best_f1, best_thr = 0.0, 0.5
        for thr in np.arange(0.3, 0.7, 0.01):
            y_pred_bin = (y_proba_flat > thr).astype(int)
            if len(np.unique(y_true_bin)) < 2 or len(np.unique(y_pred_bin)) < 2:
                continue
            try:
                f1 = f1_score(y_true_bin, y_pred_bin, zero_division=0)
                if f1 > best_f1:
                    best_f1, best_thr = f1, thr
            except:
                continue
        threshold = best_thr
    
    # 4) ìµœì¢… ì´ì§„ ì˜ˆì¸¡
    y_pred_bin = (y_proba_flat > threshold).astype(int)
    
    # 5) ë©”íŠ¸ë¦­ ê³„ì‚°
    acc   = accuracy_score(y_true_bin, y_pred_bin)
    pre   = precision_score(y_true_bin, y_pred_bin, zero_division=0)
    rec   = recall_score(y_true_bin, y_pred_bin, zero_division=0)
    f1    = f1_score(y_true_bin, y_pred_bin, zero_division=0)
    try:
        auc  = roc_auc_score(y_true_bin, y_proba_flat)
    except:
        auc  = 0.5
    
    return acc, pre, rec, f1, auc, threshold

def TGCN_ANOMALY(_X, _weights, _biases):
    """
    TGCN ì´ìƒíƒì§€ìš© ëª¨ë¸ (ê¸°ì¡´ TGCN + ì‹œê·¸ëª¨ì´ë“œ ì¶œë ¥)
    """
    ###
    cell_1 = tgcnCell(gru_units, adj, num_nodes=num_nodes)
    cell = tf_v1.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)
    _X = tf_v1.unstack(_X, axis=1)
    outputs, states = tf_v1.nn.static_rnn(cell, _X, dtype=tf_v1.float32)
    m = []
    for i in outputs:
        o = tf_v1.reshape(i, shape=[-1, num_nodes, gru_units])
        o = tf_v1.reshape(o, shape=[-1, gru_units])
        # Dropout ë¹„ìœ¨ ì¦ê°€
        o = tf_v1.nn.dropout(o, keep_prob=0.7)
        m.append(o)
    last_output = m[-1]
    
    # ì´ìƒíƒì§€ìš© ì¶œë ¥ (ì‹œê·¸ëª¨ì´ë“œ)
    logits = tf_v1.matmul(last_output, _weights['out']) + _biases['out']
    logits = tf_v1.reshape(logits, shape=[-1, num_nodes, pre_len])
    logits = tf_v1.transpose(logits, perm=[0, 2, 1])
    logits = tf_v1.reshape(logits, shape=[-1, num_nodes])
    
    # ìˆ˜ì¹˜ ì•ˆì •ì„± ì¶”ê°€ (í´ë¦¬í•‘ ë²”ìœ„ ì¡°ì •)
    logits = tf_v1.clip_by_value(logits, -12.0, 12.0)
    
    # ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” (ì´ìƒ í™•ë¥ )
    output = tf_v1.sigmoid(logits)
    
    return output, logits, m, states

def simple_evaluation(y_true, y_pred, threshold=0.5):
    """ê°„ë‹¨í•œ ì´ìƒíƒì§€ í‰ê°€"""
    try:
        y_true_flat = y_true.flatten()
        y_pred_flat = y_pred.flatten()
        
        # thresholdê°€ Noneì´ë©´ ê¸°ë³¸ê°’ 0.5
        if threshold is None:
            threshold = 0.5
        # (1) y_true, (2) y_pred ëª¨ë‘ ê°™ì€ threshold ì‚¬ìš©
        y_true_bin = (y_true_flat > threshold).astype(int)
        y_pred_bin = (y_pred_flat >= threshold).astype(int)
        
        acc = accuracy_score(y_true_bin, y_pred_bin)
        
        if len(np.unique(y_true_bin)) > 1:
            pre = precision_score(y_true_bin, y_pred_bin, zero_division=0)
            rec = recall_score(y_true_bin, y_pred_bin, zero_division=0)
            f1 = f1_score(y_true_bin, y_pred_bin, zero_division=0)
        else:
            pre = rec = f1 = 0.0
            
        return acc, pre, rec, f1, threshold
    except Exception as e:
        print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 0.5, 0.0, 0.0, 0.0, 0.5

def evaluate_anomaly_stable(y_true, y_pred, threshold=FIXED_THRESHOLD):
    """
    ê³ ì • ì„ê³„ê°’ ê¸°ë°˜ ì•ˆì •ì  ì´ìƒíƒì§€ í‰ê°€
    """
    try:
        # ë°ì´í„° í‰í‰í™”
        y_true_flat = y_true.flatten()
        y_pred_flat = y_pred.flatten()
        
        # NaN/Inf ì•ˆì „ ì²˜ë¦¬
        y_pred_flat = np.nan_to_num(y_pred_flat, 0.0)
        y_pred_flat = np.clip(y_pred_flat, 0.0, 1.0)
        
        # ê³ ì • ì„ê³„ê°’ìœ¼ë¡œ ì´ì§„í™”
        y_true_bin = (y_true_flat > threshold).astype(int)
        y_pred_bin = (y_pred_flat > threshold).astype(int)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        acc = accuracy_score(y_true_bin, y_pred_bin)
        
        # í´ë˜ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ê³„ì‚°
        if len(np.unique(y_true_bin)) > 1:
            pre = precision_score(y_true_bin, y_pred_bin, zero_division=0)
            rec = recall_score(y_true_bin, y_pred_bin, zero_division=0)
            f1 = f1_score(y_true_bin, y_pred_bin, zero_division=0)
            try:
                auc = roc_auc_score(y_true_bin, y_pred_flat)
            except:
                auc = 0.5
        else:
            pre = rec = f1 = auc = 0.0
        
        return acc, pre, rec, f1, auc
    except Exception as e:
        print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        return 0.5, 0.0, 0.0, 0.0, 0.5

def create_stable_loss_function(pos_weight=2.0):
    """
    ì•ˆì •í™”ëœ ê· í˜• ì†ì‹¤í•¨ìˆ˜ (ê°€ì¤‘ì¹˜ ì œí•œ)
    """
    def stable_loss(y_true, y_pred):
        # Weighted Binary Cross Entropy (ì œí•œëœ ê°€ì¤‘ì¹˜)
        weighted_bce = tf_v1.nn.weighted_cross_entropy_with_logits(
            labels=y_true,
            logits=y_pred,
            pos_weight=pos_weight
        )
        
        # ê°€ë²¼ìš´ Focal Loss ì¶”ê°€
        y_pred_sigmoid = tf_v1.nn.sigmoid(y_pred)
        y_pred_sigmoid = tf_v1.clip_by_value(y_pred_sigmoid, 1e-8, 1.0 - 1e-8)
        
        ce_loss = -y_true * tf_v1.math.log(y_pred_sigmoid) - (1 - y_true) * tf_v1.math.log(1 - y_pred_sigmoid)
        pt = tf_v1.where(tf_v1.equal(y_true, 1), y_pred_sigmoid, 1 - y_pred_sigmoid)
        focal_weight = 0.25 * tf_v1.pow(1 - pt, 2.0)
        focal_loss = focal_weight * ce_loss
        
        # ê²°í•© (BCE 80%, Focal 20%)
        combined_loss = 0.8 * weighted_bce + 0.2 * focal_loss
        
        return tf_v1.reduce_mean(combined_loss)
    
    return stable_loss

###### placeholders ######
print(f"\nğŸ§  ê°œì„ ëœ ì´ìƒíƒì§€ ëª¨ë¸ êµ¬ì„±...")

# ì‹¤ì œ ë°ì´í„° ì°¨ì› í™•ì¸
print(f"   ğŸ“Š ì‹¤ì œ ë°ì´í„° í˜•íƒœ:")
print(f"     trainX: {trainX.shape}")
print(f"     trainY: {trainY.shape}")
print(f"     seq_len: {seq_len}")
print(f"     pre_len: {pre_len}")
print(f"     num_nodes: {num_nodes}")

# ì•ˆì „í•œ placeholder ìƒì„±
actual_seq_len = trainX.shape[1]
actual_num_nodes = trainX.shape[2]
actual_pre_len = trainY.shape[1]

print(f"   ğŸ”§ Placeholder ì°¨ì›:")
print(f"     ì…ë ¥: [{None}, {actual_seq_len}, {actual_num_nodes}]")
print(f"     ë¼ë²¨: [{None}, {actual_pre_len}, {actual_num_nodes}]")

# ë™ì  ì°¨ì›ìœ¼ë¡œ placeholder ìƒì„±
inputs = tf_v1.placeholder(tf_v1.float32, shape=[None, actual_seq_len, actual_num_nodes])
labels = tf_v1.placeholder(tf_v1.float32, shape=[None, actual_pre_len, actual_num_nodes])
optimal_threshold = tf_v1.placeholder(tf_v1.float32, shape=())

# Graph weights (ì´ìƒíƒì§€ìš©)
weights = {
    'out': tf_v1.Variable(tf_v1.random_normal([gru_units, actual_pre_len], mean=0.0, stddev=0.01), name='weight_o')}

biases = {
    'out': tf_v1.Variable(tf_v1.random_normal([actual_pre_len], stddev=0.01), name='bias_o')}

pred, logits, ttts, ttto = TGCN_ANOMALY(inputs, weights, biases)

y_pred = pred  # ì‹œê·¸ëª¨ì´ë“œ ì¶œë ¥ (0-1 í™•ë¥ )
y_logits = logits  # ë¡œì§€íŠ¸ (ì†ì‹¤ ê³„ì‚°ìš©)

###### optimizer (ê°œì„ ëœ ì´ìƒíƒì§€ìš© ì†ì‹¤í•¨ìˆ˜) ######
lambda_loss = 0.0015
Lreg = lambda_loss * sum(tf_v1.nn.l2_loss(tf_var) for tf_var in tf_v1.trainable_variables())
label = tf_v1.reshape(labels, [-1, actual_num_nodes])
logits_reshaped = tf_v1.reshape(y_logits, [-1, actual_num_nodes])

print('y_pred_shape:', y_pred.shape)
print('label_shape:', label.shape)
print('logits_shape:', logits_reshaped.shape)

# ê°œì„ ëœ ê· í˜• ì†ì‹¤í•¨ìˆ˜ ì‚¬ìš©
print(f"ğŸ¯ ê°œì„ ëœ ì†ì‹¤í•¨ìˆ˜ êµ¬ì„± (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f})")
balanced_loss_fn = create_balanced_loss_function(pos_weight=pos_weight, use_focal=True)
main_loss = balanced_loss_fn(label, logits_reshaped)
loss = main_loss + Lreg

# TF ê·¸ë˜í”„ì—ì„œ accuracy ê³„ì‚°
predictions = tf_v1.cast(tf_v1.greater(y_pred, optimal_threshold), tf_v1.float32)
accuracy = tf_v1.reduce_mean(tf_v1.cast(tf_v1.equal(predictions, tf_v1.reshape(labels, [-1, actual_num_nodes])), tf_v1.float32))

# ê°œì„ ëœ ìµœì í™”ê¸° (í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§)
global_step = tf_v1.Variable(0, trainable=False)
learning_rate = tf_v1.train.exponential_decay(
    lr, global_step, 
    decay_steps=500,  # ë” ë¹ ë¥¸ ê°ì†Œ
    decay_rate=0.9,   # ë” ê¸‰ê²©í•œ ê°ì†Œ
    staircase=True
)

# Gradient clipping (ê°œì„ )
opt = tf_v1.train.AdamOptimizer(learning_rate)
grads_and_vars = opt.compute_gradients(loss)
clipped_grads_and_vars = [(tf_v1.clip_by_value(grad, -0.5, 0.5), var)  # ë” ì¢ì€ í´ë¦¬í•‘ ë²”ìœ„
                          for grad, var in grads_and_vars if grad is not None]
optimizer = opt.apply_gradients(clipped_grads_and_vars, global_step=global_step)

###### Initialize session ######
variables = tf_v1.global_variables()
saver = tf_v1.train.Saver(tf_v1.global_variables())

# GPU ì„¤ì •
config = tf_v1.ConfigProto()
config.allow_soft_placement = True
config.gpu_options.allow_growth = True

sess = tf_v1.Session(config=config)
sess.run(tf_v1.global_variables_initializer())

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • (ì´ìƒíƒì§€ìš©)
if data_name == 'v2x':
    out = f'out/v2x_{model_name}_anomaly_scheme{scheme}_gpu'
else:
    out = f'out/{model_name}_anomaly_{noise_name}_gpu'

path1 = f'{model_name}_anomaly_{name}_{data_name}_lr{lr}_batch{batch_size}_unit{gru_units}_seq{seq_len}_pre{pre_len}_epoch{training_epoch}_scheme{scheme}_PG{PG}_GPU'
path = os.path.join(out, path1)
if not os.path.exists(path):
    os.makedirs(path)

print(f"ğŸ“‚ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {path}")

print(f"\nğŸš€ V2X AST-GCN ê°œì„ ëœ ì´ìƒíƒì§€ GPU í•™ìŠµ ì‹œì‘...")
print(f"   Epochs: {training_epoch}")
print(f"   Batch size: {batch_size}")
print(f"   Learning rate: {lr} (ì§€ìˆ˜ ê°ì†Œ)")
print(f"   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f}")
print(f"   ì„ê³„ê°’: {threshold_used:.3f}")

x_axe, batch_loss, batch_acc = [], [], []
test_loss, test_acc, test_precision, test_recall, test_f1, test_auc, test_pred = [], [], [], [], [], [], []

best_f1 = 0.0
patience_counter = 0
early_stopping_patience = 15  # ë” ê¸´ ì¸ë‚´ì‹¬

for epoch in range(training_epoch):
    epoch_start_time = time.time()
    
    # í•™ìŠµë¥  ì¶œë ¥ (5 ì—í¬í¬ë§ˆë‹¤)
    if epoch % 5 == 0:
        current_lr = sess.run(learning_rate)
        print(f"   ğŸ“‰ í˜„ì¬ í•™ìŠµë¥ : {current_lr:.6f}")
    
    # ë°°ì¹˜ë³„ í•™ìŠµ
    epoch_batch_loss, epoch_batch_acc = [], []
    for m in range(totalbatch):
        mini_batch = trainX[m * batch_size : (m+1) * batch_size]
        mini_label = trainY[m * batch_size : (m+1) * batch_size]
        
        # NaN ì²´í¬
        if np.isnan(mini_batch).any() or np.isnan(mini_label).any():
            print(f"âš ï¸ Epoch {epoch}, Batch {m}: NaN ë°œê²¬, ê±´ë„ˆë›°ê¸°")
            continue
            
        # í•™ìŠµ ì‹¤í–‰
        _, loss1 = sess.run([optimizer, loss],
                           feed_dict={inputs: mini_batch, labels: mini_label})
        
        if np.isnan(loss1):
            print(f"âš ï¸ Epoch {epoch}, Batch {m}: í•™ìŠµ ì¤‘ NaN ë°œìƒ")
            continue
            
        epoch_batch_loss.append(loss1)
    
    # ì—í¬í¬ í‰ê·  ê³„ì‚°
    if epoch_batch_loss:
        epoch_train_loss = np.mean(epoch_batch_loss)
    else:
        epoch_train_loss = 0.0
    
    # í…ŒìŠ¤íŠ¸ í‰ê°€ (ì•ˆì •í™”ëœ ë°©ì‹)
    try:
        # ì†ì‹¤ê³¼ ì˜ˆì¸¡ê°’ ê³„ì‚°
        loss2, test_output = sess.run(
            [loss, y_pred],
            feed_dict={inputs: testX, labels: testY}
        )
        
        # í™•ë¥ ê°’ ì•ˆì „ ì²˜ë¦¬
        test_prob = np.clip(test_output, 0.0, 1.0)
        
        # ğŸ¯ í•µì‹¬: ê³ ì • ì„ê³„ê°’ìœ¼ë¡œ í‰ê°€
        accuracy_val, precision_val, recall_val, f1_val, auc_val = evaluate_anomaly_stable(
            testY.reshape(-1, actual_num_nodes), 
            test_prob
        )
        
        # ê²°ê³¼ ì €ì¥
        test_loss.append(loss2)
        test_acc.append(accuracy_val)
        test_precision.append(precision_val)
        test_recall.append(recall_val)
        test_f1.append(f1_val)
        test_auc.append(auc_val)
        test_pred.append(test_prob)
        
        # ì¶œë ¥ (ê³ ì • ì„ê³„ê°’ í‘œì‹œ)
        epoch_time = time.time() - epoch_start_time
        print(f"Epoch:{epoch:2d}",
              f"train_loss:{epoch_train_loss:.4f}",
              f"test_loss:{loss2:.4f}",
              f"test_acc:{accuracy_val:.4f}",
              f"test_pre:{precision_val:.4f}",
              f"test_rec:{recall_val:.4f}",
              f"test_f1:{f1_val:.4f}",
              f"test_auc:{auc_val:.4f}",
              f"threshold:{FIXED_THRESHOLD:.3f}[FIXED]",
              f"time:{epoch_time:.1f}s")
        
        # Early Stopping ì²´í¬
        if f1_val > best_f1:
            best_f1 = f1_val
            patience_counter = 0
            print(f"   ğŸ¯ ìƒˆë¡œìš´ ìµœê³  F1 ì ìˆ˜: {best_f1:.4f}")
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if epoch > 5:
                model_path = os.path.join(path, 'best_model')
                if not os.path.exists(model_path):
                    os.makedirs(model_path)
                saver.save(sess, f'{model_path}/V2X_ASTGCN_STABLE_BEST', global_step=epoch)
        else:
            patience_counter += 1
            
        if patience_counter >= early_stopping_patience:
            print(f"   â¹ï¸ Early Stopping at epoch {epoch} (patience: {patience_counter})")
            break
            
    except Exception as e:
        print(f"âŒ Epoch {epoch} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        continue

time_end = time.time()
print(f'\nâ±ï¸ GPU ê°œì„ ëœ ì´ìƒíƒì§€ í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {time_end-time_start:.2f}ì´ˆ')

############## ê²°ê³¼ ë¶„ì„ ë° ì €ì¥ ###############
if test_f1 and len(test_f1) > 0:
    try:
        # ìµœê³  F1 ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ
        best_index = np.argmax(test_f1)
        best_test_result = test_pred[best_index]
        
        print(f"\nğŸ” ì•ˆì •í™”ëœ ìµœì¢… ì„±ëŠ¥ ë¶„ì„:")
        print(f"   ìµœê³  F1 ì ìˆ˜ ë‹¬ì„± ì—í¬í¬: {best_index}")
        print(f"   ì‚¬ìš©ëœ ê³ ì • ì„ê³„ê°’: {FIXED_THRESHOLD}")
        print(f"   ìµœê³  F1 ì ìˆ˜: {test_f1[best_index]:.4f}")
        
        # ìƒì„¸ í˜¼ë™í–‰ë ¬ ë¶„ì„
        test_label_final = testY.reshape(-1, actual_num_nodes)
        y_true_binary_final = (test_label_final.flatten() > FIXED_THRESHOLD).astype(int)
        y_pred_binary_final = (best_test_result.flatten() > FIXED_THRESHOLD).astype(int)
        
        cm = confusion_matrix(y_true_binary_final, y_pred_binary_final)
        print(f"   ğŸ“Š ìµœì¢… í˜¼ë™í–‰ë ¬:")
        print(f"     [[TN={cm[0,0]:6d}, FP={cm[0,1]:6d}],")
        print(f"      [FN={cm[1,0]:6d}, TP={cm[1,1]:6d}]]")
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
        if len(np.unique(y_true_binary_final)) > 1:
            class_report = classification_report(y_true_binary_final, y_pred_binary_final, 
                                               target_names=['Normal', 'Anomaly'], 
                                               digits=4)
            print(f"   ğŸ“‹ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¦¬í¬íŠ¸:")
            print(class_report)
        
        # ê²°ê³¼ ì €ì¥
        var = pd.DataFrame(best_test_result)
        var.to_csv(path + '/test_anomaly_result_stable.csv', index=False, header=False)
        
        # ìƒì„¸ ë©”íŠ¸ë¦­ ì €ì¥
        detailed_metrics = {
            'model_type': 'stable_fixed_threshold',
            'fixed_threshold': float(FIXED_THRESHOLD),
            'max_class_weight': float(MAX_CLASS_WEIGHT),
            'best_epoch': int(best_index),
            'best_f1': float(test_f1[best_index]),
            'final_pos_weight': float(pos_weight),
            'confusion_matrix': cm.tolist(),
            'training_time_seconds': float(time_end - time_start)
        }
        
        import json
        with open(path + '/stable_training_metadata.json', 'w') as f:
            json.dump(detailed_metrics, f, indent=2)
        
        # í•™ìŠµ ê³¡ì„  ì €ì¥
        min_length = len(test_f1)
        training_curves = pd.DataFrame({
            'epoch': range(min_length),
            'test_loss': test_loss[:min_length],
            'test_acc': test_acc[:min_length],
            'test_precision': test_precision[:min_length],
            'test_recall': test_recall[:min_length],
            'test_f1': test_f1[:min_length],
            'test_auc': test_auc[:min_length],
            'fixed_threshold': [FIXED_THRESHOLD] * min_length
        })
        training_curves.to_csv(path + '/stable_training_curves.csv', index=False)
        
        # í‰ê°€ ë©”íŠ¸ë¦­ ì €ì¥
        evaluation_metrics = [
            test_acc[best_index],
            test_precision[best_index],
            test_recall[best_index],
            test_f1[best_index],
            test_auc[best_index]
        ]
        
        evaluation_df = pd.DataFrame(evaluation_metrics, 
                                   index=['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'])
        evaluation_df.to_csv(path + '/stable_anomaly_evaluation.csv')
        
        print("âœ… ì•ˆì •í™”ëœ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

        print(f'\nğŸ‰ V2X AST-GCN ì•ˆì •í™”ëœ ì´ìƒíƒì§€ ê²°ê³¼:')
        print(f'   model_name: {model_name}_stable')
        print(f'   dataset: {data_name}')
        print(f'   scheme: {scheme} ({name})')
        print(f'   task: anomaly_detection_stable')
        print(f'   fixed_threshold: {FIXED_THRESHOLD}')
        print(f'   max_class_weight: {MAX_CLASS_WEIGHT}')
        print(f'   best_accuracy: {test_acc[best_index]:.4f}')
        print(f'   best_precision: {test_precision[best_index]:.4f}')
        print(f'   best_recall: {test_recall[best_index]:.4f}')
        print(f'   best_f1: {test_f1[best_index]:.4f}')
        print(f'   best_auc: {test_auc[best_index]:.4f}')
        print(f'   final_pos_weight: {pos_weight:.2f}')
        print(f'   training_time: {time_end-time_start:.2f}s')
        print(f'ğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {path}')
        
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

print("\nğŸ‰ V2X AST-GCN ì•ˆì •í™”ëœ ì´ìƒíƒì§€ ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ!")
print("ğŸ¯ ì•ˆì •í™” ì ìš© ì‚¬í•­:")
print(f"  - ê³ ì • ì„ê³„ê°’: {FIXED_THRESHOLD} (í•™ìŠµ ì¤‘ ë³€ê²½ ì—†ìŒ)")
print(f"  - ì œí•œëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f} (ìµœëŒ€ {MAX_CLASS_WEIGHT})")
print(f"  - ì•ˆì •í™”ëœ í‰ê°€: ì¼ê´€ëœ ë©”íŠ¸ë¦­ ê³„ì‚°")
print(f"  - ê°œì„ ëœ Early Stopping: ë” ì•ˆì •ì ì¸ ìˆ˜ë ´")
print(f"  - ìˆ˜ì¹˜ ì•ˆì •ì„±: NaN/Inf ì²˜ë¦¬ ê°•í™”")

print("\nğŸ“‹ ì•ˆì •í™” ê²°ê³¼ íŒŒì¼:")
print("  - test_anomaly_result_stable.csv: ì•ˆì •í™”ëœ ì˜ˆì¸¡ ê²°ê³¼")
print("  - stable_anomaly_evaluation.csv: í•µì‹¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­")
print("  - stable_training_curves.csv: ì—í¬í¬ë³„ í•™ìŠµ ê³¡ì„ ")
print("  - stable_training_metadata.json: ì•ˆì •í™” ì„¤ì • ì •ë³´")

if test_f1 and len(test_f1) > 0:
    final_f1 = max(test_f1)
    if final_f1 > 0.25:
        print("\nâœ… ì•ˆì •í™” ì„±ê³µ!")
        print("  - ì¼ê´€ëœ í•™ìŠµ íŒ¨í„´ ë‹¬ì„±")
        print("  - ì‹¤ì œ ìš´ì˜ í™˜ê²½ ì ìš© ê°€ëŠ¥")
    else:
        print("\nğŸ”§ ì¶”ê°€ ê°œì„  ì œì•ˆ:")
        print(f"  - ì„ê³„ê°’ ì¡°ì •: {FIXED_THRESHOLD} â†’ {FIXED_THRESHOLD * 0.8:.2f}")
        print(f"  - í•™ìŠµ ì—í¬í¬ ì¦ê°€: {training_epoch} â†’ {training_epoch + 20}")
else:
    print("\nâŒ í•™ìŠµ ê²°ê³¼ê°€ ì—†ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")