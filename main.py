#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
V2X AST-GCN ì´ìƒíƒì§€ ë©”ì¸ ì‹¤í–‰ íŒŒì¼ - ìˆ˜ì •ëœ ë²„ì „

í•µì‹¬ ìˆ˜ì •ì‚¬í•­:
1. ë°ì´í„° ë¶„í¬ ê¸°ë°˜ ë™ì  ì„ê³„ê°’ ê³„ì‚°
2. í‰ê°€ ë¡œì§ ì¼ê´€ì„± í™•ë³´
3. ëª¨ë¸ ì¶œë ¥ ê²€ì¦ ì¶”ê°€
4. ì‹œê°í™” ê°œì„ 

Author: V2X Anomaly Detection Team (Fixed)
Date: 2025-06-09
"""

import pickle as pkl
import tensorflow as tf
import os
import time
import numpy as np
import tensorflow.compat.v1 as tf_v1
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# ğŸ¯ ë™ì  ì„ê³„ê°’ ì„¤ì • (ë°ì´í„° ë¶„í¬ ê¸°ë°˜)
DYNAMIC_THRESHOLD = True  # ë™ì  ì„ê³„ê°’ ì‚¬ìš© ì—¬ë¶€
BASE_THRESHOLD = 0.3      # ê¸°ë³¸ ì„ê³„ê°’ (í´ë°±ìš©)
MAX_CLASS_WEIGHT = 5.0    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ìµœëŒ€ê°’ ì¦ê°€

print(f"ğŸ¯ ìˆ˜ì •ëœ ì„¤ì •:")
print(f"   ë™ì  ì„ê³„ê°’: {DYNAMIC_THRESHOLD}")
print(f"   ê¸°ë³¸ ì„ê³„ê°’: {BASE_THRESHOLD}")
print(f"   ìµœëŒ€ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {MAX_CLASS_WEIGHT}")

# Apple GPU (Metal) ì„¤ì •
print("ğŸ” GPU ì„¤ì • ì¤‘...")
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print(f"ğŸš€ Apple GPU (Metal) í™œì„±í™” ì„±ê³µ! - {gpus[0]}")
    except RuntimeError as e:
        print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
else:
    print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")

# TF 1.x í˜¸í™˜ ëª¨ë“œë¡œ ì „í™˜
import tensorflow.compat.v1 as tf_v1
tf_v1.disable_v2_behavior()

import pandas as pd
import numpy as np
import math
import os
import numpy.linalg as la
from acell import preprocess_data, load_assist_data, load_v2x_data
from tgcn import tgcnCell

from visualization import plot_result, plot_error
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, confusion_matrix, classification_report)
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import time

time_start = time.time()

###### Settings ######
flags = tf_v1.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_float('learning_rate', 0.0001, 'Initial learning rate.')
flags.DEFINE_integer('training_epoch', 50, 'Number of epochs to train.')
flags.DEFINE_integer('gru_units', 64, 'hidden units of gru.')
flags.DEFINE_integer('seq_len', 10, 'time length of inputs.')
flags.DEFINE_integer('pre_len', 3, 'time length of prediction.')
flags.DEFINE_float('train_rate', 0.8, 'rate of training set.')
flags.DEFINE_integer('batch_size', 32, 'batch size.')
flags.DEFINE_string('dataset', 'v2x', 'dataset')
flags.DEFINE_string('model_name', 'ast-gcn', 'ast-gcn')
flags.DEFINE_integer('scheme', 3, 'scheme')
flags.DEFINE_string('noise_name', 'None', 'None or Gauss or Possion')
flags.DEFINE_float('noise_param', 0, 'Parameter for noise')

model_name = FLAGS.model_name
noise_name = FLAGS.noise_name
data_name = FLAGS.dataset
train_rate = FLAGS.train_rate
seq_len = FLAGS.seq_len
output_dim = pre_len = FLAGS.pre_len
batch_size = FLAGS.batch_size
lr = FLAGS.learning_rate
training_epoch = FLAGS.training_epoch
gru_units = FLAGS.gru_units
scheme = FLAGS.scheme
PG = FLAGS.noise_param

###### load data ######
print(f"ğŸš¨ V2X ì´ìƒíƒì§€ ë°ì´í„° ë¡œë”©: {data_name}")

if data_name == 'v2x':
    data, adj, poi_data, weather_data = load_v2x_data('v2x')
    print(f"âœ… V2X ì´ìƒíƒì§€ ë°ì´í„° ë¡œë”© ì™„ë£Œ")
elif data_name == 'sz':
    data, adj = load_assist_data('sz')
    poi_data, weather_data = None, None
    print(f"âœ… Shenzhen ë°ì´í„° ë¡œë”© ì™„ë£Œ")
else:
    raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {data_name}")

### Perturbation Analysis (ê¸°ì¡´ê³¼ ë™ì¼)
def MaxMinNormalization(x, Max, Min):
    x = (x - Min) / (Max - Min)
    return x

if noise_name == 'Gauss':
    Gauss = np.random.normal(0, PG, size=data.shape)
    noise_Gauss = MaxMinNormalization(Gauss, np.max(Gauss), np.min(Gauss))
    data = data + noise_Gauss
elif noise_name == 'Possion':
    Possion = np.random.poisson(PG, size=data.shape)
    noise_Possion = MaxMinNormalization(Possion, np.max(Possion), np.min(Possion))
    data = data + noise_Possion
else:
    data = data

time_len = data.shape[0]
num_nodes = data.shape[1]

# ğŸ”§ ìˆ˜ì •: matrix íƒ€ì… ì‚¬ìš© ê¸ˆì§€, arrayë¡œ ê°•ì œ ë³€í™˜
data1 = np.array(data, dtype=np.float32)  # np.mat ëŒ€ì‹  np.array ì‚¬ìš©

#### normalization (ì´ìƒì ìˆ˜ëŠ” ì´ë¯¸ 0-1 ë²”ìœ„ì´ë¯€ë¡œ ìµœì†Œí•œì˜ ì •ê·œí™”)
max_value = np.max(data1)
if max_value > 0:
    data1 = data1 / max_value
else:
    print("âš ï¸ ëª¨ë“  ì´ìƒì ìˆ˜ê°€ 0ì…ë‹ˆë‹¤. ì •ê·œí™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

# ğŸ”§ ìˆ˜ì •: ì»¬ëŸ¼ ì •ë³´ ì²˜ë¦¬
if hasattr(data, 'columns'):
    # pandas DataFrameì˜ ì»¬ëŸ¼ ì •ë³´ëŠ” ë”°ë¡œ ì €ì¥
    column_names = data.columns
else:
    column_names = None

# ğŸ”§ í•µì‹¬ ìˆ˜ì •: ë°ì´í„° ë¶„í¬ ê¸°ë°˜ ìµœì  ì„ê³„ê°’ ê³„ì‚°
def calculate_optimal_threshold(data_values, method='sigmoid_compatible'):
    """ëª¨ë¸ ì¶œë ¥ê³¼ í˜¸í™˜ë˜ëŠ” ìµœì  ì„ê³„ê°’ ê³„ì‚°"""
    print("ğŸ” ëª¨ë¸ í˜¸í™˜ ìµœì  ì„ê³„ê°’ ê³„ì‚° ì¤‘...")
    
    # ğŸ”§ ìˆ˜ì •: matrix íƒ€ì…ì„ arrayë¡œ ê°•ì œ ë³€í™˜
    if isinstance(data_values, np.matrix):
        data_values = np.asarray(data_values)
    
    # ë°ì´í„° í†µê³„
    data_flat = data_values.flatten()
    non_zero_data = data_flat[data_flat > 0]
    
    # ğŸ”§ ìˆ˜ì •: ê° ê°’ì„ floatìœ¼ë¡œ ë³€í™˜í•˜ì—¬ format ë¬¸ì œ í•´ê²°
    mean_val = float(data_flat.mean())
    std_val = float(data_flat.std())
    median_val = float(np.median(data_flat))
    non_zero_mean = float(non_zero_data.mean()) if len(non_zero_data) > 0 else 0.0
    
    print(f"   ğŸ“Š ì „ì²´ ë°ì´í„° í†µê³„:")
    print(f"     í‰ê· : {mean_val:.4f}")
    print(f"     í‘œì¤€í¸ì°¨: {std_val:.4f}")
    print(f"     ì¤‘ê°„ê°’: {median_val:.4f}")
    print(f"     0ì´ ì•„ë‹Œ ê°’ë“¤ í‰ê· : {non_zero_mean:.4f}")
    
    if method == 'sigmoid_compatible':
        # ğŸ¯ í•µì‹¬ ìˆ˜ì •: ì‹œê·¸ëª¨ì´ë“œ ì¶œë ¥(~0.5)ê³¼ í˜¸í™˜ë˜ëŠ” ì„ê³„ê°’ ì„¤ì •
        # ë°ì´í„° ê¸°ë°˜ ê³„ì‚° í›„ ì‹œê·¸ëª¨ì´ë“œ ë²”ìœ„ë¡œ ì¡°ì •
        data_based_threshold = float(np.percentile(data_flat, 85))
        
        # ì‹œê·¸ëª¨ì´ë“œ ì¶œë ¥ ê³ ë ¤: 0.3~0.7 ë²”ìœ„ë¡œ ì¡°ì •
        if data_based_threshold < 0.1:
            threshold = 0.45  # ì‹œê·¸ëª¨ì´ë“œ ì¤‘ì•™ê°’ ê·¼ì²˜
        elif data_based_threshold > 0.5:
            threshold = 0.55  # ì•½ê°„ ë†’ì€ ê°’
        else:
            threshold = 0.5   # ì‹œê·¸ëª¨ì´ë“œ ì¤‘ì•™ê°’
            
        print(f"   ğŸ”§ ì‹œê·¸ëª¨ì´ë“œ í˜¸í™˜ ì¡°ì •: {data_based_threshold:.4f} â†’ {threshold:.4f}")
        
    elif method == 'percentile_based':
        # ê¸°ì¡´ ë°©ì‹ (ë¬¸ì œê°€ ìˆì—ˆë˜ ë°©ì‹)
        threshold = float(np.percentile(data_flat, 85))
        threshold = float(np.clip(threshold, 0.05, 0.5))
    else:
        # ê¸°ë³¸ê°’
        threshold = 0.5
    
    # ğŸ”§ ìˆ˜ì •: ì‹œê·¸ëª¨ì´ë“œ ì¶œë ¥ ë²”ìœ„ì— ë§ëŠ” ì•ˆì „ ë²”ìœ„ (0.3 ~ 0.7)
    threshold = float(np.clip(threshold, 0.3, 0.7))
    
    # ê²°ê³¼ ì´ìƒ ë¹„ìœ¨ ê³„ì‚° (ì›ë³¸ ë°ì´í„° ê¸°ì¤€)
    anomaly_ratio = float((data_flat > data_based_threshold if 'data_based_threshold' in locals() else mean_val + std_val).mean())
    
    print(f"   ğŸ¯ ìµœì¢… ì„ê³„ê°’: {threshold:.4f} (ì‹œê·¸ëª¨ì´ë“œ í˜¸í™˜)")
    print(f"   ğŸ“Š ì˜ˆìƒ ì´ìƒ ë¹„ìœ¨: {anomaly_ratio*100:.2f}%")
    
    return threshold

# ë™ì  ì„ê³„ê°’ ê³„ì‚°
if DYNAMIC_THRESHOLD:
    optimal_threshold = calculate_optimal_threshold(data1, method='sigmoid_compatible')
else:
    optimal_threshold = 0.5  # ì‹œê·¸ëª¨ì´ë“œ ì¤‘ì•™ê°’

print(f"   âœ… ìµœì¢… ì‚¬ìš© ì„ê³„ê°’: {optimal_threshold:.4f} (ì‹œê·¸ëª¨ì´ë“œ í˜¸í™˜)")

# ëª¨ë¸ ë° ìŠ¤í‚´ ì •ë³´ ì¶œë ¥
if model_name == 'ast-gcn':
    if scheme == 1:
        name = 'V2X POI only (Anomaly Detection)' if data_name == 'v2x' else 'add poi dim'
    elif scheme == 2:
        name = 'V2X Weather only (Anomaly Detection)' if data_name == 'v2x' else 'add weather dim'
    else:
        name = 'V2X POI + Weather (Anomaly Detection)' if data_name == 'v2x' else 'add poi + weather dim'
else:
    name = 'tgcn (Anomaly Detection)'

print('ğŸ“Š ì´ìƒíƒì§€ ëª¨ë¸ ì •ë³´:')
print(f'   task: anomaly_detection')
print(f'   model: {model_name}')
print(f'   dataset: {data_name}')
print(f'   scheme: {scheme} ({name})')
print(f'   data shape: {data1.shape}')
print(f'   time_len: {time_len}, num_nodes: {num_nodes}')
print(f'   optimal_threshold: {optimal_threshold:.4f}')
print(f'   noise_name: {noise_name}')
print(f'   noise_param: {PG}')

# ğŸ”§ ìˆ˜ì •ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜ (ìµœì  ì„ê³„ê°’ ì‚¬ìš©)
def preprocess_data_fixed(data1, time_len, train_rate, seq_len, pre_len, 
                         model_name, scheme, poi_data=None, weather_data=None,
                         threshold=None):
    """ìˆ˜ì •ëœ V2X ì´ìƒíƒì§€ ì „ì²˜ë¦¬ - ìµœì  ì„ê³„ê°’ ì‚¬ìš©"""
    print(f"ğŸ› ï¸ ìˆ˜ì •ëœ V2X ì´ìƒíƒì§€ ì „ì²˜ë¦¬:")
    print(f"   ğŸ“Š ë°ì´í„° í˜•íƒœ: {data1.shape}")
    print(f"   ğŸ¯ ìµœì  ì„ê³„ê°’: {threshold:.4f}")
    print(f"   ğŸ”§ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}, ì˜ˆì¸¡ ê¸¸ì´: {pre_len}")
    
    # ì•ˆì „í•œ ë°ì´í„° ë³€í™˜
    if isinstance(data1, np.matrix):
        data1 = np.asarray(data1)
    
    data_values = np.array(data1, dtype=np.float32)
    
    # NaN/Inf ì²˜ë¦¬
    data_values = np.nan_to_num(data_values, nan=0.0, posinf=1.0, neginf=0.0)
    
    print(f"   ğŸ“Š ë°ì´í„° í†µê³„:")
    print(f"     ë²”ìœ„: {data_values.min():.3f} ~ {data_values.max():.3f}")
    print(f"     í‰ê· : {data_values.mean():.3f}")
    
    # ğŸ¯ í•µì‹¬ ìˆ˜ì •: ì‹œê·¸ëª¨ì´ë“œ í˜¸í™˜ ë¼ë²¨ ìƒì„±
    # ì›ë³¸ ë°ì´í„°ì˜ ìƒìœ„ 10%ë¥¼ ì´ìƒìœ¼ë¡œ ì„¤ì • (ë” ê· í˜•ì¡íŒ ì ‘ê·¼)
    data_threshold_for_labels = float(np.percentile(data_values.flatten(), 90))
    binary_labels = (data_values > data_threshold_for_labels).astype(float)
    anomaly_ratio = binary_labels.mean()
    
    print(f"   ğŸ“Š ë¼ë²¨ ìƒì„± í†µê³„:")
    print(f"     ë¼ë²¨ ìƒì„± ì„ê³„ê°’: {data_threshold_for_labels:.4f} (90th percentile)")
    print(f"     í‰ê°€ ì„ê³„ê°’: {threshold:.4f} (ì‹œê·¸ëª¨ì´ë“œ í˜¸í™˜)")
    print(f"     ìƒì„±ëœ ì´ìƒ ë¹„ìœ¨: {anomaly_ratio:.3%}")
    
    if anomaly_ratio == 0:
        print(f"   âš ï¸ ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤! 85th percentileë¡œ ì¬ì‹œë„")
        data_threshold_for_labels = float(np.percentile(data_values.flatten(), 85))
        binary_labels = (data_values > data_threshold_for_labels).astype(float)
        anomaly_ratio = binary_labels.mean()
        print(f"   ğŸ”§ ì¡°ì •ëœ ë¼ë²¨ ì„ê³„ê°’: {data_threshold_for_labels:.4f}, ì´ìƒ ë¹„ìœ¨: {anomaly_ratio:.3%}")
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    train_size = int(time_len * train_rate)
    
    train_data = data_values[:train_size]
    test_data = data_values[train_size:]
    
    train_labels = binary_labels[:train_size]
    test_labels = binary_labels[train_size:]
    
    print(f"   âœ‚ï¸ ë¶„í•  ì™„ë£Œ:")
    print(f"     í›ˆë ¨: {train_data.shape}")
    print(f"     í…ŒìŠ¤íŠ¸: {test_data.shape}")
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    trainX, trainY, testX, testY = [], [], [], []
    
    # í›ˆë ¨ ì‹œí€€ìŠ¤
    for i in range(seq_len, len(train_data) - pre_len + 1):
        # ì…ë ¥: ì—°ì†ê°’ (ì›ë³¸ ì´ìƒì ìˆ˜)
        seq_x = train_data[i-seq_len:i].T  # (nodes, seq_len)
        # ë¼ë²¨: ì´ì§„ê°’ (0 ë˜ëŠ” 1)
        seq_y = train_labels[i:i+pre_len].T  # (nodes, pre_len)
        
        trainX.append(seq_x)
        trainY.append(seq_y)
    
    # í…ŒìŠ¤íŠ¸ ì‹œí€€ìŠ¤
    for i in range(seq_len, len(test_data) - pre_len + 1):
        seq_x = test_data[i-seq_len:i].T
        seq_y = test_labels[i:i+pre_len].T
        
        testX.append(seq_x)
        testY.append(seq_y)
    
    # ë°°ì—´ ë³€í™˜
    trainX = np.array(trainX)
    trainY = np.array(trainY)
    testX = np.array(testX)
    testY = np.array(testY)
    
    # ì°¨ì› ì¡°ì •: (samples, seq_len, nodes)
    trainX = np.transpose(trainX, (0, 2, 1))
    trainY = np.transpose(trainY, (0, 2, 1))
    testX = np.transpose(testX, (0, 2, 1))
    testY = np.transpose(testY, (0, 2, 1))
    
    # ğŸ”§ ìˆ˜ì •ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    y_flat = trainY.flatten()
    pos_count = np.sum(y_flat == 1)
    neg_count = np.sum(y_flat == 0)
    
    if pos_count > 0:
        pos_weight = min(MAX_CLASS_WEIGHT, neg_count / pos_count)
    else:
        pos_weight = 1.0
    
    # ìµœì¢… ê²€ì¦
    train_anomaly_ratio = (trainY == 1).mean()
    test_anomaly_ratio = (testY == 1).mean()
    
    print(f"   âœ… ìˆ˜ì •ëœ ì „ì²˜ë¦¬ ì™„ë£Œ:")
    print(f"     trainX: {trainX.shape}")
    print(f"     trainY: {trainY.shape}")
    print(f"     testX: {testX.shape}")
    print(f"     testY: {testY.shape}")
    print(f"     í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f}")
    print(f"     í›ˆë ¨ ì´ìƒ ë¹„ìœ¨: {train_anomaly_ratio:.2%}")
    print(f"     í…ŒìŠ¤íŠ¸ ì´ìƒ ë¹„ìœ¨: {test_anomaly_ratio:.2%}")
    
    # ê· í˜• ì •ë³´ ë°˜í™˜
    balance_info = {
        'pos_weight': pos_weight,
        'threshold_used': threshold,  # í‰ê°€ìš© ì„ê³„ê°’
        'label_threshold': data_threshold_for_labels,  # ë¼ë²¨ ìƒì„± ì„ê³„ê°’
        'train_anomaly_ratio': train_anomaly_ratio,
        'test_anomaly_ratio': test_anomaly_ratio
    }
    
    return trainX, trainY, testX, testY, balance_info

# ìˆ˜ì •ëœ ì „ì²˜ë¦¬ í˜¸ì¶œ
print(f"\nğŸ”„ ìˆ˜ì •ëœ ì´ìƒíƒì§€ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")

trainX, trainY, testX, testY, balance_info = preprocess_data_fixed(
    data1, time_len, train_rate, seq_len, pre_len, model_name, scheme,
    poi_data, weather_data, threshold=optimal_threshold
)

# í´ë˜ìŠ¤ ê· í˜• ì •ë³´ ì¶”ì¶œ
pos_weight = balance_info['pos_weight']
threshold_used = balance_info['threshold_used']
label_threshold = balance_info['label_threshold']
train_anomaly_ratio = balance_info['train_anomaly_ratio']
test_anomaly_ratio = balance_info['test_anomaly_ratio']

print(f"ğŸ¯ ìˆ˜ì •ëœ í´ë˜ìŠ¤ ê· í˜• ì •ë³´:")
print(f"   ì–‘ì„± í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f}")
print(f"   ë¼ë²¨ ìƒì„± ì„ê³„ê°’: {label_threshold:.4f} (90th percentile)")
print(f"   í‰ê°€ìš© ì„ê³„ê°’: {threshold_used:.4f} (ì‹œê·¸ëª¨ì´ë“œ í˜¸í™˜)")
print(f"   í›ˆë ¨ ì´ìƒ ë¹„ìœ¨: {train_anomaly_ratio:.2%}")
print(f"   í…ŒìŠ¤íŠ¸ ì´ìƒ ë¹„ìœ¨: {test_anomaly_ratio:.2%}")

totalbatch = int(trainX.shape[0] / batch_size)
training_data_count = len(trainX)

print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ:")
print(f"   total batches: {totalbatch}")
print(f"   training samples: {training_data_count}")

# ğŸ”§ ìˆ˜ì •ëœ í‰ê°€ í•¨ìˆ˜
def evaluate_anomaly_fixed(y_true, y_pred_proba, threshold):
    """ìˆ˜ì •ëœ ì´ìƒíƒì§€ í‰ê°€ í•¨ìˆ˜"""
    try:
        # ë°ì´í„° í‰í‰í™”
        y_true_flat = y_true.flatten()
        y_pred_flat = y_pred_proba.flatten()
        
        # NaN/Inf ì•ˆì „ ì²˜ë¦¬
        y_pred_flat = np.nan_to_num(y_pred_flat, 0.0)
        y_pred_flat = np.clip(y_pred_flat, 0.0, 1.0)
        
        # ğŸ¯ í•µì‹¬: ë™ì¼í•œ ì„ê³„ê°’ìœ¼ë¡œ ì´ì§„í™”
        y_pred_binary = (y_pred_flat > threshold).astype(int)
        y_true_binary = y_true_flat.astype(int)  # ì´ë¯¸ ì´ì§„ê°’
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        acc = accuracy_score(y_true_binary, y_pred_binary)
        
        # í´ë˜ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ê³„ì‚°
        if len(np.unique(y_true_binary)) > 1:
            pre = precision_score(y_true_binary, y_pred_binary, zero_division=0)
            rec = recall_score(y_true_binary, y_pred_binary, zero_division=0)
            f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)
            try:
                auc = roc_auc_score(y_true_binary, y_pred_flat)
            except:
                auc = 0.5
        else:
            pre = rec = f1 = auc = 0.0
        
        return acc, pre, rec, f1, auc
    except Exception as e:
        print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        return 0.5, 0.0, 0.0, 0.0, 0.5

def TGCN_ANOMALY(_X, _weights, _biases):
    """TGCN ì´ìƒíƒì§€ìš© ëª¨ë¸ - ì¶œë ¥ ì•ˆì •í™” ê°œì„ """
    ###
    cell_1 = tgcnCell(gru_units, adj, num_nodes=num_nodes)
    cell = tf_v1.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)
    _X = tf_v1.unstack(_X, axis=1)
    outputs, states = tf_v1.nn.static_rnn(cell, _X, dtype=tf_v1.float32)
    m = []
    for i in outputs:
        o = tf_v1.reshape(i, shape=[-1, num_nodes, gru_units])
        o = tf_v1.reshape(o, shape=[-1, gru_units])
        # Dropout
        o = tf_v1.nn.dropout(o, keep_prob=0.8)
        m.append(o)
    last_output = m[-1]
    
    # ğŸ”§ ìˆ˜ì •: ë” ë‹¤ì–‘í•œ ì¶œë ¥ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ê°œì„ 
    logits = tf_v1.matmul(last_output, _weights['out']) + _biases['out']
    logits = tf_v1.reshape(logits, shape=[-1, num_nodes, pre_len])
    logits = tf_v1.transpose(logits, perm=[0, 2, 1])
    logits = tf_v1.reshape(logits, shape=[-1, num_nodes])
    
    # ğŸ”§ ìˆ˜ì •: ë¡œì§€íŠ¸ ë²”ìœ„ë¥¼ ë„“í˜€ì„œ ë‹¤ì–‘í•œ ì‹œê·¸ëª¨ì´ë“œ ì¶œë ¥ ìƒì„±
    logits = tf_v1.clip_by_value(logits, -5.0, 5.0)  # ë²”ìœ„ í™•ëŒ€
    
    # ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” (ì´ìƒ í™•ë¥ )
    output = tf_v1.sigmoid(logits)
    
    return output, logits, m, states

def create_improved_loss_function(pos_weight=3.0):
    """ê°œì„ ëœ ê· í˜• ì†ì‹¤í•¨ìˆ˜"""
    
    def improved_loss(y_true, y_pred):
        # ğŸ”§ ìˆ˜ì •: ë” ì•ˆì •ì ì¸ weighted BCE
        weighted_bce = tf_v1.nn.weighted_cross_entropy_with_logits(
            labels=y_true,
            logits=y_pred,
            pos_weight=pos_weight
        )
        
        # ê°€ë²¼ìš´ Focal Loss ì¶”ê°€
        y_pred_sigmoid = tf_v1.nn.sigmoid(y_pred)
        y_pred_sigmoid = tf_v1.clip_by_value(y_pred_sigmoid, 1e-7, 1.0 - 1e-7)
        
        ce_loss = -y_true * tf_v1.math.log(y_pred_sigmoid) - (1 - y_true) * tf_v1.math.log(1 - y_pred_sigmoid)
        pt = tf_v1.where(tf_v1.equal(y_true, 1), y_pred_sigmoid, 1 - y_pred_sigmoid)
        focal_weight = 0.2 * tf_v1.pow(1 - pt, 1.5)  # ë” ì•½í•œ focal loss
        focal_loss = focal_weight * ce_loss
        
        # ê²°í•© (BCE 85%, Focal 15%)
        combined_loss = 0.85 * weighted_bce + 0.15 * focal_loss
        
        return tf_v1.reduce_mean(combined_loss)
    
    return improved_loss

###### placeholders ######
print(f"\nğŸ§  ìˆ˜ì •ëœ ì´ìƒíƒì§€ ëª¨ë¸ êµ¬ì„±...")

# ì‹¤ì œ ë°ì´í„° ì°¨ì› í™•ì¸
print(f"   ğŸ“Š ì‹¤ì œ ë°ì´í„° í˜•íƒœ:")
print(f"     trainX: {trainX.shape}")
print(f"     trainY: {trainY.shape}")
print(f"     seq_len: {seq_len}")
print(f"     pre_len: {pre_len}")
print(f"     num_nodes: {num_nodes}")

# ì•ˆì „í•œ placeholder ìƒì„±
actual_seq_len = trainX.shape[1]
actual_num_nodes = trainX.shape[2]
actual_pre_len = trainY.shape[1]

print(f"   ğŸ”§ Placeholder ì°¨ì›:")
print(f"     ì…ë ¥: [{None}, {actual_seq_len}, {actual_num_nodes}]")
print(f"     ë¼ë²¨: [{None}, {actual_pre_len}, {actual_num_nodes}]")

# ë™ì  ì°¨ì›ìœ¼ë¡œ placeholder ìƒì„±
inputs = tf_v1.placeholder(tf_v1.float32, shape=[None, actual_seq_len, actual_num_nodes])
labels = tf_v1.placeholder(tf_v1.float32, shape=[None, actual_pre_len, actual_num_nodes])

# Graph weights (ì´ìƒíƒì§€ìš©) - ğŸ”§ ìˆ˜ì •: ë” ë‚˜ì€ ì´ˆê¸°í™”
weights = {
    'out': tf_v1.Variable(tf_v1.random_normal([gru_units, actual_pre_len], mean=0.0, stddev=0.1), name='weight_o')}  # stddev ì¦ê°€

biases = {
    'out': tf_v1.Variable(tf_v1.random_normal([actual_pre_len], mean=0.0, stddev=0.1), name='bias_o')}  # stddev ì¦ê°€

pred, logits, ttts, ttto = TGCN_ANOMALY(inputs, weights, biases)

y_pred = pred  # ì‹œê·¸ëª¨ì´ë“œ ì¶œë ¥ (0-1 í™•ë¥ )
y_logits = logits  # ë¡œì§€íŠ¸ (ì†ì‹¤ ê³„ì‚°ìš©)

###### optimizer (ìˆ˜ì •ëœ ì´ìƒíƒì§€ìš© ì†ì‹¤í•¨ìˆ˜) ######
lambda_loss = 0.001  # L2 ì •ê·œí™” ê°ì†Œ
Lreg = lambda_loss * sum(tf_v1.nn.l2_loss(tf_var) for tf_var in tf_v1.trainable_variables())
label = tf_v1.reshape(labels, [-1, actual_num_nodes])
logits_reshaped = tf_v1.reshape(y_logits, [-1, actual_num_nodes])

print('y_pred_shape:', y_pred.shape)
print('label_shape:', label.shape)
print('logits_shape:', logits_reshaped.shape)

# ìˆ˜ì •ëœ ê· í˜• ì†ì‹¤í•¨ìˆ˜ ì‚¬ìš©
print(f"ğŸ¯ ìˆ˜ì •ëœ ì†ì‹¤í•¨ìˆ˜ êµ¬ì„± (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f})")
improved_loss_fn = create_improved_loss_function(pos_weight=pos_weight)
main_loss = improved_loss_fn(label, logits_reshaped)
loss = main_loss + Lreg

# ìˆ˜ì •ëœ ìµœì í™”ê¸°
global_step = tf_v1.Variable(0, trainable=False)
learning_rate = tf_v1.train.exponential_decay(
    lr, global_step, 
    decay_steps=300,  # ë” ë¹ ë¥¸ ê°ì†Œ
    decay_rate=0.95,  # ë” ì•ˆì •ì ì¸ ê°ì†Œ
    staircase=True
)

# Gradient clipping (ìˆ˜ì •)
opt = tf_v1.train.AdamOptimizer(learning_rate)
grads_and_vars = opt.compute_gradients(loss)
clipped_grads_and_vars = [(tf_v1.clip_by_value(grad, -1.0, 1.0), var)  # í´ë¦¬í•‘ ë²”ìœ„ í™•ëŒ€
                          for grad, var in grads_and_vars if grad is not None]
optimizer = opt.apply_gradients(clipped_grads_and_vars, global_step=global_step)

###### Initialize session ######
variables = tf_v1.global_variables()
saver = tf_v1.train.Saver(tf_v1.global_variables())

# GPU ì„¤ì •
config = tf_v1.ConfigProto()
config.allow_soft_placement = True
config.gpu_options.allow_growth = True

sess = tf_v1.Session(config=config)
sess.run(tf_v1.global_variables_initializer())

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
if data_name == 'v2x':
    out = f'out/v2x_{model_name}_anomaly_fixed_scheme{scheme}_gpu'
else:
    out = f'out/{model_name}_anomaly_fixed_{noise_name}_gpu'

path1 = f'{model_name}_anomaly_fixed_{name}_{data_name}_lr{lr}_batch{batch_size}_unit{gru_units}_seq{seq_len}_pre{pre_len}_epoch{training_epoch}_scheme{scheme}_threshold{threshold_used:.3f}_GPU'
path = os.path.join(out, path1)
if not os.path.exists(path):
    os.makedirs(path)

print(f"ğŸ“‚ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {path}")

print(f"\nğŸš€ V2X AST-GCN ìˆ˜ì •ëœ ì´ìƒíƒì§€ GPU í•™ìŠµ ì‹œì‘...")
print(f"   Epochs: {training_epoch}")
print(f"   Batch size: {batch_size}")
print(f"   Learning rate: {lr} (ì§€ìˆ˜ ê°ì†Œ)")
print(f"   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f}")
print(f"   ë¼ë²¨ ì„ê³„ê°’: {label_threshold:.4f}")
print(f"   í‰ê°€ ì„ê³„ê°’: {threshold_used:.4f}")

x_axe, batch_loss, batch_acc = [], [], []
test_loss, test_acc, test_precision, test_recall, test_f1, test_auc, test_pred = [], [], [], [], [], [], []

best_f1 = 0.0
patience_counter = 0
early_stopping_patience = 10

for epoch in range(training_epoch):
    epoch_start_time = time.time()
    
    # í•™ìŠµë¥  ì¶œë ¥ (5 ì—í¬í¬ë§ˆë‹¤)
    if epoch % 5 == 0:
        current_lr = sess.run(learning_rate)
        print(f"   ğŸ“‰ í˜„ì¬ í•™ìŠµë¥ : {current_lr:.6f}")
    
    # ë°°ì¹˜ë³„ í•™ìŠµ
    epoch_batch_loss, epoch_batch_acc = [], []
    for m in range(totalbatch):
        mini_batch = trainX[m * batch_size : (m+1) * batch_size]
        mini_label = trainY[m * batch_size : (m+1) * batch_size]
        
        # NaN ì²´í¬
        if np.isnan(mini_batch).any() or np.isnan(mini_label).any():
            print(f"âš ï¸ Epoch {epoch}, Batch {m}: NaN ë°œê²¬, ê±´ë„ˆë›°ê¸°")
            continue
            
        # í•™ìŠµ ì‹¤í–‰
        _, loss1 = sess.run([optimizer, loss],
                           feed_dict={inputs: mini_batch, labels: mini_label})
        
        if np.isnan(loss1):
            print(f"âš ï¸ Epoch {epoch}, Batch {m}: í•™ìŠµ ì¤‘ NaN ë°œìƒ")
            continue
            
        epoch_batch_loss.append(loss1)
    
    # ì—í¬í¬ í‰ê·  ê³„ì‚°
    if epoch_batch_loss:
        epoch_train_loss = np.mean(epoch_batch_loss)
    else:
        epoch_train_loss = 0.0
    
    # í…ŒìŠ¤íŠ¸ í‰ê°€ (ìˆ˜ì •ëœ ë°©ì‹)
    try:
        # ì†ì‹¤ê³¼ ì˜ˆì¸¡ê°’ ê³„ì‚°
        loss2, test_output = sess.run(
            [loss, y_pred],
            feed_dict={inputs: testX, labels: testY}
        )
        
        # ğŸ”§ ìˆ˜ì •: ì˜ˆì¸¡ í™•ë¥  ë¶„ì„
        test_prob = np.clip(test_output, 0.0, 1.0)
        
        # ì˜ˆì¸¡ ë¶„í¬ ì¶œë ¥ (ì²« 5 ì—í¬í¬ë§Œ)
        if epoch < 5:
            print(f"   ğŸ“Š ì˜ˆì¸¡ í™•ë¥  ë¶„í¬:")
            print(f"     í‰ê· : {test_prob.mean():.4f}")
            print(f"     í‘œì¤€í¸ì°¨: {test_prob.std():.4f}")
            print(f"     ìµœì†Œ/ìµœëŒ€: {test_prob.min():.4f}/{test_prob.max():.4f}")
            print(f"     > ì„ê³„ê°’({threshold_used:.3f}) ë¹„ìœ¨: {(test_prob > threshold_used).mean():.3%}")
        
        # ğŸ¯ í•µì‹¬: ìˆ˜ì •ëœ í‰ê°€ í•¨ìˆ˜ ì‚¬ìš©
        accuracy_val, precision_val, recall_val, f1_val, auc_val = evaluate_anomaly_fixed(
            testY.reshape(-1, actual_num_nodes), 
            test_prob,
            threshold_used
        )
        
        # ê²°ê³¼ ì €ì¥
        test_loss.append(loss2)
        test_acc.append(accuracy_val)
        test_precision.append(precision_val)
        test_recall.append(recall_val)
        test_f1.append(f1_val)
        test_auc.append(auc_val)
        test_pred.append(test_prob)
        
        # ì¶œë ¥ (ìˆ˜ì •ëœ ì„ê³„ê°’ í‘œì‹œ)
        epoch_time = time.time() - epoch_start_time
        print(f"Epoch:{epoch:2d}",
              f"train_loss:{epoch_train_loss:.4f}",
              f"test_loss:{loss2:.4f}",
              f"test_acc:{accuracy_val:.4f}",
              f"test_pre:{precision_val:.4f}",
              f"test_rec:{recall_val:.4f}",
              f"test_f1:{f1_val:.4f}",
              f"test_auc:{auc_val:.4f}",
              f"threshold:{threshold_used:.3f}[FIXED]",
              f"time:{epoch_time:.1f}s")
        
        # Early Stopping ì²´í¬
        if f1_val > best_f1:
            best_f1 = f1_val
            patience_counter = 0
            print(f"   ğŸ¯ ìƒˆë¡œìš´ ìµœê³  F1 ì ìˆ˜: {best_f1:.4f}")
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if epoch > 5:
                model_path = os.path.join(path, 'best_model')
                if not os.path.exists(model_path):
                    os.makedirs(model_path)
                saver.save(sess, f'{model_path}/V2X_ASTGCN_FIXED_BEST', global_step=epoch)
        else:
            patience_counter += 1
            
        if patience_counter >= early_stopping_patience:
            print(f"   â¹ï¸ Early Stopping at epoch {epoch} (patience: {patience_counter})")
            break
            
    except Exception as e:
        print(f"âŒ Epoch {epoch} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        continue

time_end = time.time()
print(f'\nâ±ï¸ GPU ìˆ˜ì •ëœ ì´ìƒíƒì§€ í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {time_end-time_start:.2f}ì´ˆ')

############## ê²°ê³¼ ë¶„ì„ ë° ì €ì¥ ###############
if test_f1 and len(test_f1) > 0:
    try:
        # ìµœê³  F1 ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ
        best_index = np.argmax(test_f1)
        best_test_result = test_pred[best_index]
        
        print(f"\nğŸ” ìˆ˜ì •ëœ ìµœì¢… ì„±ëŠ¥ ë¶„ì„:")
        print(f"   ìµœê³  F1 ì ìˆ˜ ë‹¬ì„± ì—í¬í¬: {best_index}")
        print(f"   ì‚¬ìš©ëœ ìµœì  ì„ê³„ê°’: {threshold_used:.4f}")
        print(f"   ìµœê³  F1 ì ìˆ˜: {test_f1[best_index]:.4f}")
        
        # ğŸ”§ ìˆ˜ì •ëœ ìƒì„¸ í˜¼ë™í–‰ë ¬ ë¶„ì„
        test_label_final = testY.reshape(-1, actual_num_nodes)
        y_true_binary_final = test_label_final.flatten().astype(int)
        y_pred_binary_final = (best_test_result.flatten() > threshold_used).astype(int)
        
        cm = confusion_matrix(y_true_binary_final, y_pred_binary_final)
        print(f"   ğŸ“Š ìˆ˜ì •ëœ ìµœì¢… í˜¼ë™í–‰ë ¬:")
        print(f"     [[TN={cm[0,0]:6d}, FP={cm[0,1]:6d}],")
        print(f"      [FN={cm[1,0]:6d}, TP={cm[1,1]:6d}]]")
        
        # ì¶”ê°€ ë¶„ì„
        total_samples = len(y_true_binary_final)
        actual_positive = np.sum(y_true_binary_final)
        predicted_positive = np.sum(y_pred_binary_final)
        
        print(f"   ğŸ“Š ìƒì„¸ ë¶„ì„:")
        print(f"     ì „ì²´ ìƒ˜í”Œ: {total_samples:,}")
        print(f"     ì‹¤ì œ ì´ìƒ: {actual_positive:,} ({actual_positive/total_samples:.2%})")
        print(f"     ì˜ˆì¸¡ ì´ìƒ: {predicted_positive:,} ({predicted_positive/total_samples:.2%})")
        
        # ğŸ”§ ìˆ˜ì •: ROC ì»¤ë¸Œ ë¶„ì„ ì¶”ê°€
        if len(np.unique(y_true_binary_final)) > 1:
            from sklearn.metrics import roc_curve
            fpr, tpr, thresholds = roc_curve(y_true_binary_final, best_test_result.flatten())
            
            # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (Youden's J statistic)
            optimal_idx = np.argmax(tpr - fpr)
            optimal_threshold_roc = thresholds[optimal_idx]
            
            print(f"   ğŸ¯ ROC ê¸°ë°˜ ìµœì  ì„ê³„ê°’: {optimal_threshold_roc:.4f}")
            print(f"     (í˜„ì¬ ì‚¬ìš©: {threshold_used:.4f})")
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
        if len(np.unique(y_true_binary_final)) > 1:
            class_report = classification_report(y_true_binary_final, y_pred_binary_final, 
                                               target_names=['Normal', 'Anomaly'], 
                                               digits=4)
            print(f"   ğŸ“‹ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¦¬í¬íŠ¸:")
            print(class_report)
        
        # ê²°ê³¼ ì €ì¥
        var = pd.DataFrame(best_test_result)
        var.to_csv(path + '/test_anomaly_result_fixed.csv', index=False, header=False)
        
        # ìƒì„¸ ë©”íŠ¸ë¦­ ì €ì¥
        detailed_metrics = {
            'model_type': 'fixed_dynamic_threshold',
            'optimal_threshold': float(threshold_used),
            'max_class_weight': float(MAX_CLASS_WEIGHT),
            'best_epoch': int(best_index),
            'best_f1': float(test_f1[best_index]),
            'final_pos_weight': float(pos_weight),
            'confusion_matrix': cm.tolist(),
            'training_time_seconds': float(time_end - time_start),
            'data_statistics': {
                'data_mean': float(data1.mean()),
                'data_std': float(data1.std()),
                'anomaly_ratio': float((data1 > threshold_used).mean())
            }
        }
        
        import json
        with open(path + '/fixed_training_metadata.json', 'w') as f:
            json.dump(detailed_metrics, f, indent=2)
        
        # í•™ìŠµ ê³¡ì„  ì €ì¥
        min_length = len(test_f1)
        training_curves = pd.DataFrame({
            'epoch': range(min_length),
            'test_loss': test_loss[:min_length],
            'test_acc': test_acc[:min_length],
            'test_precision': test_precision[:min_length],
            'test_recall': test_recall[:min_length],
            'test_f1': test_f1[:min_length],
            'test_auc': test_auc[:min_length],
            'optimal_threshold': [threshold_used] * min_length
        })
        training_curves.to_csv(path + '/fixed_training_curves.csv', index=False)
        
        # í‰ê°€ ë©”íŠ¸ë¦­ ì €ì¥
        evaluation_metrics = [
            test_acc[best_index],
            test_precision[best_index],
            test_recall[best_index],
            test_f1[best_index],
            test_auc[best_index]
        ]
        
        evaluation_df = pd.DataFrame(evaluation_metrics, 
                                   index=['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'])
        evaluation_df.to_csv(path + '/fixed_anomaly_evaluation.csv')
        
        print("âœ… ìˆ˜ì •ëœ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

        print(f'\nğŸ‰ V2X AST-GCN ìˆ˜ì •ëœ ì´ìƒíƒì§€ ê²°ê³¼:')
        print(f'   model_name: {model_name}_fixed')
        print(f'   dataset: {data_name}')
        print(f'   scheme: {scheme} ({name})')
        print(f'   task: anomaly_detection_fixed')
        print(f'   optimal_threshold: {threshold_used:.4f}')
        print(f'   max_class_weight: {MAX_CLASS_WEIGHT}')
        print(f'   best_accuracy: {test_acc[best_index]:.4f}')
        print(f'   best_precision: {test_precision[best_index]:.4f}')
        print(f'   best_recall: {test_recall[best_index]:.4f}')
        print(f'   best_f1: {test_f1[best_index]:.4f}')
        print(f'   best_auc: {test_auc[best_index]:.4f}')
        print(f'   final_pos_weight: {pos_weight:.2f}')
        print(f'   training_time: {time_end-time_start:.2f}s')
        print(f'ğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {path}')
        
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

print("\nğŸ‰ V2X AST-GCN ìˆ˜ì •ëœ ì´ìƒíƒì§€ ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ!")
print("ğŸ¯ ì£¼ìš” ìˆ˜ì • ì‚¬í•­:")
print(f"  - ë™ì  ì„ê³„ê°’: {threshold_used:.4f} (ë°ì´í„° ë¶„í¬ ê¸°ë°˜)")
print(f"  - ê°œì„ ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight:.2f} (ìµœëŒ€ {MAX_CLASS_WEIGHT})")
print(f"  - ìˆ˜ì •ëœ í‰ê°€: ì¼ê´€ëœ ì„ê³„ê°’ ì‚¬ìš©")
print(f"  - ì˜ˆì¸¡ ë¶„í¬ ëª¨ë‹ˆí„°ë§: ì‹¤ì‹œê°„ í™•ì¸")
print(f"  - ROC ê¸°ë°˜ ë¶„ì„: ìµœì  ì„ê³„ê°’ ì œì•ˆ")

print("\nğŸ“‹ ìˆ˜ì •ëœ ê²°ê³¼ íŒŒì¼:")
print("  - test_anomaly_result_fixed.csv: ìˆ˜ì •ëœ ì˜ˆì¸¡ ê²°ê³¼")
print("  - fixed_anomaly_evaluation.csv: í•µì‹¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­")
print("  - fixed_training_curves.csv: ì—í¬í¬ë³„ í•™ìŠµ ê³¡ì„ ")
print("  - fixed_training_metadata.json: ìˆ˜ì •ëœ ì„¤ì • ì •ë³´")

if test_f1 and len(test_f1) > 0:
    final_f1 = max(test_f1)
    if final_f1 > 0.3:
        print("\nâœ… ìˆ˜ì • ì„±ê³µ!")
        print("  - ê· í˜•ì¡íŒ ì„±ëŠ¥ ë‹¬ì„±")
        print("  - ì‹¤ì œ ìš´ì˜ í™˜ê²½ ì ìš© ê°€ëŠ¥")
    else:
        print("\nğŸ”§ ì¶”ê°€ ê°œì„  ê¶Œì¥:")
        print(f"  - ì„ê³„ê°’ ì¶”ê°€ ì¡°ì •: {threshold_used:.4f} â†’ {threshold_used * 0.8:.4f}")
        print(f"  - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¦ê°€: {pos_weight:.2f} â†’ {min(pos_weight * 1.5, MAX_CLASS_WEIGHT):.2f}")
else:
    print("\nâŒ í•™ìŠµ ê²°ê³¼ê°€ ì—†ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")