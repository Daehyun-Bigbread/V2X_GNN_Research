# -*- coding: utf-8 -*-

import pickle as pkl
import tensorflow as tf

# Apple GPU (Metal) ì„¤ì • - TF 2.x ë„¤ì´í‹°ë¸Œ ëª¨ë“œì—ì„œ
print("ğŸ” GPU ì„¤ì • ì¤‘...")
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        # GPU ë©”ëª¨ë¦¬ ì¦ê°€ ì„¤ì •
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print(f"ğŸš€ Apple GPU (Metal) í™œì„±í™” ì„±ê³µ! - {gpus[0]}")
    except RuntimeError as e:
        print(f"âš ï¸ GPU ì„¤ì • ì˜¤ë¥˜: {e}")
else:
    print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")

# ì´ì œ TF 1.x í˜¸í™˜ ëª¨ë“œë¡œ ì „í™˜ (GPU ì„¤ì • ì´í›„)
import tensorflow.compat.v1 as tf_v1
tf_v1.disable_v2_behavior()

import pandas as pd
import numpy as np
import math
import os
import numpy.linalg as la
from acell import preprocess_data, load_assist_data, load_v2x_data
from tgcn import tgcnCell

from visualization import plot_result,plot_error
from sklearn.metrics import mean_squared_error,mean_absolute_error
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import time

time_start = time.time()

###### Settings ######
flags = tf_v1.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_float('learning_rate', 0.0001, 'Initial learning rate.')  # í•™ìŠµë¥  ë‚®ì¶¤
flags.DEFINE_integer('training_epoch', 20, 'Number of epochs to train.')
flags.DEFINE_integer('gru_units', 128, 'hidden units of gru.')
flags.DEFINE_integer('seq_len',10 , 'time length of inputs.')
flags.DEFINE_integer('pre_len', 3, 'time length of prediction.')
flags.DEFINE_float('train_rate', 0.8, 'rate of training set.')
flags.DEFINE_integer('batch_size', 32, 'batch size.')
flags.DEFINE_string('dataset', 'v2x', 'dataset')
flags.DEFINE_string('model_name', 'ast-gcn', 'ast-gcn')
flags.DEFINE_integer('scheme', 3, 'scheme')
flags.DEFINE_string('noise_name', 'None', 'None or Gauss or Possion')
flags.DEFINE_float('noise_param', 0, 'Parameter for noise')

model_name = FLAGS.model_name
noise_name = FLAGS.noise_name
data_name = FLAGS.dataset
train_rate =  FLAGS.train_rate
seq_len = FLAGS.seq_len
output_dim = pre_len = FLAGS.pre_len
batch_size = FLAGS.batch_size
lr = FLAGS.learning_rate
training_epoch = FLAGS.training_epoch
gru_units = FLAGS.gru_units
scheme = FLAGS.scheme
PG = FLAGS.noise_param

###### load data ######
print(f"ğŸš— ë°ì´í„° ë¡œë”©: {data_name}")

if data_name == 'v2x':
    data, adj, poi_data, weather_data = load_v2x_data('v2x')
    print(f"âœ… V2X ë°ì´í„° ë¡œë”© ì™„ë£Œ")
elif data_name == 'sz':
    data, adj = load_assist_data('sz')
    poi_data, weather_data = None, None
    print(f"âœ… Shenzhen ë°ì´í„° ë¡œë”© ì™„ë£Œ")
else:
    raise ValueError(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {data_name}")

### Perturbation Analysis
def MaxMinNormalization(x,Max,Min):
    x = (x-Min)/(Max-Min)
    return x

if noise_name == 'Gauss':
    Gauss = np.random.normal(0,PG,size=data.shape)
    noise_Gauss = MaxMinNormalization(Gauss,np.max(Gauss),np.min(Gauss))
    data = data + noise_Gauss
elif noise_name == 'Possion':
    Possion = np.random.poisson(PG,size=data.shape)
    noise_Possion = MaxMinNormalization(Possion,np.max(Possion),np.min(Possion))
    data = data + noise_Possion
else:
    data = data

time_len = data.shape[0]
num_nodes = data.shape[1]
data1 = np.mat(data, dtype=np.float32)

#### normalization
max_value = np.max(data1)
data1 = data1/max_value

if hasattr(data, 'columns'):
    data1.columns = data.columns

# ëª¨ë¸ ë° ìŠ¤í‚´ ì •ë³´ ì¶œë ¥
if model_name == 'ast-gcn':
    if scheme == 1:
        name = 'V2X POI only' if data_name == 'v2x' else 'add poi dim'
    elif scheme == 2:
        name = 'V2X Weather only' if data_name == 'v2x' else 'add weather dim'
    else:
        name = 'V2X POI + Weather' if data_name == 'v2x' else 'add poi + weather dim'
else:
    name = 'tgcn'

print('ğŸ“Š ëª¨ë¸ ì •ë³´:')
print(f'   model: {model_name}')
print(f'   dataset: {data_name}')
print(f'   scheme: {scheme} ({name})')
print(f'   data shape: {data1.shape}')
print(f'   time_len: {time_len}, num_nodes: {num_nodes}')
print(f'   noise_name: {noise_name}')
print(f'   noise_param: {PG}')

# ì „ì²˜ë¦¬
print(f"\nğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
trainX, trainY, testX, testY = preprocess_data(
    data1, time_len, train_rate, seq_len, pre_len, model_name, scheme,
    poi_data, weather_data
)

totalbatch = int(trainX.shape[0]/batch_size)
training_data_count = len(trainX)

print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ:")
print(f"   total batches: {totalbatch}")
print(f"   training samples: {training_data_count}")

def TGCN(_X, _weights, _biases):
    ###
    cell_1 = tgcnCell(gru_units, adj, num_nodes=num_nodes)
    cell = tf_v1.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)
    _X = tf_v1.unstack(_X, axis=1)
    outputs, states = tf_v1.nn.static_rnn(cell, _X, dtype=tf_v1.float32)
    m = []
    for i in outputs:
        o = tf_v1.reshape(i,shape=[-1,num_nodes,gru_units])
        o = tf_v1.reshape(o,shape=[-1,gru_units])
        m.append(o)
    last_output = m[-1]
    output = tf_v1.matmul(last_output, _weights['out']) + _biases['out']
    output = tf_v1.reshape(output,shape=[-1,num_nodes,pre_len])
    output = tf_v1.transpose(output, perm=[0,2,1])
    output = tf_v1.reshape(output, shape=[-1,num_nodes])
    
    # ìˆ˜ì¹˜ ì•ˆì •ì„± ì¶”ê°€
    output = tf_v1.clip_by_value(output, -100.0, 100.0)
    
    return output, m, states
    
    
###### placeholders ######
print(f"\nğŸ§  ëª¨ë¸ êµ¬ì„±...")

if model_name == 'ast-gcn':
    actual_time_len = trainX.shape[1]
    print(f"   V2X input dimension: {actual_time_len}")
    input_dim = actual_time_len
    
    inputs = tf_v1.placeholder(tf_v1.float32, shape=[None, input_dim, num_nodes])
else:
    inputs = tf_v1.placeholder(tf_v1.float32, shape=[None, seq_len, num_nodes])

labels = tf_v1.placeholder(tf_v1.float32, shape=[None, pre_len, num_nodes])

# Graph weights
weights = {
    'out': tf_v1.Variable(tf_v1.random_normal([gru_units, pre_len], mean=0.0, stddev=0.1), name='weight_o')}  # ì´ˆê¸°ê°’ ì¡°ì •
biases = {
    'out': tf_v1.Variable(tf_v1.random_normal([pre_len], stddev=0.1),name='bias_o')}  # ì´ˆê¸°ê°’ ì¡°ì •

pred,ttts,ttto = TGCN(inputs, weights, biases)

y_pred = pred
      
###### optimizer with gradient clipping ######
lambda_loss = 0.0015
Lreg = lambda_loss * sum(tf_v1.nn.l2_loss(tf_var) for tf_var in tf_v1.trainable_variables())
label = tf_v1.reshape(labels, [-1,num_nodes])

##loss
print('y_pred_shape:', y_pred.shape)
print('label_shape:', label.shape)
loss = tf_v1.reduce_mean(tf_v1.nn.l2_loss(y_pred-label) + Lreg)

##rmse
error = tf_v1.sqrt(tf_v1.reduce_mean(tf_v1.square(y_pred-label)))

# Gradient clipping ì¶”ê°€
opt = tf_v1.train.AdamOptimizer(lr)
grads_and_vars = opt.compute_gradients(loss)
clipped_grads_and_vars = [(tf_v1.clip_by_value(grad, -1.0, 1.0), var) 
                          for grad, var in grads_and_vars if grad is not None]
optimizer = opt.apply_gradients(clipped_grads_and_vars)

###### Initialize session ######
variables = tf_v1.global_variables()
saver = tf_v1.train.Saver(tf_v1.global_variables())  

# GPU ì„¤ì • (TF 1.x ìŠ¤íƒ€ì¼)
config = tf_v1.ConfigProto()
config.allow_soft_placement = True
config.gpu_options.allow_growth = True  # GPU ë©”ëª¨ë¦¬ ì ì§„ì  í• ë‹¹

sess = tf_v1.Session(config=config)
sess.run(tf_v1.global_variables_initializer())

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
if data_name == 'v2x':
    out = f'out/v2x_{model_name}_scheme{scheme}_gpu'
else:
    out = f'out/{model_name}_{noise_name}_gpu'

path1 = f'{model_name}_{name}_{data_name}_lr{lr}_batch{batch_size}_unit{gru_units}_seq{seq_len}_pre{pre_len}_epoch{training_epoch}_scheme{scheme}_PG{PG}_GPU'
path = os.path.join(out, path1)
if not os.path.exists(path):
    os.makedirs(path)
    
print(f"ğŸ“‚ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {path}")

###### NaN ì•ˆì „ evaluation í•¨ìˆ˜ ######
def evaluation(a, b):
    """V2X ë°ì´í„°ìš© NaN ì•ˆì „ í‰ê°€ í•¨ìˆ˜"""
    
    print(f"ğŸ” í‰ê°€ ë°ì´í„° ì²´í¬:")
    print(f"   ì‹¤ì œê°’(a) í˜•íƒœ: {a.shape}")
    print(f"   ì˜ˆì¸¡ê°’(b) í˜•íƒœ: {b.shape}")
    
    # 1ì°¨ì›ìœ¼ë¡œ flatten
    a_flat = a.flatten()
    b_flat = b.flatten()
    
    # NaN/Inf ê²€ì‚¬
    a_nan_count = np.isnan(a_flat).sum()
    b_nan_count = np.isnan(b_flat).sum()
    a_inf_count = np.isinf(a_flat).sum()
    b_inf_count = np.isinf(b_flat).sum()
    
    print(f"   ì‹¤ì œê°’ NaN: {a_nan_count}, Inf: {a_inf_count}")
    print(f"   ì˜ˆì¸¡ê°’ NaN: {b_nan_count}, Inf: {b_inf_count}")
    
    # ì „ì²´ ë°ì´í„°ê°€ ë¬¸ì œì¸ ê²½ìš°
    total_elements = len(a_flat)
    problem_ratio = (a_nan_count + b_nan_count + a_inf_count + b_inf_count) / (2 * total_elements)
    
    if problem_ratio > 0.5:
        print(f"âŒ ë¬¸ì œ ë°ì´í„° ë¹„ìœ¨: {problem_ratio:.1%} - ê¸°ë³¸ê°’ ë°˜í™˜")
        return 999.0, 999.0, -999.0, -999.0, -999.0
    
    # ì•ˆì „í•œ ë°ì´í„° ì²˜ë¦¬
    # NaN/Infë¥¼ ì¤‘ê°„ê°’ìœ¼ë¡œ ëŒ€ì²´
    a_median = np.nanmedian(a_flat[np.isfinite(a_flat)])
    b_median = np.nanmedian(b_flat[np.isfinite(b_flat)])
    
    if np.isnan(a_median):
        a_median = 0.0
    if np.isnan(b_median):
        b_median = 0.0
    
    a_clean = np.where(np.isfinite(a_flat), a_flat, a_median)
    b_clean = np.where(np.isfinite(b_flat), b_flat, b_median)
    
    # ê·¹ê°’ í´ë¦¬í•‘ (outlier ì œê±°)
    a_std = np.std(a_clean)
    b_std = np.std(b_clean)
    a_mean = np.mean(a_clean)
    b_mean = np.mean(b_clean)
    
    # 3-sigma ê·œì¹™ ì ìš©
    a_clipped = np.clip(a_clean, a_mean - 3*a_std, a_mean + 3*a_std)
    b_clipped = np.clip(b_clean, b_mean - 3*b_std, b_mean + 3*b_std)
    
    try:
        # ì•ˆì „í•œ ë©”íŠ¸ë¦­ ê³„ì‚°
        rmse = math.sqrt(mean_squared_error(a_clipped, b_clipped))
        mae = mean_absolute_error(a_clipped, b_clipped)
        
        # Frobenius norm (ì•ˆì „í•˜ê²Œ)
        diff_norm = la.norm(a_clipped - b_clipped)
        a_norm = la.norm(a_clipped)
        if a_norm < 1e-10:
            F_norm = 0.0
        else:
            F_norm = diff_norm / a_norm
        
        # R2 ê³„ì‚° (ë¶„ì‚° ì²´í¬)
        a_var = np.var(a_clipped)
        if a_var < 1e-10:
            r2 = 0.0
            print("âš ï¸ ì‹¤ì œê°’ ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìŒ, R2=0 ì„¤ì •")
        else:
            ss_res = np.sum((a_clipped - b_clipped) ** 2)
            ss_tot = np.sum((a_clipped - np.mean(a_clipped)) ** 2)
            r2 = 1 - (ss_res / (ss_tot + 1e-10))
            r2 = max(r2, -1.0)  # R2 í•˜í•œ ì„¤ì •
        
        # Variance score
        var_a = np.var(a_clipped)
        var_diff = np.var(a_clipped - b_clipped)
        if var_a < 1e-10:
            var_score = 0.0
        else:
            var_score = 1 - (var_diff / var_a)
        
        print(f"ğŸ“Š í‰ê°€ ê²°ê³¼:")
        print(f"   RMSE: {rmse:.4f}")
        print(f"   MAE: {mae:.4f}")
        print(f"   F_norm: {1-F_norm:.4f}")
        print(f"   R2: {r2:.4f}")
        print(f"   Var Score: {var_score:.4f}")
        
        return rmse, mae, 1-F_norm, r2, var_score
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ì´ë¼ë„ ê³„ì‚°
        try:
            rmse = np.sqrt(np.mean((a_clipped - b_clipped)**2))
            mae = np.mean(np.abs(a_clipped - b_clipped))
            acc = 0.5
            r2 = 0.0
            var_score = 0.0
            print(f"âš¡ ê¸°ë³¸ ë©”íŠ¸ë¦­: RMSE={rmse:.4f}, MAE={mae:.4f}")
            return rmse, mae, acc, r2, var_score
        except:
            print("âŒ ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨")
            return 999.0, 999.0, -999.0, -999.0, -999.0

print(f"\nğŸš€ V2X AST-GCN GPU í•™ìŠµ ì‹œì‘...")
print(f"   Epochs: {training_epoch}")
print(f"   Batch size: {batch_size}")
print(f"   Learning rate: {lr}")
   
x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []
test_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[]
  
for epoch in range(training_epoch):
    for m in range(totalbatch):
        mini_batch = trainX[m * batch_size : (m+1) * batch_size]
        mini_label = trainY[m * batch_size : (m+1) * batch_size]
        
        # NaN ì²´í¬
        if np.isnan(mini_batch).any() or np.isnan(mini_label).any():
            print(f"âš ï¸ Epoch {epoch}, Batch {m}: ì…ë ¥ ë°ì´í„°ì— NaN ë°œê²¬, ê±´ë„ˆëœ€")
            continue
            
        _, loss1, rmse1, train_output = sess.run([optimizer, loss, error, y_pred],
                                                 feed_dict = {inputs:mini_batch, labels:mini_label})
        
        # í•™ìŠµ ì¤‘ NaN ì²´í¬
        if np.isnan(loss1) or np.isnan(rmse1):
            print(f"âš ï¸ Epoch {epoch}, Batch {m}: í•™ìŠµ ì¤‘ NaN ë°œìƒ")
            print(f"   loss: {loss1}, rmse: {rmse1}")
            continue
            
        batch_loss.append(loss1)
        batch_rmse.append(rmse1 * max_value)

    # Test completely at every epoch
    try:
        loss2, rmse2, test_output = sess.run([loss, error, y_pred],
                                             feed_dict = {inputs:testX, labels:testY})
        
        # í…ŒìŠ¤íŠ¸ ì¤‘ NaN ì²´í¬
        if np.isnan(loss2) or np.isnan(rmse2) or np.isnan(test_output).any():
            print(f"âš ï¸ Epoch {epoch}: í…ŒìŠ¤íŠ¸ ì¤‘ NaN ë°œìƒ")
            print(f"   test_loss: {loss2}, test_rmse: {rmse2}")
            print(f"   test_output NaN ê°œìˆ˜: {np.isnan(test_output).sum()}")
            
            # NaNì„ ì•ˆì „í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´
            test_output = np.nan_to_num(test_output, nan=0.0, posinf=1.0, neginf=-1.0)
            loss2 = 999.0 if np.isnan(loss2) else loss2
            rmse2 = 999.0 if np.isnan(rmse2) else rmse2

        testoutput = np.abs(test_output)
        test_label = np.reshape(testY,[-1,num_nodes])
        rmse, mae, acc, r2_score, var_score = evaluation(test_label, testoutput)
        test_label1 = test_label * max_value
        test_output1 = testoutput * max_value
        test_loss.append(loss2)
        test_rmse.append(rmse * max_value)
        test_mae.append(mae * max_value)
        test_acc.append(acc)
        test_r2.append(r2_score)
        test_var.append(var_score)
        test_pred.append(test_output1)
        
        print('Epoch:{}'.format(epoch),
              'train_rmse:{:.4}'.format(batch_rmse[-1] if batch_rmse else 0),
              'test_loss:{:.4}'.format(loss2),
              'test_rmse:{:.4}'.format(rmse * max_value),
              'test_acc:{:.4}'.format(acc))
        
    except Exception as e:
        print(f"âŒ Epoch {epoch} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        continue
    
    if (epoch % 20 == 0) and epoch > 0:
        model_path = os.path.join(path, 'model_100')
        if not os.path.exists(model_path):
            os.makedirs(model_path)
        saver.save(sess, f'{model_path}/V2X_ASTGCN_GPU_pre_{epoch}', global_step = epoch)
        
time_end = time.time()
print(f'\nâ±ï¸ GPU í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {time_end-time_start:.2f}ì´ˆ')

############## visualization ###############
if batch_rmse and test_rmse:  # ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ë§Œ
    try:
        b = int(len(batch_rmse)/totalbatch)
        batch_rmse1 = [i for i in batch_rmse]
        train_rmse = [(sum(batch_rmse1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]
        batch_loss1 = [i for i in batch_loss]
        train_loss = [(sum(batch_loss1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]

        index = test_rmse.index(np.min(test_rmse))
        test_result = test_pred[index]
        var = pd.DataFrame(test_result)
        var.to_csv(path+'/test_result.csv',index = False,header = False)
        plot_result(test_result,test_label1,path)
        plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path)

        evaluation_metrics = []
        evaluation_metrics.append(np.min(test_rmse))
        evaluation_metrics.append(test_mae[index])
        evaluation_metrics.append(test_acc[index])
        evaluation_metrics.append(test_r2[index])
        evaluation_metrics.append(test_var[index])
        evaluation_df = pd.DataFrame(evaluation_metrics)
        evaluation_df.to_csv(path+'/evaluation.csv',index=False,header=None)

        print(f'\nğŸ‰ V2X AST-GCN GPU ê²°ê³¼:')
        print(f'   model_name: {model_name}')
        print(f'   dataset: {data_name}')
        print(f'   scheme: {scheme} ({name})')
        print(f'   noise_name: {noise_name}')
        print(f'   PG: {PG}')
        print(f'   min_rmse: {np.min(test_rmse):.4f}')
        print(f'   min_mae: {test_mae[index]:.4f}')
        print(f'   max_acc: {test_acc[index]:.4f}')
        print(f'   r2: {test_r2[index]:.4f}')
        print(f'   var: {test_var[index]:.4f}')
        print(f'ğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {path}')
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
else:
    print("âŒ í•™ìŠµ ê²°ê³¼ê°€ ì—†ì–´ ì €ì¥í•˜ì§€ ì•ŠìŒ")