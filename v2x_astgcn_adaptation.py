#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
V2X ë°ì´í„°ë¥¼ ê´‘ì£¼ ì§€ì—­ ê·¸ë¦¬ë“œ ê¸°ë°˜ AST-GCN ì´ìƒíƒì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

í•µì‹¬ ìˆ˜ì •ì‚¬í•­:
- ê´‘ì£¼ê´‘ì—­ì‹œ ì§€ì—­ì—ë§Œ íŠ¹í™”
- ìœ„ì¹˜ ë°ì´í„° ì´ìƒê°’ ì œê±° ë° ê´‘ì£¼ ë²”ìœ„ ì œí•œ
- ê²©ì ìˆ˜ ìµœì í™” (ê´€ë¦¬ ê°€ëŠ¥í•œ ìˆ˜ì¤€)
- ì´ìƒíƒì§€ ë¡œì§ ê°œì„ 

Author: V2X Grid-based Anomaly Detection Team
Date: 2025-06-07
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# ê´‘ì£¼ê´‘ì—­ì‹œ ì¢Œí‘œ ë²”ìœ„ ì •ì˜
GWANGJU_BOUNDS = {
    'lon_min': 126.7, 'lon_max': 127.2,  # ê²½ë„ ë²”ìœ„ (ì•½ 50km)
    'lat_min': 35.0, 'lat_max': 35.3      # ìœ„ë„ ë²”ìœ„ (ì•½ 30km)
}

def load_v2x_data(data_dir):
    """V2X ì›ë³¸ ë°ì´í„° ë¡œë”©"""
    print(f"ğŸ“‚ V2X ì´ìƒíƒì§€ ë°ì´í„° ë¡œë”©: {data_dir}")
    
    # 1. ëª¨ë“  ì£¼í–‰ ë°ì´í„° ë¡œë”©
    raw_files = [f for f in os.listdir(data_dir) if f.endswith('_raw.csv')]
    if not raw_files:
        raise FileNotFoundError(f"âŒ {data_dir}ì—ì„œ *_raw.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    print(f"   ğŸ“„ ë°œê²¬ëœ ì£¼í–‰ ë°ì´í„° íŒŒì¼: {len(raw_files)}ê°œ")
    
    # ëª¨ë“  íŒŒì¼ ë¡œë”© (ì „ì²´ ë°ì´í„° ì‚¬ìš©)
    all_raw_data = []
    max_files = len(raw_files)  # ëª¨ë“  íŒŒì¼ ë¡œë”©
    
    for i, raw_file in enumerate(sorted(raw_files)[:max_files]):
        file_path = os.path.join(data_dir, raw_file)
        try:
            df = pd.read_csv(file_path)
            all_raw_data.append(df)
            print(f"       âœ… {raw_file}: {df.shape}")
        except Exception as e:
            print(f"       âŒ {raw_file} ë¡œë”© ì‹¤íŒ¨: {e}")
    
    # ëª¨ë“  ë°ì´í„° í•©ì¹˜ê¸°
    if all_raw_data:
        raw_df = pd.concat(all_raw_data, ignore_index=True)
        print(f"   ğŸ“Š í†µí•© ì£¼í–‰ ë°ì´í„°: {raw_df.shape}")
    else:
        raise ValueError("âŒ ë¡œë”©ëœ ì£¼í–‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    # 2. í•´ë‹¹í•˜ëŠ” ë¼ë²¨ ë°ì´í„° ë¡œë”©
    label_files = [f for f in os.listdir(data_dir) if f.endswith('_label.jsonl')]
    label_data = []
    
    if label_files:
        print(f"   ğŸ“„ ë°œê²¬ëœ ë¼ë²¨ ë°ì´í„° íŒŒì¼: {len(label_files)}ê°œ")
        
        for i, label_file in enumerate(sorted(label_files)):
            file_path = os.path.join(data_dir, label_file)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    file_labels = []
                    for line in f:
                        file_labels.append(json.loads(line.strip()))
                    label_data.extend(file_labels)
                    print(f"       âœ… {label_file}: {len(file_labels)}ê°œ")
            except Exception as e:
                print(f"       âŒ {label_file} ë¡œë”© ì‹¤íŒ¨: {e}")
        
        print(f"   ğŸ“Š í†µí•© ë¼ë²¨ ë°ì´í„°: {len(label_data)}ê°œ")
    else:
        print("   âš ï¸ ë¼ë²¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì†ë„ ê¸°ë°˜ ì´ìƒíƒì§€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    return raw_df, label_data

def clean_and_filter_location_data(raw_df):
    """
    ğŸ§¹ ìœ„ì¹˜ ë°ì´í„° ì •ì œ ë° ê´‘ì£¼ ë²”ìœ„ í•„í„°ë§
    """
    print("ğŸ§¹ ìœ„ì¹˜ ë°ì´í„° ì •ì œ ì¤‘ (ê´‘ì£¼ ì§€ì—­ íŠ¹í™”)...")
    
    initial_count = len(raw_df)
    bounds = GWANGJU_BOUNDS
    
    # 1. ê¸°ë³¸ ìœ„ì¹˜ ë°ì´í„° í™•ì¸
    if 'LONGITUDE' not in raw_df.columns or 'LATITUDE' not in raw_df.columns:
        print("   âš ï¸ ìœ„ì¹˜ ì»¬ëŸ¼ì´ ì—†ì–´ ê´‘ì£¼ ì§€ì—­ ì¤‘ì‹¬ìœ¼ë¡œ ê°€ìƒ ìœ„ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        # ê´‘ì£¼ ì§€ì—­ ì¤‘ì‹¬ìœ¼ë¡œ ê°€ìƒ ìœ„ì¹˜ ìƒì„±
        num_records = len(raw_df)
        raw_df['LONGITUDE'] = np.random.uniform(bounds['lon_min'], bounds['lon_max'], num_records)
        raw_df['LATITUDE'] = np.random.uniform(bounds['lat_min'], bounds['lat_max'], num_records)
        print(f"   âœ… ê´‘ì£¼ ì§€ì—­ ê°€ìƒ ìœ„ì¹˜ ìƒì„±: {num_records:,}ê°œ")
        return raw_df
    
    # 2. ì´ìƒê°’ ì œê±°
    print("   ğŸ” ìœ„ì¹˜ ë°ì´í„° ì´ìƒê°’ ê²€ì‚¬...")
    
    # ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬
    valid_lon = (raw_df['LONGITUDE'] >= -180) & (raw_df['LONGITUDE'] <= 180)
    valid_lat = (raw_df['LATITUDE'] >= -90) & (raw_df['LATITUDE'] <= 90)
    basic_valid = valid_lon & valid_lat & (raw_df['LONGITUDE'] != 0) & (raw_df['LATITUDE'] != 0)
    
    print(f"     ê¸°ë³¸ ìœ íš¨ ë ˆì½”ë“œ: {basic_valid.sum():,}ê°œ ({basic_valid.mean()*100:.1f}%)")
    
    # ê´‘ì£¼ ì§€ì—­ ë²”ìœ„ í•„í„°ë§
    in_gwangju = (
        (raw_df['LONGITUDE'] >= bounds['lon_min']) & 
        (raw_df['LONGITUDE'] <= bounds['lon_max']) &
        (raw_df['LATITUDE'] >= bounds['lat_min']) & 
        (raw_df['LATITUDE'] <= bounds['lat_max'])
    )
    
    gwangju_count = in_gwangju.sum()
    print(f"     ê´‘ì£¼ ì§€ì—­ ë‚´ ë ˆì½”ë“œ: {gwangju_count:,}ê°œ ({gwangju_count/initial_count*100:.1f}%)")
    
    # 3. í•„í„°ë§ ì „ëµ ê²°ì •
    if gwangju_count < initial_count * 0.1:  # ê´‘ì£¼ ì§€ì—­ ë°ì´í„°ê°€ 10% ë¯¸ë§Œì¸ ê²½ìš°
        print("   âš ï¸ ê´‘ì£¼ ì§€ì—­ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì „ì²´ ë°ì´í„°ë¥¼ ê´‘ì£¼ ë²”ìœ„ë¡œ ì¬ë§¤í•‘í•©ë‹ˆë‹¤.")
        
        # ì „ì²´ ë°ì´í„°ì˜ ìœ„ì¹˜ë¥¼ ê´‘ì£¼ ë²”ìœ„ë¡œ ì •ê·œí™”
        if basic_valid.sum() > 0:
            valid_df = raw_df[basic_valid].copy()
            
            # ì›ë˜ ìœ„ì¹˜ ë°ì´í„°ì˜ ë²”ìœ„ ê³„ì‚°
            orig_lon_min, orig_lon_max = valid_df['LONGITUDE'].min(), valid_df['LONGITUDE'].max()
            orig_lat_min, orig_lat_max = valid_df['LATITUDE'].min(), valid_df['LATITUDE'].max()
            
            # ê´‘ì£¼ ë²”ìœ„ë¡œ ì •ê·œí™”
            lon_normalized = (valid_df['LONGITUDE'] - orig_lon_min) / (orig_lon_max - orig_lon_min)
            lat_normalized = (valid_df['LATITUDE'] - orig_lat_min) / (orig_lat_max - orig_lat_min)
            
            valid_df['LONGITUDE'] = bounds['lon_min'] + lon_normalized * (bounds['lon_max'] - bounds['lon_min'])
            valid_df['LATITUDE'] = bounds['lat_min'] + lat_normalized * (bounds['lat_max'] - bounds['lat_min'])
            
            print(f"   âœ… ìœ„ì¹˜ ì¬ë§¤í•‘ ì™„ë£Œ: {len(valid_df):,}ê°œ ë ˆì½”ë“œ")
            return valid_df
        else:
            # ì™„ì „íˆ ìƒˆë¡œìš´ ê°€ìƒ ìœ„ì¹˜ ìƒì„±
            print("   ğŸ†• ê´‘ì£¼ ì§€ì—­ ê°€ìƒ ìœ„ì¹˜ ìƒì„±")
            raw_df['LONGITUDE'] = np.random.uniform(bounds['lon_min'], bounds['lon_max'], len(raw_df))
            raw_df['LATITUDE'] = np.random.uniform(bounds['lat_min'], bounds['lat_max'], len(raw_df))
            return raw_df
    else:
        # ê´‘ì£¼ ì§€ì—­ ë°ì´í„°ë§Œ ì‚¬ìš©
        filtered_df = raw_df[basic_valid & in_gwangju].copy()
        print(f"   âœ… ê´‘ì£¼ ì§€ì—­ í•„í„°ë§ ì™„ë£Œ: {len(filtered_df):,}ê°œ ë ˆì½”ë“œ ìœ ì§€")
        return filtered_df

def create_anomaly_labels(raw_df, label_data):
    """V2X ë°ì´í„°ì—ì„œ ì´ìƒìƒí™© ë¼ë²¨ ìƒì„± (ê°œì„ ëœ ë¡œì§)"""
    print("ğŸš¨ ì´ìƒìƒí™© ë¼ë²¨ ìƒì„± ì¤‘...")
    
    # ì°¨ëŸ‰ ID ë§¤í•‘
    if 'VEHICLE_ID' in raw_df.columns:
        raw_df['TRIP_ID'] = raw_df['VEHICLE_ID']
    
    # ì´ìƒ ì ìˆ˜ ì´ˆê¸°í™”
    raw_df['anomaly_score'] = 0.0
    
    # 1. ì†ë„ ê¸°ë°˜ ì´ìƒíƒì§€ (ë” ì—„ê²©í•œ ê¸°ì¤€)
    print("   ğŸš— ì†ë„ ê¸°ë°˜ ì´ìƒíƒì§€...")
    
    # ì €ì† ì´ìƒ (3km/h ë¯¸ë§Œ = ì •ì²´/ì‚¬ê³ ) - ë” ì—„ê²©í•˜ê²Œ
    low_speed_mask = raw_df['SPEED'] < 3
    raw_df.loc[low_speed_mask, 'anomaly_score'] += 0.3
    print(f"     ì €ì† ì´ìƒ (< 3km/h): {low_speed_mask.sum():,}ê±´")
    
    # ê³ ì† ì´ìƒ (ê´‘ì£¼ ì‹œë‚´ ê¸°ì¤€ 70km/h ì´ˆê³¼) - ë” ì—„ê²©í•˜ê²Œ
    high_speed_mask = raw_df['SPEED'] > 70
    raw_df.loc[high_speed_mask, 'anomaly_score'] += 0.2
    print(f"     ê³ ì† ì´ìƒ (> 70km/h): {high_speed_mask.sum():,}ê±´")
    
    # ê¸‰ë³€ì† ì´ìƒ (ì†ë„ ë³€í™”ê°€ í° ê²½ìš°)
    if len(raw_df) > 1:
        raw_df['speed_diff'] = raw_df['SPEED'].diff().abs()
        rapid_change_mask = raw_df['speed_diff'] > 30  # 30km/h ì´ìƒ ê¸‰ë³€
        raw_df.loc[rapid_change_mask, 'anomaly_score'] += 0.2
        print(f"     ê¸‰ë³€ì† ì´ìƒ (> 30km/h ë³€í™”): {rapid_change_mask.sum():,}ê±´")
    
    # 2. ë¸Œë ˆì´í¬ ê¸°ë°˜ ì´ìƒíƒì§€
    if 'BRAKE_STATUS' in raw_df.columns:
        print("   ğŸ›‘ ê¸‰ì œë™ ê¸°ë°˜ ì´ìƒíƒì§€...")
        brake_mask = raw_df['BRAKE_STATUS'] == 1
        raw_df.loc[brake_mask, 'anomaly_score'] += 0.15
        print(f"     ê¸‰ì œë™ ì´ìƒ: {brake_mask.sum():,}ê±´")
    
    # 3. ë¼ë²¨ ê¸°ë°˜ ì´ìƒíƒì§€ (ë” ì‹ ì¤‘í•˜ê²Œ)
    if label_data:
        print("   ğŸ·ï¸ ë¼ë²¨ ê¸°ë°˜ ì´ìƒíƒì§€...")
        
        # ë¼ë²¨ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        label_dict = {}
        for label in label_data:
            annotation = label.get('Annotation', {})
            vehicle_id = annotation.get('Vehicle_ID')
            if vehicle_id:
                if vehicle_id not in label_dict:
                    label_dict[vehicle_id] = []
                label_dict[vehicle_id].append(annotation)
        
        hazard_count = 0
        for vehicle_id, vehicle_labels in label_dict.items():
            vehicle_mask = raw_df['TRIP_ID'] == vehicle_id
            
            for annotation in vehicle_labels:
                # Hazard=Trueì¸ ê²½ìš°ë§Œ ë†’ì€ ì´ìƒì ìˆ˜
                if annotation.get('Hazard') == 'True':
                    raw_df.loc[vehicle_mask, 'anomaly_score'] += 0.4
                    hazard_count += vehicle_mask.sum()
        
        print(f"     ìœ„í—˜ ë¼ë²¨ ì´ìƒ: {hazard_count:,}ê±´")
    
    # 4. ì´ìƒ ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„)
    raw_df['anomaly_score'] = np.clip(raw_df['anomaly_score'], 0, 1)
    
    # 5. ì´ì§„ ë¼ë²¨ ìƒì„± (ì„ê³„ê°’ 0.4ë¡œ ìƒí–¥ì¡°ì •)
    raw_df['is_anomaly'] = (raw_df['anomaly_score'] >= 0.4).astype(int)
    
    # í†µê³„ ì¶œë ¥
    total_records = len(raw_df)
    anomaly_records = raw_df['is_anomaly'].sum()
    anomaly_ratio = anomaly_records / total_records
    
    print(f"   âœ… ì´ìƒíƒì§€ ë¼ë²¨ ìƒì„± ì™„ë£Œ:")
    print(f"     ì „ì²´ ë ˆì½”ë“œ: {total_records:,}ê°œ")
    print(f"     ì´ìƒ ë ˆì½”ë“œ: {anomaly_records:,}ê°œ")
    print(f"     ì´ìƒ ë¹„ìœ¨: {anomaly_ratio*100:.2f}%")
    print(f"     í‰ê·  ì´ìƒ ì ìˆ˜: {raw_df['anomaly_score'].mean():.3f}")
    
    return raw_df

def create_grid_system(raw_df, grid_size=0.01):
    """
    ğŸ—ºï¸ ê´‘ì£¼ ì§€ì—­ ê·¸ë¦¬ë“œ ì‹œìŠ¤í…œ ìƒì„± (ìµœì í™”ëœ ê²©ì í¬ê¸°)
    
    Args:
        raw_df: V2X ì°¨ëŸ‰ ë°ì´í„°
        grid_size: ê²©ì í¬ê¸° (ë„ ë‹¨ìœ„, 0.01ë„ â‰ˆ 1.1km)
    
    Returns:
        grid_centers: ê²©ì ì¤‘ì‹¬ì ë“¤
        vehicle_to_grid: ì°¨ëŸ‰ ë ˆì½”ë“œë³„ ì†Œì† ê²©ì ë§¤í•‘
        grid_info: ê²©ì ì •ë³´
    """
    print(f"ğŸ—ºï¸ ê´‘ì£¼ ì§€ì—­ ê·¸ë¦¬ë“œ ì‹œìŠ¤í…œ ìƒì„± (ê²©ì í¬ê¸°: {grid_size:.3f}ë„ â‰ˆ {int(grid_size * 111)}km)")
    
    # 1. ìœ„ì¹˜ ë°ì´í„° í™•ì¸
    bounds = GWANGJU_BOUNDS
    
    # ì‹¤ì œ ë°ì´í„° ë²”ìœ„ vs ê´‘ì£¼ ë²”ìœ„
    actual_lon_range = (raw_df['LONGITUDE'].min(), raw_df['LONGITUDE'].max())
    actual_lat_range = (raw_df['LATITUDE'].min(), raw_df['LATITUDE'].max())
    
    print(f"   ğŸ“Š ë°ì´í„° ë²”ìœ„:")
    print(f"     ê²½ë„: {actual_lon_range[0]:.6f} ~ {actual_lon_range[1]:.6f}")
    print(f"     ìœ„ë„: {actual_lat_range[0]:.6f} ~ {actual_lat_range[1]:.6f}")
    
    # 2. ê²©ì ë²”ìœ„ ì„¤ì • (ì‹¤ì œ ë°ì´í„° ë²”ìœ„ + ì—¬ìœ ê³µê°„)
    margin = grid_size * 0.5
    min_lon = max(bounds['lon_min'], actual_lon_range[0] - margin)
    max_lon = min(bounds['lon_max'], actual_lon_range[1] + margin)
    min_lat = max(bounds['lat_min'], actual_lat_range[0] - margin)
    max_lat = min(bounds['lat_max'], actual_lat_range[1] + margin)
    
    # 3. ê²©ì ìƒì„±
    lon_grids = np.arange(min_lon, max_lon + grid_size, grid_size)
    lat_grids = np.arange(min_lat, max_lat + grid_size, grid_size)
    
    print(f"   ğŸ“ ê²©ì ì •ë³´:")
    print(f"     ê²½ë„ ê²©ì: {len(lon_grids)-1}ê°œ")
    print(f"     ìœ„ë„ ê²©ì: {len(lat_grids)-1}ê°œ")
    print(f"     ì´ ê²©ì: {(len(lon_grids)-1) * (len(lat_grids)-1)}ê°œ")
    
    # ê²©ì ìˆ˜ ì²´í¬ (ë„ˆë¬´ ë§ìœ¼ë©´ ê²©ì í¬ê¸° ì¡°ì •)
    total_grids = (len(lon_grids)-1) * (len(lat_grids)-1)
    if total_grids > 500:  # 500ê°œ ì´ˆê³¼ì‹œ ê²©ì í¬ê¸° ì¦ê°€
        new_grid_size = grid_size * 2
        print(f"   âš ï¸ ê²©ì ìˆ˜ê°€ ë„ˆë¬´ ë§ìŒ ({total_grids}ê°œ). ê²©ì í¬ê¸°ë¥¼ {new_grid_size:.3f}ë„ë¡œ ì¦ê°€")
        return create_grid_system(raw_df, new_grid_size)
    
    # 4. ê²©ì ì¤‘ì‹¬ì  ë° ID ìƒì„±
    grid_centers = []
    grid_ids = []
    grid_mapping = {}  # (lon_idx, lat_idx) -> grid_index
    
    grid_index = 0
    for i, lon in enumerate(lon_grids[:-1]):
        for j, lat in enumerate(lat_grids[:-1]):
            center_lon = lon + grid_size / 2
            center_lat = lat + grid_size / 2
            grid_centers.append([center_lon, center_lat])
            grid_ids.append(f"Grid_{i}_{j}")
            grid_mapping[(i, j)] = grid_index
            grid_index += 1
    
    grid_centers = np.array(grid_centers)
    print(f"   âœ… ì „ì²´ ê²©ì ìƒì„±: {len(grid_centers)}ê°œ")
    
    # 5. ì°¨ëŸ‰ ë ˆì½”ë“œë¥¼ ê²©ìì— í• ë‹¹
    print("   ğŸš— ì°¨ëŸ‰ ë ˆì½”ë“œë¥¼ ê²©ìì— í• ë‹¹ ì¤‘...")
    
    vehicle_to_grid = {}
    grid_record_counts = np.zeros(len(grid_centers))
    
    for idx in raw_df.index:
        lon = raw_df.loc[idx, 'LONGITUDE']
        lat = raw_df.loc[idx, 'LATITUDE']
        
        # í•´ë‹¹ ë ˆì½”ë“œê°€ ì†í•œ ê²©ì ì°¾ê¸°
        lon_idx = int((lon - min_lon) / grid_size)
        lat_idx = int((lat - min_lat) / grid_size)
        
        # ë²”ìœ„ ì²´í¬
        lon_idx = max(0, min(lon_idx, len(lon_grids) - 2))
        lat_idx = max(0, min(lat_idx, len(lat_grids) - 2))
        
        if (lon_idx, lat_idx) in grid_mapping:
            grid_idx = grid_mapping[(lon_idx, lat_idx)]
            vehicle_to_grid[idx] = grid_idx
            grid_record_counts[grid_idx] += 1
    
    print(f"   ğŸ“ ë ˆì½”ë“œ í• ë‹¹ ì™„ë£Œ: {len(vehicle_to_grid):,}ê°œ ë ˆì½”ë“œ")
    
    # 6. í™œì„± ê²©ì ì„ ë³„ (ë ˆì½”ë“œê°€ ìˆëŠ” ê²©ìë§Œ)
    active_mask = grid_record_counts > 0
    active_centers = grid_centers[active_mask]
    active_ids = [grid_ids[i] for i in range(len(grid_ids)) if active_mask[i]]
    active_counts = grid_record_counts[active_mask]
    
    # ì°¨ëŸ‰ ë§¤í•‘ ì—…ë°ì´íŠ¸ (í™œì„± ê²©ì ì¸ë±ìŠ¤ë¡œ)
    old_to_new_mapping = {}
    new_idx = 0
    for old_idx in range(len(grid_centers)):
        if active_mask[old_idx]:
            old_to_new_mapping[old_idx] = new_idx
            new_idx += 1
    
    updated_vehicle_to_grid = {}
    for record_idx, old_grid_idx in vehicle_to_grid.items():
        if old_grid_idx in old_to_new_mapping:
            updated_vehicle_to_grid[record_idx] = old_to_new_mapping[old_grid_idx]
    
    print(f"   ğŸ”¥ í™œì„± ê²©ì (ë°ì´í„° ìˆìŒ): {len(active_centers)}ê°œ")
    print(f"   ğŸ“Š ê²©ìë³„ í‰ê·  ë ˆì½”ë“œ ìˆ˜: {active_counts.mean():.1f}ê°œ")
    print(f"   ğŸ“Š ê²©ìë³„ ë ˆì½”ë“œ ìˆ˜ ë²”ìœ„: {active_counts.min():.0f} ~ {active_counts.max():.0f}ê°œ")
    
    # ê²©ì ì •ë³´ ë°˜í™˜
    grid_info = {
        'grid_size': grid_size,
        'total_grids': len(grid_centers),
        'active_grids': len(active_centers),
        'grid_centers': active_centers,
        'grid_ids': active_ids,
        'grid_counts': active_counts,
        'lon_range': (min_lon, max_lon),
        'lat_range': (min_lat, max_lat)
    }
    
    return active_centers, updated_vehicle_to_grid, grid_info

def create_grid_adjacency_matrix(grid_centers, connection_threshold=1500):
    """ğŸ”— ê²©ì ê°„ ì¸ì ‘í–‰ë ¬ ìƒì„±"""
    print(f"ğŸ”— ê²©ì ì¸ì ‘í–‰ë ¬ ìƒì„± (ì—°ê²° ì„ê³„ê°’: {connection_threshold}m)")
    
    num_grids = len(grid_centers)
    adjacency_matrix = np.zeros((num_grids, num_grids))
    
    for i in range(num_grids):
        for j in range(num_grids):
            if i == j:
                adjacency_matrix[i, j] = 1.0  # ìê¸° ìì‹ ê³¼ëŠ” ì™„ì „ ì—°ê²°
            else:
                # ê²©ì ê°„ ê±°ë¦¬ ê³„ì‚° (ë¯¸í„° ë‹¨ìœ„)
                dist_lon = (grid_centers[i, 0] - grid_centers[j, 0]) * 111000  # 1ë„ â‰ˆ 111km
                dist_lat = (grid_centers[i, 1] - grid_centers[j, 1]) * 111000
                distance = np.sqrt(dist_lon**2 + dist_lat**2)
                
                # ì—°ê²° ê°•ë„ ê³„ì‚° (ê±°ë¦¬ ê¸°ë°˜ ì§€ìˆ˜ ê°ì†Œ)
                if distance < connection_threshold:
                    connection_strength = np.exp(-distance / connection_threshold)
                    adjacency_matrix[i, j] = connection_strength
                else:
                    adjacency_matrix[i, j] = 0.0
    
    # ì—°ê²° í†µê³„
    connection_ratio = np.count_nonzero(adjacency_matrix) / (num_grids * num_grids)
    avg_connections = np.count_nonzero(adjacency_matrix, axis=1).mean()
    
    print(f"   âœ… ê²©ì ì¸ì ‘í–‰ë ¬ ì™„ì„±: {num_grids}Ã—{num_grids}")
    print(f"   ğŸ“Š ì—°ê²° ë¹„ìœ¨: {connection_ratio:.3f}")
    print(f"   ğŸ“Š ê²©ìë‹¹ í‰ê·  ì—°ê²° ìˆ˜: {avg_connections:.1f}ê°œ")
    
    return adjacency_matrix

def create_grid_anomaly_matrix(raw_df, vehicle_to_grid, num_grids, time_interval='15min'):
    """ğŸš¨ ê²©ì ê¸°ë°˜ ì‹œê°„Ã—ê²©ì ì´ìƒì ìˆ˜ í–‰ë ¬ ìƒì„±"""
    print(f"ğŸš¨ ê²©ì ê¸°ë°˜ ì´ìƒì ìˆ˜ í–‰ë ¬ ìƒì„± (ê°„ê²©: {time_interval})")
    
    # 1. ì‹œê°„ ë°ì´í„° ì „ì²˜ë¦¬
    if 'ISSUE_DATE' in raw_df.columns:
        try:
            # ISSUE_DATE íŒŒì‹± ì‹œë„ (ë‹¤ì–‘í•œ í˜•ì‹ ì‹œë„)
            raw_df['datetime'] = pd.to_datetime(
                raw_df['ISSUE_DATE'].astype(str), 
                format='%Y%m%d%H%M%S', 
                errors='coerce'
            )
            
            # íŒŒì‹± ì‹¤íŒ¨í•œ ê²½ìš° ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
            if raw_df['datetime'].isna().sum() > len(raw_df) * 0.5:
                raw_df['datetime'] = pd.to_datetime(
                    raw_df['ISSUE_DATE'], 
                    errors='coerce'
                )
            
            # ì—¬ì „íˆ ì‹¤íŒ¨í•˜ë©´ ê· ë“± ë¶„í¬ë¡œ ìƒì„±
            if raw_df['datetime'].isna().sum() > len(raw_df) * 0.5:
                raise ValueError("Too many parsing failures")
            
            print(f"   âœ… ì‹œê°„ íŒŒì‹± ì„±ê³µ: {raw_df['datetime'].min()} ~ {raw_df['datetime'].max()}")
            
        except:
            print(f"   âš ï¸ ISSUE_DATE íŒŒì‹± ì‹¤íŒ¨, 8ì›” ë‚´ ê· ë“± ë¶„í¬ë¡œ ìƒì„±")
            # 8ì›” ì²« 5ì¼ ë‚´ ê· ë“± ë¶„í¬ë¡œ ìƒì„±
            start_date = pd.Timestamp('2022-08-01')
            end_date = pd.Timestamp('2022-08-05 23:59:59')
            raw_df['datetime'] = pd.date_range(start_date, end_date, periods=len(raw_df))
    else:
        print(f"   âš ï¸ ISSUE_DATE ì»¬ëŸ¼ ì—†ìŒ, 8ì›” ë‚´ ê· ë“± ë¶„í¬ë¡œ ìƒì„±")
        start_date = pd.Timestamp('2022-08-01')
        end_date = pd.Timestamp('2022-08-05 23:59:59')
        raw_df['datetime'] = pd.date_range(start_date, end_date, periods=len(raw_df))
    
    # 2. ì‹œê°„ ê°„ê²©ë³„ ê·¸ë£¹í™”
    raw_df['time_bin'] = raw_df['datetime'].dt.floor(time_interval)
    
    # 3. ê²©ì ì •ë³´ ì¶”ê°€ (ë ˆì½”ë“œë³„)
    raw_df['grid_id'] = raw_df.index.map(vehicle_to_grid)
    
    # 4. ê²©ì ë§¤í•‘ì´ ì—†ëŠ” ë ˆì½”ë“œ ì œê±°
    before_filter = len(raw_df)
    raw_df = raw_df.dropna(subset=['grid_id'])
    raw_df['grid_id'] = raw_df['grid_id'].astype(int)
    after_filter = len(raw_df)
    
    print(f"   ğŸ“ ê²©ì ë§¤í•‘ëœ ë ˆì½”ë“œ: {after_filter:,}ê°œ ({after_filter/before_filter*100:.1f}%)")
    
    # 5. ê²©ìë³„ ì‹œê°„ëŒ€ë³„ ì´ìƒì ìˆ˜ ì§‘ê³„
    print("   ğŸ“Š ê²©ìë³„ ì‹œê°„ëŒ€ë³„ ì´ìƒì ìˆ˜ ì§‘ê³„ ì¤‘...")
    
    # ê·¸ë£¹ë³„ í‰ê·  ì´ìƒì ìˆ˜ ê³„ì‚°
    grid_time_anomaly = raw_df.groupby(['time_bin', 'grid_id'])['anomaly_score'].agg([
        'mean',  # í‰ê·  ì´ìƒì ìˆ˜
        'max',   # ìµœëŒ€ ì´ìƒì ìˆ˜  
        'count'  # ë ˆì½”ë“œ ìˆ˜
    ]).reset_index()
    
    # í”¼ë²— í…Œì´ë¸” ìƒì„± (ì‹œê°„ Ã— ê²©ì)
    anomaly_pivot = grid_time_anomaly.pivot_table(
        index='time_bin', 
        columns='grid_id', 
        values='mean',  # í‰ê·  ì´ìƒì ìˆ˜ ì‚¬ìš©
        fill_value=0.0
    )
    
    # 6. ëª¨ë“  ê²©ìê°€ í¬í•¨ë˜ë„ë¡ ë³´ì •
    all_grid_ids = list(range(num_grids))
    for grid_id in all_grid_ids:
        if grid_id not in anomaly_pivot.columns:
            anomaly_pivot[grid_id] = 0.0
    
    # ê²©ì ìˆœì„œëŒ€ë¡œ ì •ë ¬
    anomaly_pivot = anomaly_pivot.reindex(columns=all_grid_ids, fill_value=0.0)
    
    print(f"   âœ… ê²©ì ì´ìƒì ìˆ˜ í–‰ë ¬ ì™„ì„±: {anomaly_pivot.shape} (ì‹œê°„ Ã— ê²©ì)")
    print(f"   ğŸ“Š ì‹œê°„ ë²”ìœ„: {anomaly_pivot.index.min()} ~ {anomaly_pivot.index.max()}")
    print(f"   ğŸš¨ ì „ì²´ í‰ê·  ì´ìƒì ìˆ˜: {anomaly_pivot.values.mean():.3f}")
    
    return anomaly_pivot.values, list(anomaly_pivot.index)

def create_grid_poi_features(grid_centers, grid_info):
    """ğŸ¢ ê²©ìë³„ ì •ì  ì†ì„± ìƒì„± (ê´‘ì£¼ íŠ¹í™”)"""
    print("ğŸ¢ ê²©ìë³„ ì •ì  ì†ì„± (POI) ìƒì„± - ê´‘ì£¼ íŠ¹í™”")
    
    grid_features = []
    num_grids = len(grid_centers)
    
    # ê´‘ì£¼ ì§€ì—­ ì¤‘ì‹¬ì ë“¤
    gwangju_city_center = (126.9, 35.15)  # ê´‘ì£¼ ì‹œì²­ ê·¼ì²˜
    buk_gu_center = (126.92, 35.18)       # ë¶êµ¬ ì¤‘ì‹¬
    dong_gu_center = (126.92, 35.14)      # ë™êµ¬ ì¤‘ì‹¬
    seo_gu_center = (126.88, 35.15)       # ì„œêµ¬ ì¤‘ì‹¬
    nam_gu_center = (126.90, 35.12)       # ë‚¨êµ¬ ì¤‘ì‹¬
    gwangsan_gu_center = (126.95, 35.20)  # ê´‘ì‚°êµ¬ ì¤‘ì‹¬
    
    for i, (center_lon, center_lat) in enumerate(grid_centers):
        features = []
        
        # 1. ê¸°ë³¸ ìœ„ì¹˜ íŠ¹ì„±
        features.append(center_lon)  # ê²½ë„
        features.append(center_lat)  # ìœ„ë„
        
        # 2. ê´‘ì£¼ ì‹œì²­(ë„ì‹¬)ê³¼ì˜ ê±°ë¦¬
        dist_to_center = np.sqrt(
            ((center_lon - gwangju_city_center[0]) * 111000) ** 2 +
            ((center_lat - gwangju_city_center[1]) * 111000) ** 2
        )
        features.append(dist_to_center / 1000)  # km ë‹¨ìœ„
        
        # 3. ë„ì‹¬/ì™¸ê³½ êµ¬ë¶„
        is_downtown = 1.0 if dist_to_center < 2000 else 0.0  # 2km ì´ë‚´ëŠ” ë„ì‹¬
        features.append(is_downtown)
        
        # 4. êµ¬ë³„ íŠ¹ì„± (ê°€ì¥ ê°€ê¹Œìš´ êµ¬ ì¤‘ì‹¬)
        districts = {
            'buk': buk_gu_center,
            'dong': dong_gu_center,
            'seo': seo_gu_center,
            'nam': nam_gu_center,
            'gwangsan': gwangsan_gu_center
        }
        
        district_distances = {}
        for district, (d_lon, d_lat) in districts.items():
            dist = np.sqrt(
                ((center_lon - d_lon) * 111000) ** 2 +
                ((center_lat - d_lat) * 111000) ** 2
            )
            district_distances[district] = dist
        
        # ê°€ì¥ ê°€ê¹Œìš´ êµ¬
        closest_district = min(district_distances, key=district_distances.get)
        
        # êµ¬ë³„ ì›í•« ì¸ì½”ë”©
        for district in districts.keys():
            features.append(1.0 if district == closest_district else 0.0)
        
        # 5. êµí†µ íŠ¹ì„±
        # ì‹œë‚´ ì¤‘ì‹¬ì—ì„œ ê°€ê¹Œìš¸ìˆ˜ë¡ êµí†µ ë°€ë„ ë†’ìŒ
        traffic_density = max(0.1, 1.0 - dist_to_center / 5000)
        features.append(traffic_density)
        
        # 6. ì§€ì—­ íŠ¹ì„± ì‹œë®¬ë ˆì´ì…˜
        # ìƒì—…ì§€ì—­ ì ìˆ˜ (ë„ì‹¬ + ë™êµ¬ì—ì„œ ë†’ìŒ)
        commercial_score = 0.8 if is_downtown else 0.3
        if closest_district == 'dong':  # ë™êµ¬ëŠ” ìƒì—…ì§€ì—­
            commercial_score = max(commercial_score, 0.6)
        features.append(commercial_score)
        
        # ì£¼ê±°ì§€ì—­ ì ìˆ˜ (ê´‘ì‚°êµ¬, ë¶êµ¬ì—ì„œ ë†’ìŒ)
        residential_score = 0.7 if closest_district in ['gwangsan', 'buk'] else 0.4
        features.append(residential_score)
        
        # ì‚°ì—…ì§€ì—­ ì ìˆ˜ (ê´‘ì‚°êµ¬ì—ì„œ ë†’ìŒ)
        industrial_score = 0.8 if closest_district == 'gwangsan' else 0.2
        features.append(industrial_score)
        
        # 7. ë ˆì½”ë“œ ë°€ë„
        if i < len(grid_info['grid_counts']):
            record_density = min(1.0, grid_info['grid_counts'][i] / 1000)
        else:
            record_density = 0.1
        features.append(record_density)
        
        grid_features.append(features)
    
    grid_matrix = np.array(grid_features)
    
    print(f"   âœ… ê²©ì ì •ì  ì†ì„± ì™„ì„±: {grid_matrix.shape} (ê²©ì Ã— íŠ¹ì„±)")
    feature_names = ['ê²½ë„', 'ìœ„ë„', 'ì‹œì²­ê±°ë¦¬', 'ë„ì‹¬ì—¬ë¶€', 'ë¶êµ¬', 'ë™êµ¬', 'ì„œêµ¬', 'ë‚¨êµ¬', 'ê´‘ì‚°êµ¬', 
                    'êµí†µë°€ë„', 'ìƒì—…ì ìˆ˜', 'ì£¼ê±°ì ìˆ˜', 'ì‚°ì—…ì ìˆ˜', 'ë ˆì½”ë“œë°€ë„']
    print(f"   ğŸ“Š íŠ¹ì„± ëª©ë¡: {feature_names}")
    
    return grid_matrix

def create_time_weather_features(time_index):
    """ğŸŒ¤ï¸ ì‹œê°„ë³„ ë™ì  ì†ì„± ìƒì„±"""
    print("ğŸŒ¤ï¸ ì‹œê°„ë³„ ë™ì  ì†ì„± (Weather) ìƒì„±")
    
    weather_features = []
    
    for timestamp in time_index:
        features = []
        
        # 1. ì‹œê°„ íŒ¨í„´
        hour = timestamp.hour
        
        # Period ì›í•« ì¸ì½”ë”©
        period_f = 1.0 if 6 <= hour < 12 else 0.0   # ì˜¤ì „
        period_a = 1.0 if 12 <= hour < 18 else 0.0  # ì˜¤í›„
        period_n = 1.0 if 18 <= hour < 24 else 0.0  # ë°¤
        period_d = 1.0 if 0 <= hour < 6 else 0.0    # ìƒˆë²½
        
        features.extend([period_f, period_a, period_n, period_d])
        
        # 2. ìš”ì¼ ì •ë³´
        weekday = timestamp.weekday()
        is_weekend = 1.0 if weekday >= 5 else 0.0
        is_weekday = 1.0 - is_weekend
        
        features.extend([is_weekday, is_weekend])
        
        # 3. êµí†µ íŒ¨í„´
        rush_morning = 1.0 if 7 <= hour <= 9 else 0.0
        rush_evening = 1.0 if 17 <= hour <= 19 else 0.0
        lunch_time = 1.0 if 11 <= hour <= 13 else 0.0
        night_time = 1.0 if 22 <= hour <= 6 else 0.0
        
        features.extend([rush_morning, rush_evening, lunch_time, night_time])
        
        # 4. ì •ê·œí™”ëœ ì‹œê°„ íŠ¹ì„±
        features.append(hour / 24.0)
        features.append(weekday / 6.0)
        features.append(timestamp.day / 31.0)
        
        # 5. ì´ìƒ ìœ„í—˜ë„ (ê´‘ì£¼ êµí†µ íŒ¨í„´ ë°˜ì˜)
        anomaly_risk = 0.1
        if rush_morning or rush_evening:
            anomaly_risk = 0.3 + np.random.normal(0, 0.05)
        elif night_time:
            anomaly_risk = 0.25 + np.random.normal(0, 0.03)
        else:
            anomaly_risk = 0.1 + np.random.normal(0, 0.02)
        
        features.append(np.clip(anomaly_risk, 0, 1))
        
        # 6. ë‚ ì”¨ ì‹œë®¬ë ˆì´ì…˜ (8ì›” ì—¬ë¦„)
        weather_impact = 0.6 + np.random.normal(0, 0.1)  # ì—¬ë¦„ì²  ë†’ì€ ì˜¨ë„
        features.append(np.clip(weather_impact, 0, 1))
        
        weather_features.append(features)
    
    weather_matrix = np.array(weather_features)
    
    print(f"   âœ… ë™ì  ì†ì„± ì™„ì„±: {weather_matrix.shape} (ì‹œê°„ Ã— íŠ¹ì„±)")
    
    return weather_matrix

def save_grid_astgcn_format(anomaly_matrix, grid_adjacency, grid_poi, weather_matrix, 
                           grid_info, time_index, output_dir='v2x_astgcn_data'):
    """ğŸ’¾ ê´‘ì£¼ ì§€ì—­ ê·¸ë¦¬ë“œ ê¸°ë°˜ AST-GCN í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì €ì¥"""
    print(f"ğŸ’¾ ê´‘ì£¼ ì§€ì—­ ê·¸ë¦¬ë“œ ê¸°ë°˜ AST-GCN ë°ì´í„° ì €ì¥: {output_dir}")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ì´ìƒì ìˆ˜ í–‰ë ¬ ì €ì¥
    anomaly_df = pd.DataFrame(anomaly_matrix)
    anomaly_path = os.path.join(output_dir, 'v2x_speed.csv')
    anomaly_df.to_csv(anomaly_path, header=False, index=False)
    print(f"   âœ… {anomaly_path}: {anomaly_matrix.shape}")
    
    # 2. ê²©ì ì¸ì ‘í–‰ë ¬ ì €ì¥
    adj_df = pd.DataFrame(grid_adjacency)
    adj_path = os.path.join(output_dir, 'v2x_adj.csv')
    adj_df.to_csv(adj_path, header=False, index=False)
    print(f"   âœ… {adj_path}: {grid_adjacency.shape}")
    
    # 3. ê²©ìë³„ ì •ì  ì†ì„± ì €ì¥
    poi_df = pd.DataFrame(grid_poi)
    poi_path = os.path.join(output_dir, 'v2x_poi.csv')
    poi_df.to_csv(poi_path, header=False, index=False)
    print(f"   âœ… {poi_path}: {grid_poi.shape}")
    
    # 4. ì‹œê°„ë³„ ë™ì  ì†ì„± ì €ì¥
    weather_df = pd.DataFrame(weather_matrix)
    weather_path = os.path.join(output_dir, 'v2x_weather.csv')
    weather_df.to_csv(weather_path, header=False, index=False)
    print(f"   âœ… {weather_path}: {weather_matrix.shape}")
    
    # 5. ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'task_type': 'anomaly_detection',
        'region': 'gwangju',
        'node_type': 'regional_grid',
        'grid_info': {
            'grid_size_degrees': grid_info['grid_size'],
            'grid_size_meters': int(grid_info['grid_size'] * 111000),
            'active_grids': grid_info['active_grids'],
            'region_bounds': GWANGJU_BOUNDS
        },
        'data_dimensions': {
            'time_steps': anomaly_matrix.shape[0],
            'num_grids': anomaly_matrix.shape[1],
            'poi_features': grid_poi.shape[1],
            'weather_features': weather_matrix.shape[1]
        },
        'time_range': {
            'start': str(time_index[0]),
            'end': str(time_index[-1]),
            'interval': '15min'
        },
        'average_anomaly_score': float(anomaly_matrix.mean())
    }
    
    metadata_path = os.path.join(output_dir, 'metadata.json')
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"   âœ… {metadata_path}: ê´‘ì£¼ ì§€ì—­ ë©”íƒ€ë°ì´í„°")

def convert_v2x_to_gwangju_grid_anomaly_detection(data_dir='data/daily_merged/08ì›”', 
                                                 output_dir='v2x_astgcn_data',
                                                 grid_size=0.01,
                                                 time_interval='15min'):
    """ğŸ¯ ê´‘ì£¼ V2X ë°ì´í„°ë¥¼ ì§€ì—­ ê·¸ë¦¬ë“œ ê¸°ë°˜ ì´ìƒíƒì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    print("=" * 80)
    print("ğŸ—ºï¸ ê´‘ì£¼ V2X â†’ ì§€ì—­ ê·¸ë¦¬ë“œ ê¸°ë°˜ ì´ìƒíƒì§€ ë³€í™˜ ì‹œì‘")
    print("=" * 80)
    
    # 1. V2X ì›ë³¸ ë°ì´í„° ë¡œë”© (ì²˜ìŒ 5ì¼ì¹˜ë§Œ)
    raw_df, label_data = load_v2x_data(data_dir)
    
    # 2. ìœ„ì¹˜ ë°ì´í„° ì •ì œ ë° ê´‘ì£¼ ë²”ìœ„ í•„í„°ë§
    raw_df = clean_and_filter_location_data(raw_df)
    
    # 3. ì´ìƒìƒí™© ë¼ë²¨ ìƒì„±
    raw_df = create_anomaly_labels(raw_df, label_data)
    
    # 4. ê´‘ì£¼ ì§€ì—­ ê·¸ë¦¬ë“œ ì‹œìŠ¤í…œ ìƒì„±
    grid_centers, vehicle_to_grid, grid_info = create_grid_system(raw_df, grid_size)
    
    # 5. ê²©ì ê°„ ì¸ì ‘í–‰ë ¬ ìƒì„±
    grid_adjacency = create_grid_adjacency_matrix(grid_centers)
    
    # 6. ê²©ì ê¸°ë°˜ ì´ìƒì ìˆ˜ í–‰ë ¬ ìƒì„±
    anomaly_matrix, time_index = create_grid_anomaly_matrix(
        raw_df, vehicle_to_grid, len(grid_centers), time_interval
    )
    
    # 7. ê²©ìë³„ ì •ì  ì†ì„± ìƒì„± (ê´‘ì£¼ íŠ¹í™”)
    grid_poi = create_grid_poi_features(grid_centers, grid_info)
    
    # 8. ì‹œê°„ë³„ ë™ì  ì†ì„± ìƒì„±
    weather_matrix = create_time_weather_features(time_index)
    
    # 9. AST-GCN í˜•ì‹ìœ¼ë¡œ ì €ì¥
    save_grid_astgcn_format(anomaly_matrix, grid_adjacency, grid_poi, weather_matrix,
                            grid_info, time_index, output_dir)
    
    print("=" * 80)
    print("âœ… ê´‘ì£¼ ì§€ì—­ ê·¸ë¦¬ë“œ ê¸°ë°˜ V2X ì´ìƒíƒì§€ ë³€í™˜ ì™„ë£Œ!")
    print(f"ğŸ“‚ ì¶œë ¥ ìœ„ì¹˜: {output_dir}")
    print(f"ğŸ—ºï¸ í™œì„± ê²©ì ìˆ˜: {len(grid_centers)}")
    print(f"â° ì‹œê°„ ìŠ¤í…: {len(time_index)}")
    print(f"ğŸš¨ í‰ê·  ì´ìƒì ìˆ˜: {anomaly_matrix.mean():.3f}")
    print("=" * 80)
    
    return anomaly_matrix, grid_adjacency, grid_poi, weather_matrix, grid_centers

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    # ì„¤ì • (ê´‘ì£¼ íŠ¹í™”)
    DATA_DIR = "data/daily_merged/08ì›”"
    OUTPUT_DIR = "v2x_astgcn_data"
    GRID_SIZE = 0.01  # 1.1km ê²©ì (ê´€ë¦¬ ê°€ëŠ¥í•œ í¬ê¸°)
    TIME_INTERVAL = "15min"  # 30ë¶„ ê°„ê²©
    
    # ë°ì´í„° í™•ì¸
    if not os.path.exists(DATA_DIR):
        print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}")
        exit(1)
    
    try:
        print("ğŸš€ ê´‘ì£¼ ì§€ì—­ ê·¸ë¦¬ë“œ ê¸°ë°˜ V2X ì´ìƒíƒì§€ ë³€í™˜ ì‹œì‘!")
        print(f"   ğŸ“Š ì„¤ì •:")
        print(f"     ë°ì´í„° ê²½ë¡œ: {DATA_DIR}")
        print(f"     ì¶œë ¥ ê²½ë¡œ: {OUTPUT_DIR}")
        print(f"     ê²©ì í¬ê¸°: {GRID_SIZE}ë„ (ì•½ {int(GRID_SIZE * 111)}km)")
        print(f"     ì‹œê°„ ê°„ê²©: {TIME_INTERVAL}")
        print(f"     ëŒ€ìƒ ì§€ì—­: ê´‘ì£¼ê´‘ì—­ì‹œ")
        
        # ë³€í™˜ ì‹¤í–‰
        anomaly_matrix, grid_adjacency, grid_poi, weather_matrix, grid_centers = convert_v2x_to_gwangju_grid_anomaly_detection(
            data_dir=DATA_DIR,
            output_dir=OUTPUT_DIR,
            grid_size=GRID_SIZE,
            time_interval=TIME_INTERVAL
        )
        
        print("\nğŸ‰ ê´‘ì£¼ ì§€ì—­ ê·¸ë¦¬ë“œ ë³€í™˜ ì„±ê³µ!")
        print(f"\nğŸ“Š ìµœì¢… ë°ì´í„° ìš”ì•½:")
        print(f"   ğŸ—ºï¸ ê²©ì ìˆ˜: {len(grid_centers)}ê°œ")
        print(f"   â° ì‹œê°„ ìŠ¤í…: {anomaly_matrix.shape[0]}ê°œ")
        print(f"   ğŸš¨ í‰ê·  ì´ìƒì ìˆ˜: {anomaly_matrix.mean():.3f}")
        print(f"   ğŸ“ˆ ì´ìƒ ë¹„ìœ¨: {(anomaly_matrix > 0.3).mean()*100:.2f}%")
        
    except Exception as e:
        print(f"âŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()